# DraftingTable

# Overview 
Writing clean code is hard work. Writing clear text about clean code is equally challenging. While quality does not necessairily equal popularity, this project aims to suggest improvements to blog posts before they are ever posted online, to improve the chances they will be seen by more people. 

This is done by comparing the content of a draft article about data science to a historical database of articles in the same genre which were collected from websites hosted by Medium, where article success can be measured using the number of claps received. Suggestions for improvement are made with respect to the clarity in terms of analytical metrics, the similarity to existing works in terms of a content-based recommender system, and the category in terms of topic modelling to suggest useful tags. 

# Data Sources & Processing

The data set used in this project is compiled from three sources:

1) A collection of 200+ recent articles scraped from the most popular Medium-affiliated data science sites (Towards Data Science, Hacker Noon, Insight Data Science, etc.)

This scraping was performed and the relevant data blocks extracted in the script:
> scripts/07-web-scraping-recent.py
This script outputs a truncated CSV containining the processed data to:
> data/processed/articles_scraped.csv

2) A Kaggle dataset of 279,577 articles of scraped posts put together by  Aiswarya Ramachandran which contains scraped posts from Medium-affiliated plots tagged with AI, Machine Learning, Datascience, or Artificial Intelligence from September 2017 - September 2018.
(https://www.kaggle.com/aiswaryaramachandran/medium-articles-with-content/downloads/medium-articles-with-content.zip/2)

This data set was processed to perform exploratory data analysis and filter the data set to retain only articles with an associated publication name and at least one code block. This code can be found in the script:
> scripts/01-article-filterer.py
> noteboos/01-exploratory-data-analysis.ipynb
This script outputs a truncated CSV containing the processed data to:
> data/processed/filtereddata.csv

# Methodology & Algorithms
What features of code and content translate to clear, accessible content? Do these features directly translate to success? To answer these questions, several natural language processing methods were applied.

### (1) Clarity: Article Analytics 
Individual articles within the collection which contain both text and code were analysed to extract metrics describing the content (vocabulary size, average sentence length, total word count, etc.) and the code (ratio of code/comment, average comment length, etc.) This is done in the script:
- scripts/02-filtered-article-analyser.py

At this stage, the articles were further separated into three files based on the dominant coding languages involved.
> data/processed/articles_javascript.csv
> data/processed/articles_python.csv
> data/processed/articles_sql.csv

In the final web app, Suggestion 1 is generated by comparing the text of the prospective post to the article database based on these analytics. The z-score (value - mean / std) for each metric is calculated, and the metric with the highest z-score is determined and returned to the user.

### (2) Similarity: Content-Based Recommender 
The text content of the draft article is compared to the content within the article database based on cosine similarity. 

The article text is pre-processed to convert case consistency, lemmatize text, and filter out both the list of English stopwords included in the NLTK standard set as well as Python-specific words determined to be common within this particular corpus ("pandas","row", "column", etc.)

Word embedding is performed using TF-IDF scores, to avoid needing to retrain a neural network based embedding (word2vec/BERT) to handle the domain specific terminology ("Jupyter Notebook", "Latent Dirichlet Allocation", etc.) This is done in the script:
- scripts/08-recommender-system.py

### (3) Category: Topic Modelling 
Common themes within the article database are determined using the Latent Dirichlet Allocation (LDA) method as implemented by the scikit-learn package. Given the imbalance in articles across coding languages (Python >> Javascript > SQL), only the Python articles were selected for this analysis. The method of Latent Dirichlet Allocation (LDA) as implemented by the sklearn analysis was selected for this approach, and was implemented in:
> scripts/03-topic-modelling-python.py

In the final web app, Suggestion 2 is generating by estimating the topic of the draft text and suggesting relevant keywords common to that topic.

# Frontend development
A frontend was built in Flask to bring together the three analyses described above and provide a simple process for users to get suggestions for improvement. It can be found here:
> draftingboard.dev (18.189.138.64)

 The backend of the Flask app can be found in:
> draftingboard/