# DraftingTable

## Overview 
![Project splash screen: lined paper on a wooden desk.](/insight/figures/project_intro.JPG)
Writing clean code is hard work. Writing clear text about clean code is equally challenging. Even after the initial draft is complete, authors face stiff competition: when it comes to online publishing, there is a huge imbalance in readership. With respect to the data set compiled for this project, 5% of the articles written have 73% of the positive attention.

While quality does not necessairily equal popularity, this project aims to suggest improvements to blog posts before they are ever posted online, to improve the chances they will be seen by more people. 

This is done by comparing the content of a draft article about data science to a historical database of articles in the same genre which were collected from websites hosted by Medium, where article success can be measured using the number of claps received. Suggestions for improvement are made with respect to the clarity in terms of analytical metrics, the similarity to existing works in terms of a content-based recommender system, and the category in terms of topic modelling to suggest useful tags. 

## Data Sources & Processing

The data set used in this project is compiled from three sources:

1) Web scraping. A collection of 200+ recent articles was scraped from the most popular Medium-affiliated data science sites (Towards Data Science, Hacker Noon, Insight Data Science, etc.)

2) Existing data. A dataset of 279,577 articles of scraped posts put together by  Aiswarya Ramachandran which contains scraped posts from Medium-affiliated plots tagged with AI, Machine Learning, Datascience, or Artificial Intelligence from September 2017 - September 2018.
(https://www.kaggle.com/aiswaryaramachandran/medium-articles-with-content/downloads/medium-articles-with-content.zip/2)

3) User input. A web app was developed where users can input their prospective post to be analysed, generating suggestions for improvement. If entered as pseudo-HTML code where code blocks are explicitly declared using &lt;code&gt; tags, the input will be separated into text and code and these two components processed separately. If entered as a block of text, the input will be treated as one long string and run through the text NLP pipeline. In either case, a message describing how the input was processed will be returned to the user in the web app.

## Methodology & Algorithms
What features of code and content translate to clear, accessible content? Do these features directly translate to success? To answer these questions, several natural language processing methods were applied.

### Pre-processing

### (1) Clarity: Article Analytics 
Individual articles within the collection which contain both text and code were analysed to extract metrics describing the content (vocabulary size, average sentence length, total word count, etc.) and the code (ratio of code/comment, average comment length, etc.) This is done in the script:
- scripts/02-filtered-article-analyser.py

At this stage, the articles were further separated into three files based on the dominant coding languages involved.
> data/processed/articles_javascript.csv
> data/processed/articles_python.csv
> data/processed/articles_sql.csv

![Distribution of the articles by coding language, and granular analysis of the text analytics calculated for Python-containing articles.](/insight/figures/validation_metrics.png)

In the final web app, Suggestion 1 is generated by comparing the text of the prospective post to the article database based on these analytics. The z-score (value - mean / std) for each metric is calculated, and the metric with the highest z-score is determined and returned to the user.

### (2) Similarity: Content-Based Recommender 
The text content of the draft article is compared to the content within the article database based on cosine similarity. 

The article text is pre-processed to convert case consistency, lemmatize text, and filter out both the list of English stopwords included in the NLTK standard set as well as Python-specific words determined to be common within this particular corpus ("pandas","row", "column", etc.)

Word embedding is performed using TF-IDF scores, to avoid needing to retrain a neural network based embedding (word2vec/BERT) to handle the domain specific terminology ("Jupyter Notebook", "Latent Dirichlet Allocation", etc.) This is done in the script:
- scripts/08-recommender-system.py

<img source="https://github.com/stepheli/insight/blob/master/insight/figures/recommender_validation.jpg" width=500>

In the final web app, Suggestion 2 is generated by comparing the similarity score of the prospective post to the database article with the highest score.

### (3) Category: Topic Modelling 
Common themes within the article database are determined using the Latent Dirichlet Allocation (LDA) method as implemented by the scikit-learn package. Given the imbalance in articles across coding languages (Python >> Javascript > SQL), only the Python articles were selected for this analysis. The method of Latent Dirichlet Allocation (LDA) as implemented by the sklearn analysis was selected for this approach, and was implemented in:
> scripts/03-topic-modelling-python.py

The model was refined to achieve high intertopic distance (visualized through multidimensional scaling implemented by the pyLDAvis package), and minimal keyword overlap, resulting in the final list of topics:
- General machine learning
- General data science
- Natural language processing
- Neural networks
- Regressors, classifiers, and clustering

![pyLDAvis of the intertopic distance in the optimal topic model.](/insight/figures/pyLDAvis.PNG)

In the final web app, Suggestion 3 is generating by estimating the topic of the draft text and suggesting relevant keywords common to that topic.

## Frontend development
A frontend was built in Flask to bring together the three analyses described above and provide a simple process for users to get suggestions for improvement. 

It has been hosted on an AWS EC2 instance and can be found here:
> draftingboard.dev (18.189.138.64)

![Home page of the web app.](/insight/figures/screenshot_1.PNG)

When the user enters the text of a draft post, it will be analysed according to the metrics, recommender system, and topic modelling discussed above and a list of suggestions for improvement will be returned to the reader.

![First half of the web app results page.](/insight/figures/screenshot_2.PNG)
![Second half of the web app results page.](/insight/figures/screenshot_3.PNG)

If the user declares code blocks using HTML tags (&lt;code&gt;), the text and code will be separated and processed through different NLP pipelines to analyse the degree of commenting within the code. If the user enters a block of text, it will be treated as one large string and run through the text NLP pipeline.

 The backend of the Flask app can be found in:
> draftingboard/
