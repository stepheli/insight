{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pyLDAvis # this package is case sensitive\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModellingSklearn:\n",
    "    def __init__(self,text,min_df,max_df,n_components,random_state):\n",
    "        \"\"\" \n",
    "        Initialize class. \n",
    "        \n",
    "        Arguments:\n",
    "            text - DF column containing text block\n",
    "            min_df = Minimum number of articles the word must appear in for the\n",
    "                word to be considered.\n",
    "            max_df = Threshold for unique words to considered (drop words \n",
    "               appearing too frequently, as in stopwords)\n",
    "            n_topics = Number of topics to consider\n",
    "            random_seed = Random seed to use for the modelling\n",
    "        \"\"\"\n",
    "        # Set up internal class variables\n",
    "        self.text = text\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Fit an LDA model\n",
    "        self.LDA_model, self.word_frequency, self.vocabulary = self.LDA_model()\n",
    "\n",
    "            \n",
    "    def LDA_model(self):\n",
    "        \"\"\" Fit text to an LDA model \"\"\"\n",
    "        stop_words_all = list(nltk.corpus.stopwords.words('english'))\n",
    "        print(len(stop_words_all))\n",
    "        stop_words_new = [\"new\",\"like\",\"example\",\"see\",\"code\",\n",
    "                          \"use\",\"used\",\"using\",\"user\",\"one\",\"two\",\"also\",\n",
    "                          \"analysis\",\"data\",\"dataset\",\"row\",\"column\",\n",
    "                         \"set\",\"list\",\"index\",\"item\",\"array\",\n",
    "                          \"let\",\"input\",\"return\",\"function\",\"python\",\n",
    "                         \"panda\",\"package\",\"number\",\"would\",\"figure\",\"make\",\"get\"]\n",
    "        stop_words_all.extend(stop_words_new)\n",
    "        print(len(stop_words_all))\n",
    "        \n",
    "        word_frequency = TfidfVectorizer(min_df = self.min_df,\n",
    "                                        stop_words=stop_words_all)\n",
    "                        \n",
    "        vocabulary = word_frequency.fit_transform(\n",
    "                self.text.values.astype('U'))\n",
    "        \n",
    "        feature_names = word_frequency.get_feature_names()\n",
    "        corpus_index = [n for n in self.text.values]\n",
    "        df = pd.DataFrame(vocabulary.todense(), index=corpus_index, columns=feature_names)\n",
    "        print(df.head())\n",
    "        print(df.shape)\n",
    "                \n",
    "#         tfidf = TfidfVectorizer(vocabulary = myvocabulary, ngram_range = (1,3))\n",
    "#         tfs = tfidf.fit_transform(corpus.values())\n",
    "            \n",
    "        LDA = LatentDirichletAllocation(n_components = self.n_components,\n",
    "                                        random_state = self.random_state)\n",
    "        LDA_model = LDA.fit(vocabulary)\n",
    "        \n",
    "        return LDA_model, word_frequency, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(article_text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        stop_words_all = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        article_text_proc = []\n",
    "        article_text = article_text.split(\" \")\n",
    "        for word in article_text:\n",
    "            word = word.lower()\n",
    "            if word not in stop_words_all:\n",
    "                article_text_proc.append(lemmatizer.lemmatize(word))\n",
    "        processed_text = \" \".join(article_text_proc)\n",
    "                \n",
    "        return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "filedir = os.path.dirname(os.path.realpath('__file__'))\n",
    "filename = os.path.join('../data/processed/articles_python.csv')\n",
    "filename = os.path.abspath(os.path.realpath(filename))\n",
    "articles_python = pd.read_csv(filename,index_col = \"postId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text of article 10\n",
      "Classification Model Evaluation\n",
      "\n",
      "What is Model Evaluation?\n",
      "Model evaluation is the process of choosing between models, different model types, tuning parameters, and features. Better evaluation processes lead to better, more accurate models in your applications\n",
      "In this article we’ll be discussing Model Evaluation for a supervised classification model. We’ll cover evaluation procedures, evaluation metrics, and where to apply them.\n",
      "Prerequisites\n",
      "Python 3.+\n",
      "Anaconda (Scikit Learn, Numpy, Pandas, Mat\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text of article 10\")\n",
    "print(articles_python[\"text\"].iloc[325][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lemmatizing article 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lemmatizing article 250\n",
      "Now lemmatizing article 500\n",
      "Now lemmatizing article 750\n",
      "Now lemmatizing article 1000\n",
      "Now lemmatizing article 1250\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(articles_python)):\n",
    "    if np.remainder(i,250) == 0:\n",
    "        print(\"Now lemmatizing article {}\".format(i))\n",
    "    articles_python[\"text\"].iloc[i] = lemmatize_text(articles_python[\"text\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1438\n",
      "Lemmatized text of article 10\n",
      "classification model evaluation\n",
      "\n",
      "what model evaluation?\n",
      "model evaluation process choosing models, different model types, tuning parameters, features. better evaluation process lead better, accurate model applications\n",
      "in article we’ll discussing model evaluation supervised classification model. we’ll cover evaluation procedures, evaluation metrics, apply them.\n",
      "prerequisites\n",
      "python 3.+\n",
      "anaconda (scikit learn, numpy, pandas, matplotlib, seaborn)\n",
      "jupyter notebook.\n",
      "basic understanding supervised mach\n"
     ]
    }
   ],
   "source": [
    "print(len(articles_python))\n",
    "# Test that the lemmatization worked as intended\n",
    "print(\"Lemmatized text of article 10\")\n",
    "print(articles_python[\"text\"].iloc[325][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "213\n",
      "                                                     00       000  00001  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  0.000000    0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  0.000000    0.0   \n",
      "\\na complete machine learning project walk-thro...  0.0  0.019964    0.0   \n",
      "train machine learning model google’s gpus free...  0.0  0.000000    0.0   \n",
      "train ai convert design mockups html css\\nwithi...  0.0  0.000000    0.0   \n",
      "\n",
      "                                                    0001  00021  0005  001  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...   0.0    0.0   0.0  0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...   0.0    0.0   0.0  0.0   \n",
      "\\na complete machine learning project walk-thro...   0.0    0.0   0.0  0.0   \n",
      "train machine learning model google’s gpus free...   0.0    0.0   0.0  0.0   \n",
      "train ai convert design mockups html css\\nwithi...   0.0    0.0   0.0  0.0   \n",
      "\n",
      "                                                    002  004  005  ...  zones  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  0.0  0.0  ...    0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  0.0  0.0  ...    0.0   \n",
      "\\na complete machine learning project walk-thro...  0.0  0.0  0.0  ...    0.0   \n",
      "train machine learning model google’s gpus free...  0.0  0.0  0.0  ...    0.0   \n",
      "train ai convert design mockups html css\\nwithi...  0.0  0.0  0.0  ...    0.0   \n",
      "\n",
      "                                                    zoning  zoo  zoom  zoomed  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...     0.0  0.0   0.0     0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...     0.0  0.0   0.0     0.0   \n",
      "\\na complete machine learning project walk-thro...     0.0  0.0   0.0     0.0   \n",
      "train machine learning model google’s gpus free...     0.0  0.0   0.0     0.0   \n",
      "train ai convert design mockups html css\\nwithi...     0.0  0.0   0.0     0.0   \n",
      "\n",
      "                                                    zooming  zynga   β1   θ0  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...      0.0    0.0  0.0  0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...      0.0    0.0  0.0  0.0   \n",
      "\\na complete machine learning project walk-thro...      0.0    0.0  0.0  0.0   \n",
      "train machine learning model google’s gpus free...      0.0    0.0  0.0  0.0   \n",
      "train ai convert design mockups html css\\nwithi...      0.0    0.0  0.0  0.0   \n",
      "\n",
      "                                                     θ1  \n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  \n",
      "\\na complete machine learning project walk-thro...  0.0  \n",
      "train machine learning model google’s gpus free...  0.0  \n",
      "train ai convert design mockups html css\\nwithi...  0.0  \n",
      "\n",
      "[5 rows x 11599 columns]\n",
      "(1438, 11599)\n"
     ]
    }
   ],
   "source": [
    "# sklearn/LDA (unsupervised); text case consistency + lemmatization\n",
    "model_alpha = TopicModellingSklearn(text=articles_python[\"text\"],\n",
    "                                       min_df = 3,\n",
    "                                       max_df = 0.75,\n",
    "                                       n_components = 3,\n",
    "                                       random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "213\n",
      "                                                     00       000  00001  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  0.000000    0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  0.000000    0.0   \n",
      "\\na complete machine learning project walk-thro...  0.0  0.019964    0.0   \n",
      "train machine learning model google’s gpus free...  0.0  0.000000    0.0   \n",
      "train ai convert design mockups html css\\nwithi...  0.0  0.000000    0.0   \n",
      "\n",
      "                                                    0001  00021  0005  001  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...   0.0    0.0   0.0  0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...   0.0    0.0   0.0  0.0   \n",
      "\\na complete machine learning project walk-thro...   0.0    0.0   0.0  0.0   \n",
      "train machine learning model google’s gpus free...   0.0    0.0   0.0  0.0   \n",
      "train ai convert design mockups html css\\nwithi...   0.0    0.0   0.0  0.0   \n",
      "\n",
      "                                                    002  004  005  ...  zones  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  0.0  0.0  ...    0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  0.0  0.0  ...    0.0   \n",
      "\\na complete machine learning project walk-thro...  0.0  0.0  0.0  ...    0.0   \n",
      "train machine learning model google’s gpus free...  0.0  0.0  0.0  ...    0.0   \n",
      "train ai convert design mockups html css\\nwithi...  0.0  0.0  0.0  ...    0.0   \n",
      "\n",
      "                                                    zoning  zoo  zoom  zoomed  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...     0.0  0.0   0.0     0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...     0.0  0.0   0.0     0.0   \n",
      "\\na complete machine learning project walk-thro...     0.0  0.0   0.0     0.0   \n",
      "train machine learning model google’s gpus free...     0.0  0.0   0.0     0.0   \n",
      "train ai convert design mockups html css\\nwithi...     0.0  0.0   0.0     0.0   \n",
      "\n",
      "                                                    zooming  zynga   β1   θ0  \\\n",
      "\\nhow build alphazero ai using python keras\\nte...      0.0    0.0  0.0  0.0   \n",
      "\\npython perfect tool problem\\nreflecting first...      0.0    0.0  0.0  0.0   \n",
      "\\na complete machine learning project walk-thro...      0.0    0.0  0.0  0.0   \n",
      "train machine learning model google’s gpus free...      0.0    0.0  0.0  0.0   \n",
      "train ai convert design mockups html css\\nwithi...      0.0    0.0  0.0  0.0   \n",
      "\n",
      "                                                     θ1  \n",
      "\\nhow build alphazero ai using python keras\\nte...  0.0  \n",
      "\\npython perfect tool problem\\nreflecting first...  0.0  \n",
      "\\na complete machine learning project walk-thro...  0.0  \n",
      "train machine learning model google’s gpus free...  0.0  \n",
      "train ai convert design mockups html css\\nwithi...  0.0  \n",
      "\n",
      "[5 rows x 11599 columns]\n",
      "(1438, 11599)\n"
     ]
    }
   ],
   "source": [
    "# sklearn/LDA (unsupervised); text case consistency + lemmatization\n",
    "model_beta = TopicModellingSklearn(text=articles_python[\"text\"],\n",
    "                                       min_df = 3,\n",
    "                                       max_df = 0.75,\n",
    "                                       n_components = 7,\n",
    "                                       random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -150969.59261284457\n",
      "Perplexity:  16231.23394788008\n",
      "Model parameters:\n",
      "{'batch_size': 128,\n",
      " 'doc_topic_prior': None,\n",
      " 'evaluate_every': -1,\n",
      " 'learning_decay': 0.7,\n",
      " 'learning_method': 'batch',\n",
      " 'learning_offset': 10.0,\n",
      " 'max_doc_update_iter': 100,\n",
      " 'max_iter': 10,\n",
      " 'mean_change_tol': 0.001,\n",
      " 'n_components': 7,\n",
      " 'n_jobs': None,\n",
      " 'perp_tol': 0.1,\n",
      " 'random_state': 42,\n",
      " 'topic_word_prior': None,\n",
      " 'total_samples': 1000000.0,\n",
      " 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Likelihood: \", model_beta.LDA_model.score(model_beta.vocabulary))\n",
    "print(\"Perplexity: \", model_beta.LDA_model.perplexity(model_beta.vocabulary))\n",
    "print(\"Model parameters:\")\n",
    "pprint(model_beta.LDA_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'n_components' : [5,6,7,8,9,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_base = LatentDirichletAllocation()\n",
    "model_testing = GridSearchCV(lda_base, param_grid=search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=LatentDirichletAllocation(batch_size=128,\n",
       "                                                 doc_topic_prior=None,\n",
       "                                                 evaluate_every=-1,\n",
       "                                                 learning_decay=0.7,\n",
       "                                                 learning_method='batch',\n",
       "                                                 learning_offset=10.0,\n",
       "                                                 max_doc_update_iter=100,\n",
       "                                                 max_iter=10,\n",
       "                                                 mean_change_tol=0.001,\n",
       "                                                 n_components=10, n_jobs=None,\n",
       "                                                 perp_tol=0.1,\n",
       "                                                 random_state=None,\n",
       "                                                 topic_word_prior=None,\n",
       "                                                 total_samples=1000000.0,\n",
       "                                                 verbose=0),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'n_components': [5, 6, 7, 8, 9, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_testing.fit(model_beta.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = model_testing.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Model's Params: \", model_testing.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Model beta:')\n",
    "for i, topic in enumerate(model_beta.LDA_model.components_):\n",
    "    print('Top words for topic {}:'.format(i))\n",
    "    print([model_beta.word_frequency.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {0 : 'general machine learning',\n",
    "             1 : 'general data science',\n",
    "             2 : 'natural language processing',\n",
    "             3 : 'natural language processing',\n",
    "             4 : 'general data science',\n",
    "             5 : 'neural networks',\n",
    "             6 : 'clustering'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared_alpha = pyLDAvis.sklearn.prepare(model_alpha.LDA_model, model_alpha.vocabulary, model_alpha.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LDAvis_prepared_beta = pyLDAvis.sklearn.prepare(model_beta.LDA_model, model_beta.vocabulary, model_beta.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.prepared_data_to_html(LDAvis_prepared, template_type='general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_beta = model_beta.LDA_model\n",
    "# filename = 'pickled_LDA_model.sav'\n",
    "# pickle.dump(model_beta, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_to_html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_data_to_html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
