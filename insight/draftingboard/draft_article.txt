<title> DraftingBoard: Clarity in content and code </title>

<p> Writing clean code is hard work. Writing clear text about clean code is equally challenging. While quality does not necessairily equal popularity, this project aims to suggest improvements to blog posts before they are ever posted online, to improve the chances they will be seen by more people. </p>

<p> This is done by comparing the content of a draft article about data science to a historical database of articles in the same genre which were collected from websites hosted by Medium, where article success can be measured using the number of claps received. Suggestions for improvement are made with respect to the clarity in terms of analytical metrics, the similarity to existing works in terms of a content-based recommender system, and the category in terms of topic modelling to suggest useful tags. </p>

<code>
# Set up language keywords
python_keywords = ["import","#"]
javascript_keywords = ["var","//"]
cmdline_keywords = ["sudo","apt-get","pip","$"]
sql_keywords = ["SELECT"]

# Assign language based on presence of keywords
if any(keyword in self.words for keyword in python_keywords):
	detected_language = 1 # Python
if any(keyword in self.words for keyword in javascript_keywords):
	# Avoid confusion with 'for var in vars' syntax; given dominance of
	# Python articles, do not overwrite a Python assignment
	if detected_language > 0: 
		pass
	else:
		detected_language = 2 # Javascript
if any(keyword in self.words for keyword in cmdline_keywords):
	detected_language = 3 # cmdline
if any(keyword in self.words for keyword in sql_keywords):
	# Some flavours of SQL share # as the comment character. Given 
	# dominance of Python articles, do not overwrite a Python assignment
	if detected_language > 0:
		pass
	else:
		detected_language = 4 # SQL
</code>

<p> The text content of the draft article is compared to the content within the article database based on cosine similarity.The article text is pre-processed to convert case consistency, lemmatize text, and filter out both the list of English stopwords included in the NLTK standard set as well as Python-specific words determined to be common within this particular corpus ("pandas","row", "column", etc.) </p>

<p>Word embedding is performed using TF-IDF scores, to avoid needing to retrain a neural network based embedding (word2vec/BERT) to handle the domain specific terminology ("Jupyter Notebook", "Latent Dirichlet Allocation", etc.)</p>