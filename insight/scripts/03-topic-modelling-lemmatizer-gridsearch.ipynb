{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pyLDAvis # this package is case sensitive\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicModellingSklearn:\n",
    "    def __init__(self,text,min_df,max_df,n_components,random_state):\n",
    "        \"\"\" \n",
    "        Initialize class. \n",
    "        \n",
    "        Arguments:\n",
    "            text - DF column containing text block\n",
    "            min_df = Minimum number of articles the word must appear in for the\n",
    "                word to be considered.\n",
    "            max_df = Threshold for unique words to considered (drop words \n",
    "               appearing too frequently, as in stopwords)\n",
    "            n_topics = Number of topics to consider\n",
    "            random_seed = Random seed to use for the modelling\n",
    "        \"\"\"\n",
    "        # Set up internal class variables\n",
    "        self.text = text\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Fit an LDA model\n",
    "        self.LDA_model, self.word_frequency, self.vocabulary = self.LDA_model()\n",
    "\n",
    "            \n",
    "    def LDA_model(self):\n",
    "        \"\"\" Fit text to an LDA model \"\"\"\n",
    "        stop_words_all = list(nltk.corpus.stopwords.words('english'))\n",
    "        print(len(stop_words_all))\n",
    "        stop_words_new = [\"new\",\"like\",\"example\",\"see\",\"code\",\n",
    "                          \"use\",\"used\",\"using\",\"user\",\"one\",\"two\",\"also\",\n",
    "                          \"analysis\",\"data\",\"dataset\",\"row\",\"column\",\n",
    "                         \"set\",\"list\",\"index\",\"item\",\"array\",\n",
    "                          \"let\",\"input\",\"return\",\"function\",\"python\",\n",
    "                         \"panda\",\"package\",\"number\",\"would\",\"figure\",\"make\",\"get\"]\n",
    "        stop_words_all.extend(stop_words_new)\n",
    "        print(len(stop_words_all))\n",
    "        \n",
    "        word_frequency = CountVectorizer(min_df = self.min_df,\n",
    "                                        stop_words=stop_words_all)\n",
    "                \n",
    "        vocabulary = word_frequency.fit_transform(\n",
    "                self.text.values.astype('U'))\n",
    "        \n",
    "        LDA = LatentDirichletAllocation(n_components = self.n_components,\n",
    "                                        random_state = self.random_state)\n",
    "        LDA_model = LDA.fit(vocabulary)\n",
    "        \n",
    "        return LDA_model, word_frequency, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(article_text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        stop_words_all = nltk.corpus.stopwords.words('english')\n",
    "        \n",
    "        article_text_proc = []\n",
    "        article_text = article_text.split(\" \")\n",
    "        for word in article_text:\n",
    "            word = word.lower()\n",
    "            if word not in stop_words_all:\n",
    "                article_text_proc.append(lemmatizer.lemmatize(word))\n",
    "        processed_text = \" \".join(article_text_proc)\n",
    "                \n",
    "        return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "filedir = os.path.dirname(os.path.realpath('__file__'))\n",
    "filename = os.path.join('../data/processed/articles_python.csv')\n",
    "filename = os.path.abspath(os.path.realpath(filename))\n",
    "articles_python = pd.read_csv(filename,index_col = \"postId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text of article 10\n",
      "\n",
      "Automated Machine Learning Hyperparameter Tuning in Python\n",
      "A complete walk through using Bayesian optimization for automated hyperparameter tuning in Python\n",
      "Tuning machine learning hyperparameters is a tedious yet crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine learning pipeline like feature engineering and interpreting results. Grid and random search are hands-off, but r\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text of article 10\")\n",
    "print(articles_python[\"text\"].iloc[100][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now lemmatizing article 0\n",
      "Now lemmatizing article 250\n",
      "Now lemmatizing article 500\n",
      "Now lemmatizing article 750\n",
      "Now lemmatizing article 1000\n",
      "Now lemmatizing article 1250\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(articles_python)):\n",
    "    if np.remainder(i,250) == 0:\n",
    "        print(\"Now lemmatizing article {}\".format(i))\n",
    "    articles_python[\"text\"].iloc[i] = lemmatize_text(articles_python[\"text\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text of article 10\n",
      "\n",
      "automated machine learning hyperparameter tuning python\n",
      "a complete walk using bayesian optimization automated hyperparameter tuning python\n",
      "tuning machine learning hyperparameters tedious yet crucial task, performance algorithm highly dependent choice hyperparameters. manual tuning take time away important step machine learning pipeline like feature engineering interpreting results. grid random search hands-off, require long run time waste time evaluating unpromising area search space. increasin\n"
     ]
    }
   ],
   "source": [
    "# Test that the lemmatization worked as intended\n",
    "print(\"Lemmatized text of article 10\")\n",
    "print(articles_python[\"text\"].iloc[100][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sklearn/LDA (unsupervised); text case consistency + lemmatization\n",
    "model_beta = TopicModellingSklearn(text=articles_python[\"text\"],\n",
    "                                       min_df = 3,\n",
    "                                       max_df = 0.75,\n",
    "                                       n_components = 7,\n",
    "                                       random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Log Likelihood: \", model_beta.LDA_model.score(model_beta.vocabulary))\n",
    "print(\"Perplexity: \", model_beta.LDA_model.perplexity(model_beta.vocabulary))\n",
    "print(\"Model parameters:\")\n",
    "pprint(model_beta.LDA_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'n_components' : [5,6,7,8,9,10],\n",
    "                'min_df' : 3,\n",
    "                'max_df' : 0.75,\n",
    "                'random_state' : 42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_base = LatentDirichletAllocation()\n",
    "model_testing = GridSearchCV(lda_base, param_grid=search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_testing.fit(model_beta.vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lda_model = model_testing.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Model's Params: \", model_testing.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Model beta:')\n",
    "for i, topic in enumerate(model_beta.LDA_model.components_):\n",
    "    print('Top words for topic {}:'.format(i))\n",
    "    print([model_beta.word_frequency.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {0 : 'general machine learning',\n",
    "             1 : 'general data science',\n",
    "             2 : 'natural language processing',\n",
    "             3 : 'natural language processing',\n",
    "             4 : 'general data science',\n",
    "             5 : 'neural networks',\n",
    "             6 : 'clustering'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis.sklearn.prepare(model_beta.LDA_model, model_beta.vocabulary, model_beta.word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.prepared_data_to_html(LDAvis_prepared, template_type='general')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_beta = model_beta.LDA_model\n",
    "filename = 'pickled_LDA_model.sav'\n",
    "pickle.dump(model_beta, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_to_html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_to_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyLDAvis.prepared_data_to_html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
