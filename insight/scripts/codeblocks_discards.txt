Item 0: 
This series is available as a full-length e-book! Download here. Free for download, contributions appreciated (paypal.me/ml4h)
 This series is a guide for getting up-to-speed on high-level machine learning concepts in ~2-3 hours.
 If you're more interested in figuring out which courses to take, textbooks to read, projects to attempt, etc., take a look at our recommendations in the Appendix: The Best Machine Learning Resources.
 The AI effect: what actually qualifies as “artificial intelligence”?
 The exact standard for technology that qualifies as “AI” is a bit fuzzy, and interpretations change over time. The AI label tends to describe machines doing tasks traditionally in the domain of humans. Interestingly, once computers figure out how to do one of these tasks, humans have a tendency to say it wasn’t really intelligence. This is known as the AI effect.
 For example, when IBM’s Deep Blue defeated world chess champion Garry Kasparov in 1997, people complained that it was using "brute force" methods and it wasn’t “real” intelligence at all. As Pamela McCorduck wrote, “It’s part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something — play good checkers, solve simple but relatively informal problems — there was chorus of critics to say, ‘that’s not thinking’”(McCorduck, 2004).
 Perhaps there is a certain je ne sais quoi inherent to what people will reliably accept as “artificial intelligence”:
 "AI is whatever hasn't been done yet." - Douglas Hofstadter
 So does a calculator count as AI? Maybe by some interpretation. What about a self-driving car? Today, yes. In the future, perhaps not. Your cool new chatbot startup that automates a flow chart? Sure… why not.
 There are complex, high-stakes questions about AI that will require  our careful attention in the coming years.
 How can we combat AI’s propensity to further entrench systemic biases evident in existing data sets? What should we make of fundamental disagreements among the world’s most powerful technologists about the potential risks and benefits of artificial intelligence? What will happen to humans' sense of purpose in a world without work?
 
 
Item 1: 
{
  "login": "petrgazarov",
  "id": 5581195,
  "avatar_url": "https://avatars.githubusercontent.com/u/5581195?v=3",
  "gravatar_id": "",
  "url": "https://api.github.com/users/petrgazarov",
  "html_url": "https://github.com/petrgazarov",
  "followers_url": "https://api.github.com/users/petrgazarov/followers",
  "following_url": "https://api.github.com/users/petrgazarov/following{/other_user}",
  "gists_url": "https://api.github.com/users/petrgazarov/gists{/gist_id}",
  "starred_url": "https://api.github.com/users/petrgazarov/starred{/owner}{/repo}",
  "subscriptions_url": "https://api.github.com/users/petrgazarov/subscriptions",
  "organizations_url": "https://api.github.com/users/petrgazarov/orgs",
  "repos_url": "https://api.github.com/users/petrgazarov/repos",
  "events_url": "https://api.github.com/users/petrgazarov/events{/privacy}",
  "received_events_url": "https://api.github.com/users/petrgazarov/received_events",
  "type": "User",
  "site_admin": false,
  "name": "Petr Gazarov",
  "company": "PolicyGenius",
  "blog": "http://petrgazarov.com/",
  "location": "NYC",
  "email": "petrgazarov@gmail.com",
  "hireable": null,
  "bio": null,
  "public_repos": 23,
  "public_gists": 0,
  "followers": 7,
  "following": 14,
  "created_at": "2013-10-01T00:33:23Z",
  "updated_at": "2016-08-02T05:44:01Z"
}
 
 
Item 2: 
+------+------+-----------+
| Rank | Code | Frequency |
+------+------+-----------+
| #1   | 1234 | 10.713%   |
| #2   | 1111 | 6.016%    |
| #3   | 0000 | 1.881%    |
| #4   | 1212 | 1.197%    |
| #5   | 7777 | 0.745%    |
| #6   | 1004 | 0.616%    |
| #7   | 2000 | 0.613%    |
| #8   | 4444 | 0.526%    |
| #9   | 2222 | 0.516%    |
| #10  | 6969 | 0.512%    |
| #11  | 9999 | 0.451%    |
| #12  | 3333 | 0.419%    |
| #13  | 5555 | 0.395%    |
| #14  | 6666 | 0.391%    |
| #15  | 1122 | 0.366%    |
| #16  | 1313 | 0.304%    |
| #17  | 8888 | 0.303%    |
| #18  | 4321 | 0.293%    |
| #19  | 2001 | 0.290%    |
| #20  | 1010 | 0.285%    |
+------+------+-----------+
 console.log((Math.floor(Math.random() * 10000) + 10000).toString().substring(1));
 
 
Item 4: 
Recommend & share..
 
 
Item 6: 
Imagine that there were two possible states for weather: sunny or cloudy. You can always directly observe the current weather state, and it is guaranteed to always be one of the two aforementioned states.
 Now, you decide you want to be able to predict what the weather will be like tomorrow. Intuitively, you assume that there is an inherent transition in this process, in that the current weather has some bearing on what the next day’s weather will be. So, being the dedicated person that you are, you collect weather data over several years, and calculate that the chance of a sunny day occurring after a cloudy day is 0.25. You also note that, by extension, the chance of a cloudy day occurring after a cloudy day must be 0.75, since there are only two possible states.
 You can now use this distribution to predict weather for days to come, based on what the current weather state is at the time.
 
 
Item 7: 
is 'Python' in sentence? 
is 'Java' in sentence?
...
 is 'I' in corpus?
is 'like' in corpus?
is 'python' in corpus?
 Step 1: is <start>I<EOT> in dictionary? No
Step 2: is <start>like<EOT> in dictionary? No
Step 3: is <start>Python<EOT> in dictionary? Yes
 
 
Item 13: 
(3333, 20)
 Index(['State', 'Account length', 'Area code', 'International plan',
       'Voice mail plan', 'Number vmail messages', 'Total day minutes',
       'Total day calls', 'Total day charge', 'Total eve minutes',
       'Total eve calls', 'Total eve charge', 'Total night minutes',
       'Total night calls', 'Total night charge', 'Total intl minutes',
       'Total intl calls', 'Total intl charge', 'Customer service calls',
       'Churn'],
      dtype='object')
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 3333 entries, 0 to 3332
Data columns (total 20 columns):
State                     3333 non-null object
Account length            3333 non-null int64
Area code                 3333 non-null int64
International plan        3333 non-null object
Voice mail plan           3333 non-null object
Number vmail messages     3333 non-null int64
Total day minutes         3333 non-null float64
Total day calls           3333 non-null int64
Total day charge          3333 non-null float64
Total eve minutes         3333 non-null float64
Total eve calls           3333 non-null int64
Total eve charge          3333 non-null float64
Total night minutes       3333 non-null float64
Total night calls         3333 non-null int64
Total night charge        3333 non-null float64
Total intl minutes        3333 non-null float64
Total intl calls          3333 non-null int64
Total intl charge         3333 non-null float64
Customer service calls    3333 non-null int64
Churn                     3333 non-null bool
dtypes: bool(1), float64(8), int64(8), object(3)
memory usage: 498.1+ KB
None
 df.describe()
 0    2850
1     483
Name: Churn, dtype: int64
 0    0.855086
1    0.144914
Name: Churn, dtype: float64
 0.14491449144914492
 Account length            102.664596
Area code                 437.817805
Number vmail messages       5.115942
Total day minutes         206.914079
Total day calls           101.335404
Total day charge           35.175921
Total eve minutes         212.410145
Total eve calls           100.561077
Total eve charge           18.054969
Total night minutes       205.231677
Total night calls         100.399586
Total night charge          9.235528
Total intl minutes         10.700000
Total intl calls            4.163561
Total intl charge           2.889545
Customer service calls      2.229814
Churn                       1.000000
dtype: float64
 206.91407867494814
 18.899999999999999
 State                        WY
Account length              243
Area code                   510
International plan          Yes
Voice mail plan             Yes
Number vmail messages        51
Total day minutes         350.8
Total day calls             165
Total day charge          59.64
Total eve minutes         363.7
Total eve calls             170
Total eve charge          30.91
Total night minutes         395
Total night calls           175
Total night charge        17.77
Total intl minutes           20
Total intl calls             20
Total intl charge           5.4
Customer service calls        9
Churn                         1
dtype: object
 df.groupby(by=grouping_columns)[columns_to_show].function()
 pd.crosstab(df['Churn'], df['International plan'], margins=True)
 
 
Item 14: 
{
  "items": [
    {
      "item_id": 1,
      "item_name": "Soylent",
      "item_stock": 1,
      "price": 10
    }
  ],
  "users": [
    {
      "face_id": 1,
      "name": "Subhan Nadeem",
      "in_store": false,
      "cart": [
        1
      ]
    }
  ]
}
 
 
Item 17: 
Recommend & share..
 
 
Item 18: 
<script src="face-api.js"></script>
 npm i face-api.js
 
 
Item 22: 
New Q value = 
   Current Q value + 
   lr * [Reward + discount_rate * (highest Q value between possible actions from the new state s’ ) — Current Q value ]
 
 
Item 24: 
item {
  id: 1
  name: 'raccoon'
}
 tensorboard — logdir=gs://${YOUR_CLOUD_BUCKET}
 
 
Item 27: 
1. Visit your your favorite source for Tech news (Twitter, Hacker News, Term Sheet, reddit, TechCrunch, Mattermark Daily, CB Insights, etc.)
 2. Hit “crtl + f”
 3. Search “artificial intelligence” or “machine learning”
 4. If you don't find any matches, publicly shame me on Twitter.
 
 
Item 34: 
callbacks = [keras.callbacks.ModelCheckpoint(hist_model, verbose=1,save_best_only =True, monitor= ’val_IOU_calc_loss’), plot_losses]
 
 
Item 40: 
['DESCR', 'data', 'feature_names', 'target', 'target_names']
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
 ['setosa' 'versicolor' 'virginica']
 [0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 1 1 1 2 1 1 1 1 1 1 2 2 1 1 1 1 2 1 2 1 2 1 1 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2]
 
 
Item 43: 
calories = -21.83 + 7.17 * duration
 
 
Item 46: 
27 documents
9 classes ['goodbye', 'greeting', 'hours', 'mopeds', 'opentoday', 'payments', 'rental', 'thanks', 'today']
44 unique stemmed words ["'d", 'a', 'ar', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'do', 'doe', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'is', 'kind', 'lat', 'lik', 'mastercard', 'mop', 'of', 'on', 'op', 'rent', 'see', 'tak', 'thank', 'that', 'ther', 'thi', 'to', 'today', 'we', 'what', 'when', 'which', 'work', 'you']
 train_x example: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1] 
train_y example: [0, 0, 1, 0, 0, 0, 0, 0, 0]
 p = bow("is your shop open today?", words)
print (p)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]
 classify('is your shop open today?')
[('opentoday', 0.9264171123504639)]
 response('is your shop open today?')
Our hours are 9am-9pm every day
 response('do you take cash?')
We accept VISA, Mastercard and AMEX
 response('what kind of mopeds do you rent?')
We rent Yamaha, Piaggio and Vespa mopeds
 response('Goodbye, see you later')
Bye! Come back again soon.
 response('we want to rent a moped')
Are you looking to rent today or later this week?
 response('today')
Same-day rentals please call 1-800-MYMOPED
 context
{'123': 'rentalday'}
 response("Hi there!", show_details=True)
context: ''
tag: greeting
Good to see you again
 response('today')
We're open every day from 9am-9pm
 classify('today')
[('today', 0.5322513580322266), ('opentoday', 0.2611265480518341)]
 response("thanks, your great")
Happy to help!
 
 
Item 49: 
Name            | Sym.  | Description                              
----------------|-------|------------------------------------------
Bitcoin         | BTC   | Digital gold                             
Ethereum        | ETH   | Programmable contracts and money         
Bitcoin Cash    | BCH   | Bitcoin clone                            
Ripple          | XRP   | Enterprise payment settlement network    
Litecoin        | LTC   | Faster Bitcoin                           
Dash            | DASH  | Privacy-focused Bitcoin clone            
NEO             | NEO   | Chinese-market Ethereum                  
NEM             | XEM   | Batteries-included digital assets        
Monero          | XMR   | Private digital cash                     
Ethereum Classic| ETC   | Ethereum clone                           
IOTA            | MIOTA | Internet-of-things payments              
Qtum            | QTUM  | Ethereum contracts on Bitcoin            
OmiseGO         | OMG   | Banking, remittance, and exchange        
Zcash           | ZEC   | Private digital cash                     
BitConnect      | BCC   | Madoff-like investment fund              
Lisk            | LSK   | Decentralized applications in JavaScript 
Cardano         | ADA   | Layered currency and contracts           
Tether          | USDT  | Price = 1 USD                            
Stellar Lumens  | XLM   | Digital IOUs                             
EOS             | EOS   | Decentralized applications on WebAssembly
Hshare          | HSR   | Blockchain switchboard                   
Waves           | WAVES | Decentralized exchange and crowdfunding  
Stratis         | STRAT | Decentralized applications in C#         
Komodo          | KMD   | Decentralized ICOs                       
Ark             | ARK   | Blockchain switchboard
 
 
Item 52: 
python detect.py --scales 1 --images imgs/img3.jpg
 python detect.py --scales 2 --images imgs/img3.jpg
 python detect.py --scales 3 --images imgs/img3.jpg
 python detect.py --reso 320 --images imgs/imgs4.jpg
 python detect.py --reso 416 --images imgs/imgs4.jpg
 python detect.py --reso 608 --images imgs/imgs4.jpg
 python detect.py --reso 960 --images imgs/imgs4.jpg 
 
 
Item 53: 
console.log(execute("These aren't real. Kanye would never write Yeezy on the side"));
 console.log(execute("Whether we are Republican or Democrat, we must now focus on strengthening Background Checks!"));
 
 
Item 56: 
pm.traceplot(sleep_trace, ['alpha', 'beta'])
pm.autocorrplot(sleep_trace, ['alpha', 'beta'])
 9:30  PM probability of being asleep: 4.80%.
10:00 PM probability of being asleep: 27.44%.
10:30 PM probability of being asleep: 73.91%.
 The probability of sleep increases to above 50% at 10:14 PM.
 Probability of being awake at 5:30 AM: 14.10%. 
Probability of being awake at 6:00 AM: 37.94%. 
Probability of being awake at 6:30 AM: 69.49%.
 The probability of being awake passes 50% at 6:11 AM.
 Probability of at least 6.5 hours of sleep = 99.16%.
Probability of at least 8.0 hours of sleep = 44.53%.
Probability of at least 9.0 hours of sleep = 10.94%.
 The most likely duration of sleep is 7.67 hours.
 
 
Item 57: 
Recommend & share..
 
 
Item 61: 
{
  ...
  "action": "book_flight", 
  "parameters": {
    "destination": "Paris"
  }
  ...
}
 this.ws$.takeUntil(this.ngUnsubscribe$)
  .filter(r => r.type === 'bot')
  .retryWhen(err$ =>
    Observable.zip(err$, Observable.range(1, 3), (e, n) => n)
      .mergeMap(retryCount => Observable.timer(1000 * retryCount))
  )
  .delayWhen(inp => Observable.interval(100 + inp.msg.length * 10))
  .subscribe(
    (msg) => this.pushMsg(msg)
  );
 
 
Item 66: 
1. Compute a distance value between the item to be classified and every item in the training data-set
 2. Pick the k closest data points (the items with the k lowest distances)
 3. Conduct a “majority vote” among those data points — the dominating classification in that pool is decided as the final classification
 
 
Item 68: 
START
Generate the initial population
Compute fitness
REPEAT
    Selection
    Crossover
    Mutation
    Compute fitness
UNTIL population has converged
STOP
 
 
Item 72: 
[“The quick brown fox jumped over the lazy dog 42 times.”, “The dog is lazy”]
 [“the quick brown fox jumped over the lazy dog *number* times”, “the dog is lazy”]
 [[‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jumped’, ‘over’, ‘the’, ‘lazy’, ‘dog’, ‘*number*’, ‘times’], [‘the’, ‘dog’, ‘is’, ‘lazy’]]
 [[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [2, 9, 12, 8]]
 [[2, 3, 4, 5, 6, 7, 2, 8, 9, 10, 11], [0, 0, 0, 0, 0, 0, 0, 2, 9, 12, 8]]
 
 
Item 73: 
Training Step: 2000  | total loss: 0.00072 | time: 0.002s
| SGD | epoch: 2000 | loss: 0.00072 -- iter: 4/4
Testing OR operator
0 or 0: [[ 0.04013482]]
0 or 1: [[ 0.97487926]]
1 or 0: [[ 0.97542077]]
1 or 1: [[ 0.99997282]]
 git clone https://github.com/emilwallner/Deep-Learning-101.git
 cd Deep-Learning-101
floyd init 101
 floyd init 101
 floyd run --data emilwallner/datasets/cifar-10/1:data --gpu --tensorboard --mode jupyter
 
 
Item 76: 
idxs, dists = self.search_index.knnQuery(query_vector, k=k)
 
 
Item 81: 
item {
 id: 1
 name: ‘toy’
}
 
 
Item 82: 
This series is available as a full-length e-book! Download here. Free for download, contributions appreciated (paypal.me/ml4h)
 A good example of classification is determining whether a loan application is fraudulent.
 Ultimately, the lender wants to know whether they should give the borrower a loan or not, and they have some tolerance for risk that the application is in fact fraudulent. In this case, the goal of logistic regression is to calculate the probability (between 0% and 100%) that the application is fraud. With these probabilities, we can set some threshold above which we’re willing to lend to the borrower, and below which we deny their loan application or flag the application for further review.
 
 
Item 85: 
Recommend & share..
 
 
Item 86: 
Out: 116805.0
 Out: 98423.0
 0.11652680227350454 0.002677697431105852 0.05820973606789237
CPU times: user 1.96 s, sys: 17.3 ms, total: 1.98 s
Wall time: 2 s
 0.012841445048055122 0.04883371471892228 0.00943678056045777
CPU times: user 3.03 s, sys: 24.8 ms, total: 3.05 s
Wall time: 3.11 s
 Out: 54
 CPU times: user 41.3 s, sys: 2.76 s, total: 44.1 s
Wall time: 37.4 s
 
 
Item 94: 
Recommend & share..
 
 
Item 102: 
{'START': ['i', 'you'], 'i': ['like'], 'like': ['to'], 'to': ['eat'], 'you': ['eat'], 'eat': ['apples', 'oranges'], 'END': ['apples', 'oranges']}
 {'START': ['i', 'we', 'you'], 'i': ['like'], 'like': ['to'], 'to': ['eat'], 'you': ['eat'], 'we': ['eat'], 'eat': ['apples', 'oranges', 'apples'], 'END': ['apples', 'oranges', 'apples']}
 
 
Item 109: 
cv2.CV_RETR_EXTERNAL — get only outer contours.
cv2.CV_CHAIN_APPROX_TC89_L1 - use Teh-Chin chain approximation algorithm (faster)
 time = ((self.start_time + int(frame_number / self.fps)) * 100 
        + int(100.0 / self.fps) * (frame_number % self.fps))
 
 
Item 119: 
T(n) = T(n/2) + c
 swapped = true
      while swapped
        swapped = false
        for j from 0 to N - 1
           if a[j] > a[j + 1]
              swap( a[j], a[j + 1] )
              swapped = true
 
 
Item 121: 
rf [ 0.23884372  0.76115628]
ab [ 0.49269671  0.50730329]
gb [ 0.1271846  0.8728154]
knn [ 0.71428571  0.28571429]
nb [ 0.11928973  0.88071027]
gnb [ 0.9265792  0.0734208]
lr [ 0.35540594  0.64459406]
rf [1]
ab [1]
gb [1]
knn [0]
nb [1]
gnb [0]
svc [1]
svm [0]
lr [1]
 ([1], [ 0.15384615,  0.84615385])
 
 
Item 123: 
+--------+-----------------+
| Input  |  Desired output |
+--------+-----------------+
|     0  |               0 |
|     1  |               2 |
|     2  |               4 |
|     3  |               6 |
|     4  |               8 |
+--------+-----------------+
 +--------+------------------------------------+
| Input  |  Actual output of model 1 (y= 3.x) |
+--------+------------------------------------+
|     0  |                                  0 |
|     1  |                                  3 |
|     2  |                                  6 |
|     3  |                                  9 |
|     4  |                                 12 |
+--------+------------------------------------+
 +--------+-----------------+-----------------+
| Input  |  Actual output  |  Desired output |
+--------+-----------------+-----------------+
|     0  |              0  |               0 |
|     1  |              3  |               2 |
|     2  |              6  |               4 |
|     3  |              9  |               6 |
|     4  |              12 |               8 |
+--------+-----------------+-----------------+
 +--------+----------+-----------+------------------+---------------+
| Input  |  actual  |  Desired  |  Absolute Error  |  Square Error |
+--------+----------+-----------+------------------+---------------+
| 0      |       0  |        0  |               0  |             0 |
| 1      |       3  |        2  |               1  |             1 |
| 2      |       6  |        4  |               2  |             4 |
| 3      |       9  |        6  |               3  |             9 |
| 4      |      12  |        8  |               4  |            16 |
| Total: |        - |         - |               10 |            30 |
+--------+----------+-----------+------------------+---------------+
 +--------+----------+-------+-----------+------------+---------+
| Input  |  Output  |  W=3  |  rmse(3)  |  W=3.0001  |   rmse  |
+--------+----------+-------+-----------+------------+---------+
| 0      |       0  |    0  |        0  |         0  |       0 |
| 1      |       2  |    3  |        1  |    3.0001  |  1.0002 |
| 2      |       4  |    6  |        4  |    6.0002  |  4.0008 |
| 3      |       6  |    9  |        9  |    9.0003  |  9.0018 |
| 4      |       8  |   12  |       16  |   12.0004  | 16.0032 |
| Total: |        - |     - |       30  |         -  |  30.006 |
+--------+----------+-------+-----------+------------+---------+
 
 
Item 124: 
Recommend & share..
 
 
Item 132: 
time python standard_res_conversion.py
 with concurrent.futures.ProcessPoolExecutor() as executor:
 executor.map(load_and_resize, image_files)
 time python fast_res_conversion.py
 
 
Item 134: 
J(G) = alpha * J_content(C,G) + beta * J_style(S,G)
 
 
Item 137: 
item {
  id: 1
  name: 'waldo'
}
 python export_inference_graph.py — pipeline_config_path PATH_TO_PIPELINE_CONFIG --trained_checkpoint_prefix PATH_TO_CHECPOINT --output_directory OUTPUT_PATH
 python find_wally.py
 python find_wally_pretty.py
 
 
Item 139: 
RMSE: 8.052
MAE:  6.468
 
 
Item 142: 
dd = ddf.from_pandas(df, npartitions=N)
 204.313940048 seconds for get_big_mean
39.7543280125 seconds for get_big_mean_old
 131.600986004 seconds for get_big_max
43.7621600628 seconds for get_big_max_old
 120.027213097 seconds for get_big_sum
7.49701309204 seconds for get_big_sum_old
 0.581165790558 seconds for filter_df
226.700095892 seconds for filter_df_old
 369.541605949 seconds for apply_random
157.643756866 seconds for apply_random_old
 3.08352184296 seconds for get_big_mean
1.3314101696 seconds for get_big_max
1.21639800072 seconds for get_big_sum
0.228978157043 seconds for filter_df
 112.135010004 seconds for apply_random
50.2007009983 seconds for value_count_test
 
 
Item 145: 
conda install -c conda-forge scrapy
 scrapy startproject fundrazr
 scrapy shell 'https://fundrazr.com/find?category=Health'
 scrapy shell “https://fundrazr.com/find?category=Health"
 response.xpath("//h2[contains(@class, 'title headline-font')]/a[contains(@class, 'campaign-link')]//@href").extract()
 scrapy shell 'https://fundrazr.com/savemyarm'
 scrapy shell “https://fundrazr.com/savemyarm"
 response.xpath("//div[contains(@id, 'campaign-title')]/descendant::text()").extract()[0]
 response.xpath("//span[contains(@class,'stat')]/span[contains(@class, 'amount-raised')]/descendant::text()").extract()
 response.xpath("//div[contains(@class, 'stats-primary with-goal')]//span[contains(@class, 'stats-label hidden-phone')]/text()").extract()
 response.xpath("//div[contains(@class, 'stats-primary with-goal')]/@title").extract()
 response.xpath("//div[contains(@id, 'campaign-stats')]//span[contains(@class,'stats-label hidden-phone')]/span[@class='nowrap']/text()").extract()
 response.xpath("//div[contains(@class, 'stats-secondary with-goal')]//span[contains(@class, 'donation-count stat')]/text()").extract()
 response.xpath("//div[contains(@id, 'full-story')]/descendant::text()").extract()
 response.xpath("//meta[@property='og:url']/@content").extract()
 exit()
 scrapy crawl my_scraper -o MonthDay_Year.csv
 
 
Item 146: 
the quick brown fox jumped over the lazy dog
 ([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...
 (quick, the), (quick, brown), (brown, quick), (brown, fox), ...
 
 
Item 147: 
t1=time.time()
for item in l1:
    l2.append(lg10(item))
t2 = time.time()
print("With for loop and appending it took {} seconds".format(t2-t1))
speed.append(t2-t1)
 t1=time.time()
a2=np.log10(a1)
t2 = time.time()
print("With direct Numpy log10 method it took {} seconds".format(t2-t1))
speed.append(t2-t1)
 
 
Item 148: 
d_loss = d_loss_real + d_loss_fake 
 
 
Item 150: 
function tableOf(n) {
 //rest of the code
 }
 for(i = 0; i < 11; i++) {
 console.log(n, 'x', i, '=', n*i);
 }
 function tableOf(n) {
 for(i = 0; i < 11; i++) {
 console.log(n, 'x', i, '=', n*i);
 }
 }
 tableOf(2);
 
 
Item 160: 
code = Dense(code_size, activation='relu')(input_img)
 code = Dense(code_size, activation='relu', activity_regularizer=l1(10e-6))(input_img)
 
 
Item 161: 
What are you - What are you doing? [Jonathan] Documenting.
Oh, why? - Because - [Joyce chuckles] - It looks great. - [Joyce] Oh, this is just so overcooked. - And look, the potatoes are runny.
- [Jonathan] Mom. - [Joyce] They're so runny.
- [Jonathan chuckles] Mom, it's gonna be great.
 [Horns honking] Shit! Shit! Shit! Shit! Shit! [Cackles] [Exhales] Okay. Okay.
[Police sirens wailing] - Son of a bitch! We got more! - [Mick] Oh, shit! They're headed down 7th.
 [Steve] Get out.
[Dustin] Yes! Kick his ass, Steve! - [Mike] Get him! - [Dustin] Murder the son of a bitch! - [Dustin] Now! Now! - [Mike] Get that shithead! - [Dustin] Kill the son of a bitch! - [Lucas] Steve! - [Max] Billy! - [Mike] Holy shit! Shit!
 [Billy preens]
 
 
Item 165: 
- keras
- matplotlib
- tensorflow
- tqdm
 
 
Item 167: 
_gameBoard: [
[“”, “”, “”],
[“”, “”, “”],
[“”, “”, “”]
]
 
 
Item 168: 
Recommend & share..
 
 
Item 169: 
1.compute the entropy for data-set
 2.for every attribute/feature:
       1.calculate entropy for all categorical values
       2.take average information entropy for the current attribute
       3.calculate gain for the current attribute
 3. pick the highest gain attribute.
4. Repeat until we get the tree we desired.
 Actual=1 predicted 1
1 0 , 0,1, 0 0
 P(Target=1).P(Target=1) + P(Target=1).P(Target=0) + P(Target=0).P(Target=1) + P(Target=0).P(Target=0) = 1
 P(Target=1).P(Target=0) + P(Target=0).P(Target=1) = 1 — P^2(Target=0) — P^2(Target=1)
 1.compute the gini index for data-set
 2.for every attribute/feature:
       1.calculate gini index for all categorical values
       2.take average information entropy for the current attribute 
       3.calculate the gini gain
 3. pick the best gini gain attribute.
4. Repeat until we get the tree we desired.
 
 
Item 170: 
Satisfaction Level = 0.23 
Average Monthly Hours = 200 
Last Evaluation = 0.5
 0.24 ::: Baseline 
+0.28 ::: Satisfaction Level (prediction is now 0.52) 
-0.09 ::: Average Monthly Hours (prediction is now 0.43) 
-0.12 ::: Last Evaluation (prediction is now 0.31)
 = 0.31 ::: Prediction
 -1.41 ::: Baseline (Intercept) 
-1.10 ::: Satisfaction Level (prediction is now -2.51) 
+0.98 ::: Last Evaluation (prediction is now -1.53) 
+0.32 ::: Time Spent At Company (prediction is now - 1.21) 
+0.27 ::: Hours Average Monthly (prediction is now -0.94) 
-0.24 ::: Sales (prediction is now -1.18) 
-0.18 ::: Number of Projects (prediction is now -1.36)
+0.11 ::: Work Accident (prediction is now -1.25) 
-0.07 ::: Salary (prediction is now -1.32) 
+0.02 ::: Promotion Last 5 Years (prediction is now -1.30)
 = -1.299 ::: Prediction
 install.packages("devtools") 
library(devtools) 
install_github("AppliedDataSciencePartners/xgboostExplainer")
 library(data.table)
library(rpart)
library(rpart.plot)
library(caret)
library(xgboost)
library(pROC)
 set.seed(123)
 full = fread('./data/HR.csv', stringsAsFactors = T)
full = full[sample(.N)]
 #### Add Random Noise
 tmp_std = sd(full[,satisfaction_level])
full[,satisfaction_level:=satisfaction_level + runif(.N,-tmp_std,tmp_std)]
full[,satisfaction_level:=satisfaction_level - min(satisfaction_level)]
full[,satisfaction_level:=satisfaction_level / max(satisfaction_level)]
 tmp_std = sd(full[,last_evaluation])
full[,last_evaluation:=last_evaluation + runif(.N,-tmp_std,tmp_std) ]
full[,last_evaluation:=last_evaluation - min(last_evaluation)]
full[,last_evaluation:=last_evaluation / max(last_evaluation)]
 tmp_min = min(full[,number_project])
tmp_std = sd(full[,number_project])
full[,number_project:=number_project + sample(-ceiling(tmp_std):ceiling(tmp_std),.N, replace=T)]
full[,number_project:=number_project - min(number_project) + tmp_min]
 tmp_min = min(full[,average_montly_hours])
tmp_std = sd(full[,average_montly_hours])
full[,average_montly_hours:=average_montly_hours + sample(-ceiling(tmp_std):ceiling(tmp_std),.N, replace=T)]
full[,average_montly_hours:=average_montly_hours - min(average_montly_hours) + tmp_min]
 tmp_min = min(full[,time_spend_company])
tmp_std = sd(full[,time_spend_company])
full[,time_spend_company:=time_spend_company + sample(-ceiling(tmp_std):ceiling(tmp_std),.N, replace=T)]
full[,time_spend_company:=time_spend_company - min(time_spend_company) + tmp_min]
 tmp_min = min(full[,number_project])
tmp_std = sd(full[,number_project])
full[,number_project:=number_project + sample(-ceiling(tmp_std):ceiling(tmp_std),.N, replace=T)]
full[,number_project:=number_project - min(number_project) + tmp_min]
 #### Create Train / Test and Folds
 train = full[1:12000]
test = full[12001:14999]
 cv <- createFolds(train[,left], k = 10)
# Control
ctrl <- trainControl(method = "cv",index = cv)
 #### Train Tree
 tree.cv <- train(x = train[,-"left"], y = as.factor(train[,left]), method = "rpart2", tuneLength = 7,
                 trControl = ctrl, control = rpart.control())
 tree.model = tree.cv$finalModel
rpart.plot(tree.model, type = 2,extra =  7,fallen.leaves = T)
rpart.plot(tree.model, type = 2,extra =  2,fallen.leaves = T)
 tree.preds = predict(tree.model, test)[,2]
tree.roc_obj <- roc(test[,left], tree.preds)
 cat("Tree AUC ", auc(tree.roc_obj))
 #### Train XGBoost
 xgb.train.data = xgb.DMatrix(data.matrix(train[,-'left']), label = train[,left], missing = NA)
 param <- list(objective = "binary:logistic", base_score = 0.5)
xgboost.cv = xgb.cv(param=param, data = xgb.train.data, folds = cv, nrounds = 1500, early_stopping_rounds = 100, metrics='auc')
best_iteration = xgboost.cv$best_iteration
 xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=best_iteration)
 xgb.test.data = xgb.DMatrix(data.matrix(test[,-'left']), missing = NA)
xgb.preds = predict(xgb.model, xgb.test.data)
xgb.roc_obj <- roc(test[,left], xgb.preds)
 cat("Tree AUC ", auc(tree.roc_obj))
cat("XGB AUC ", auc(xgb.roc_obj))
 #### Xgb importance
col_names = attr(xgb.train.data, ".Dimnames")[[2]]
 imp = xgb.importance(col_names, xgb.model)
xgb.plot.importance(imp)
 #### THE XGBoost Explainer
 library(xgboostExplainer)
 explainer = buildExplainer(xgb.model,xgb.train.data, type="binary", base_score = 0.5, trees_idx = NULL)
pred.breakdown = explainPredictions(xgb.model, explainer, xgb.test.data)
 cat('Breakdown Complete','\n')
weights = rowSums(pred.breakdown)
pred.xgb = 1/(1+exp(-weights))
cat(max(xgb.preds-pred.xgb),'\n')
 idx_to_get = as.integer(802)
test[idx_to_get,-"left"]
showWaterfall(xgb.model, explainer, xgb.test.data, data.matrix(test[,-'left']) ,idx_to_get, type = "binary")
 ####### IMPACT AGAINST VARIABLE VALUE
plot(test[,satisfaction_level], pred.breakdown[,satisfaction_level], cex=0.4, pch=16, xlab = "Satisfaction Level", ylab = "Satisfaction Level impact on log-odds")
 plot(test[,last_evaluation], pred.breakdown[,last_evaluation], cex=0.4, pch=16, xlab = "Last evaluation", ylab = "Last evaluation impact on log-odds")
 cr <- colorRamp(c("blue", "red"))
plot(test[,last_evaluation], pred.breakdown[,last_evaluation], col = rgb(cr(round(test[,satisfaction_level])), max=255), cex=0.4, pch=16, xlab = "Last evaluation", ylab = "Last evaluation impact on log-odds")
 
 
Item 175: 
X1    x2    |  Y
==================
-2    0        1
 2    0        1
-1    0        0
 1    0        0
 plt.scatter([-2, 2], [0, 0 ], c='b')
plt.scatter([-1, 1], [0, 0 ], c='r')
 X1    x1^2  |  Y
==================
-2    4       1
 2    4       1
-1    1       0
 1    1       0
 
 
Item 180: 
Accuracy: 0.88889
 Accuracy: 0.91111
 1 component: 92.46% of initial variance 0.362 x sepal length (cm) + -0.082 x sepal width (cm) + 0.857 x petal length (cm) + 0.359 x petal width (cm) 
2 component: 5.3% of initial variance 0.657 x sepal length (cm) + 0.730 x sepal width (cm) + -0.176 x petal length (cm) + -0.075 x petal width (cm)
 Projecting 64-dimensional data to 2D
 CPU times: user 29.3 s, sys: 2.97 s, total: 32.3 s Wall time: 32.3 s
 
 
Item 181: 
Machine Learning  
Cloud Machine Learning Engine -- Managed ML (TensorFlow)
Cloud Job Discovery -- ML Job Search/Discovery
Cloud Natural Language -- Text Parsing and Analysis
Cloud Speech -- Convert Speech to Text
Cloud Translation -- Language Detection and Translation
Cloud Vision -- Image Recognition and Classification
Cloud Video Intelligence -- Scene-level Video Annotation
  
Internet of Things  
Cloud IoT Core -- Data Ingestion/Device Management
  
Big Data  
BigQuery -- Data Warehouse/Analytics
Cloud Dataflow -- Stream/batch data processing
Cloud Dataproc -- Managed Spark and Hadoop
Cloud Datalab -- Visualize and Explore Data
Cloud Dataprep -- Transform/Clean Raw Data
Cloud Pub/Sub -- Global Real-time Messaging
Genomics -- Managed Genomics Platform
Data Studio -- Collaborative Dashboards
  
Databases  
Cloud SQL -- Managed MySQL and PostgreSQL
Cloud Bigtable -- HBase compatible nonrelational
Cloud Datastore -- Horizontally scalable nonrelational
Cloud Spanner -- Horizontally scalable relational
  
Storage  
Cloud Storage -- Object Storage and Serving
Nearline -- Archival Storage
Coldline -- Archival Cold Storage
Persistent Disk -- VM-attached disks
  
Data Transfer  
Google Transfer Appliance -- Hardware for Data Migration
Cloud Storage Transfer Service -- Cloud to Cloud Transfers
BigQuery Data Transfer Service -- Bulk Import Analytics Data
  
Compute  
Compute Engine -- Virtual Machines, Disks, Network
App Engine -- Managed App Platform
Container Engine -- Managed Kubernetes/Containers
Cloud Functions -- Event-driven serverless applications
  
Networking  
Virtual Private Cloud -- Software Defined Networking
Cloud Load Balancing -- Multi-region Load Distribution
Cloud CDN -- Content Delivery Network
Cloud DNS -- Programmable DNS Serving
Dedicated Interconnect -- Dedicated private network connection
IPsec VPN -- Virtual private network connection
Direct Peering -- Peer with GCP
Carrier Peering -- Peer with a carrier
  
Identity and Security  
Cloud IAM -- Resource Access Control
Cloud Identity-Aware Proxy -- Identity-based App Signin
Cloud Data Loss Prevention API -- Classify, Redact Sensitive Data
Cloud Key Management Service -- Hosted Key Management Service
Cloud Resource Manager -- Cloud Project Metadata Management
Cloud Security Scanner -- App Engine Security Scanner
Security Key Enforcement -- Two-step Key Verification
  
Management Tools  
Stackdriver Monitoring -- Infrastructure and Application Monitoring
Stackdriver Logging -- Centralized Logging
Stackdriver Error Reporting -- App Error Reporting
Stackdriver Trace -- App Performance Insights
Stackdriver Debugger -- Live Production Debugging
Cloud Deployment Manager -- Templated Infrastructure Deployment
Cloud Console -- Web-based Management Console
Cloud Shell -- Browser-based Terminal/CLI
Cloud Mobile App -- iOS/Android GCP Manager App
Cloud Billing API -- Programmatically Manage GCP Billing
Cloud APIs -- APIs for Cloud Services
  
Developer Tools  
Cloud SDK -- CLI for GCP
Container Registry -- Private Container Registry/Storage
Container Builder -- Build/Package Container Artifacts
Cloud Source Repositories -- Hosted Private Git Repos
Cloud Tools for Android Studio -- Android Studio GCP Tools
Cloud Tools for IntelliJ -- IntelliJ GCP Tools
Cloud Tools for PowerShell -- PowerShell GCP Tools
Cloud Tools for Visual Studio -- Visual Studio GCP Tools
Cloud Tools for Eclipse -- Eclipse GCP Tools
Gradle App Engine Plugin -- Gradle App Engine Plugin
Maven App Engine Plugin -- Maven App Engine Plugin
  
API Platform and Ecosystems  
Apigee API Platform -- Develop, secure, monitor APIs
Apigee Sense -- API protection from attacks
API Monetization -- Monetize APIs
API Analytics -- API Metrics
Cloud Endpoints -- Cloud API Gateway
  
Mobile (Firebase)  
Realtime Database -- Real-time Data Synchronization
Cloud Firestore -- Document Store and Sync
Cloud Storage -- File Storage and Serving
Hosting -- Web Hosting with CDN/SSL
Authentication -- Drop-in Authentication
Cloud Functions -- Event-driven Serverless Applications
Test Lab for Android -- Mobile Device Testing Service
Performance Monitoring -- App Performance Monitoring
Crashlytics -- Crash Reporting and Analytics
Cloud Messaging -- Send Messages to Devices
See all Firebase products
 
 
Item 185: 
Recommend & share..
 
 
Item 187: 
Accuracy of Model X = 4/5 * 100 = 80%
 No Breast cancer = 70/100 * 900 = 630
Breast cancer= 70/100 * 100 = 70
 No Breast cancer = 30/100 * 900 = 270
Breast cancer= 30/100 * 100 = 30
 Hawkins Accuracy = 270 / 300 = 0.9
 Number of women with breast cancer and classified as no breast cancer (FN) = 30
 Number of women with breast cancer and classified as breast cancer(TP) = 0
 Number of women without breast cancer and classified as no breast cancer (TN) = 270
 Number of women without breast cancer and classified as breast cancer (FP) = 
 Accuracy = (TP + TN) / (TP+TN+FP+FN)
Accuracy = (0 + 270) / (0 + 270 + 0 + 30)= 0.90
Accuracy in % = 90%
 Precision = TP / (TP + FP)
Precision = 0 / (0 + 0) = 0
Precision in % = 0%
 Recall = TP / (TP + FN)
Recall = 0 / (0 + 30) = 0
Recall in % = 0%
 Number of women with breast cancer and classified as no breast cancer (FN) = 10
 Number of women with breast cancer and classified as breast cancer(TP) = 20
 Number of women without breast cancer and classified as no breast cancer (TN) = 200
 Number of women without breast cancer and classified as breast cancer (FP) = 70
 Accuracy = (TP + TN) / (TP + TN + FP + FN)
Accuracy = (20 + 200) / (20 + 200 + 10 + 70) = 0.733
Accuracy in % = 73.3%
 Precision = TP / (TP + FP)
Precision = 20 / (20 + 70) = 0.222
Precision in % = 22.2%
 Recall = TP / (TP + FN)
Recall = 20/ (20 + 10) = 0.67
Recall in % = 67%
 
 
Item 192: 
Recommend & share..
 
 
Item 193: 
{
  "text": "show me chinese restaurants",
  "intent": "restaurant_search",
  "entities": [
    {
      "start": 8,
      "end": 15,
      "value": "chinese",
      "entity": "cuisine"
    }
  ]
}
 {
    "text": "I am looking for Chinese food",
    "entities": [
        {"start": 8, "end": 15, "value": "chinese", "entity": "cuisine", "extractor": "ner_crf"}
    ],
    "intent": {"confidence": 0.6485910906220309, "name": "restaurant_search"},
    "intent_ranking": [
        {"confidence": 0.6485910906220309, "name": "restaurant_search"},
        {"confidence": 0.1416153159565678, "name": "affirm"}
    ]
}
 
 
Item 194: 
Recommend & share..
 
 
Item 197: 
Recommend & share..
 
 
Item 202: 
model <- lm(y ~ x1 + x2, data = df)
 
 
Item 204: 
nine_period_high = pd.rolling_max(pd.DataFrame(highp), window= ROLLING / 2)
nine_period_low = pd.rolling_min(pd.DataFrame(lowp), window=  ROLLING / 2)
ichimoku = (nine_period_high + nine_period_low) /2
ichimoku = ichimoku.replace([np.inf, -np.inf], np.nan)
ichimoku = ichimoku.fillna(0.).values.tolist()
 macd_indie = moving_average_convergence(pd.DataFrame(closep))
 wpr = williams_percent_r(closep)
rsi = relative_strength_index(closep,  ROLLING / 2)
 volatility1 = pd.DataFrame(closep).rolling(ROLLING).std().values#.tolist()
volatility2 = pd.DataFrame(closep).rolling(ROLLING).var().values#.tolist()
 volatility = volatility1 / volatility2
volatility = [v[0] for v in volatility]
 rolling_skewness = pd.DataFrame(closep).rolling(ROLLING).skew().values 
rolling_kurtosis = pd.DataFrame(closep).rolling(ROLLING).kurt().values
 main_input = Input(shape=(len(X[0]), ), name='main_input')
x = GaussianNoise(0.05)(main_input)
x = Dense(64, activation='relu')(x)
x = GaussianNoise(0.05)(x)
output = Dense(1, activation = "linear", name = "out")(x)
 final_model = Model(inputs=[main_input], outputs=[output])
opt = Adam(lr=0.002)
final_model.compile(optimizer=opt, loss='mse')
 
 
Item 205: 
select *
from table1
left join (foo) as subquery1
using (account_id)
 with cte1 as(foo)
 select * from table1
left join cte1
using (account_id)
 select count(distinct customer_id) as ireland_trials
from trials
where country = ‘Ireland’
 sum(revenue)
 sum(case when lead_source = 'paid' then revenue else null end) as paid_revenue
 select count(distinct customer_id) as trials,
count(distinct case when country = ‘Ireland’ then customer_id else null end) as ireland_trials,
count(distinct case when continent = ‘Europe’ then customer_id else null end) as europe_trials
from trials
 
 
Item 216: 
new_weight = existing_weight — learning_rate * gradient
 Less training time, lesser money spent on GPU cloud compute. :)
 
 
Item 228: 
+-----------+----------+---------+
|   input1  |  input2  |  output |
+-----------+----------+---------+
|     1     |     1    |    0    |
|     1     |     0    |    1    |
|     0     |     1    |    1    |
|     0     |     0    |    0    |
+-----------+----------+---------+
 def plot_data(data, labels):
    """
    argument:
    data: np.array containing the input value
    labels: 1d numpy array containing the expected label
    """
     positives = data[labels == 1, :]
    negatives = data[labels == 0, :]
    plt.scatter(positives[:, 0], positives[:, 1], 
                       color='red', marker='+', s=200)
    plt.scatter(negatives[:, 0], negatives[:, 1], 
                       color='blue', marker='_', s=200)
 
positives = np.array([[1, 0], [0, 1]])
negatives = np.array([[0, 0], [1, 1]])

data = np.concatenate([positives, negatives])
labels = np.array([1, 1, 0, 0])
 plot_data(data, labels)
 
 
Item 231: 
implementation 'com.google.firebase:firebase-ml-vision:16.0.0'
 <meta-data
      android:name="com.google.firebase.ml.vision.DEPENDENCIES"
      android:value="face" />
 val options = FirebaseVisionFaceDetectorOptions.Builder()
 .setModeType(FirebaseVisionFaceDetectorOptions.ACCURATE_MODE)
 .setModeType(FirebaseVisionFaceDetectorOptions.FAST_MODE)
 .setLandmarkType(FirebaseVisionFaceDetectorOptions.ALL_LANDMARKS)
 .setLandmarkType(FirebaseVisionFaceDetectorOptions.NO_LANDMARKS)
 .setClassificationType(FirebaseVisionFaceDetectorOptions.ALL_CLASSIFICATIONS)
 .setClassificationType(FirebaseVisionFaceDetectorOptions.NO_CLASSIFICATIONS)
 .setMinFaceSize(0.15f)
 .setTrackingEnabled(true)
 .setTrackingEnabled(false)
 val options = FirebaseVisionFaceDetectorOptions.Builder()
        .setModeType(FirebaseVisionFaceDetectorOptions.FAST_MODE)
        .setLandmarkType(
            FirebaseVisionFaceDetectorOptions.ALL_LANDMARKS)      
        .setClassificationType(
            FirebaseVisionFaceDetectorOptions.ALL_CLASSIFICATIONS)
        .setMinFaceSize(0.15f)
        .setTrackingEnabled(true)
        .build()
 val image = FirebaseVisionImage.fromBitmap(bitmap);
 val image = FirebaseVisionImage.fromMediaImage(mediaImage,    
                rotation);
 FirebaseVisionImageMetadata metadata = new 
    FirebaseVisionImageMetadata.Builder()
        .setWidth(1280)
        .setHeight(720)
        .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21)
        .setRotation(rotation)
        .build();
 val image = FirebaseVisionImage.fromByteBuffer(buffer, metadata);
 val image = FirebaseVisionImage.fromByteArray(byteArray, metadata);
 val image: FirebaseVisionImage?
try {
    image = FirebaseVisionImage.fromFilePath(context, uri);
} catch (IOException e) {
    e.printStackTrace();
}
 val detector = FirebaseVision.getInstance()
        .getVisionFaceDetector(options);
 val options = FirebaseVisionFaceDetectorOptions.Builder()
        .setModeType(FirebaseVisionFaceDetectorOptions.FAST_MODE)
        .setLandmarkType(
            FirebaseVisionFaceDetectorOptions.ALL_LANDMARKS)      
        .setClassificationType(
            FirebaseVisionFaceDetectorOptions.ALL_CLASSIFICATIONS)
        .setMinFaceSize(0.15f)
        .setTrackingEnabled(true)
        .build()
 val result = detector.detectInImage(image)
        .addOnSuccessListener {
            
        }
        .addOnFailureListener {
         }
 val bounds = face.boundingBox
 val rotationY = face.headEulerAngleY
val rotationZ = face.headEulerAngleZ
 if (face.leftEyeOpenProbability !=
        FirebaseVisionFace.UNCOMPUTED_PROBABILITY) {
    val leftEyeOpenProb = face.leftEyeOpenProbability
}
 if (face.rightEyeOpenProbability !=
        FirebaseVisionFace.UNCOMPUTED_PROBABILITY) {
    val rightEyeOpenProb = face.rightEyeOpenProbability
}
 if (face.smilingProbability !=   
        FirebaseVisionFace.UNCOMPUTED_PROBABILITY) {
    val smileProb = face.smilingProbability
}
 val leftMouth = face.getLandmark(    
        FirebaseVisionFaceLandmark.LEFT_MOUTH)
 leftMouth?.let {
    val leftMouthPos = leftMouth.position
}
 val rightMouth = face.getLandmark(    
        FirebaseVisionFaceLandmark.RIGHT_MOUTH)
 rightMouth?.let {
    val rightMouthPos = rightMouth.position
}
 val bottomMouth = face.getLandmark(    
        FirebaseVisionFaceLandmark.BOTTOM_MOUTH)
 bottomMouth?.let {
    val bottomMouthPos = bottomMouth.position
}
 val leftEar = face.getLandmark(    
        FirebaseVisionFaceLandmark.LEFT_EAR)
 leftEar?.let {
    val leftEarPos = leftEar.position
}
 val rightEar = face.getLandmark(    
        FirebaseVisionFaceLandmark.RIGHT_EAR)
 rightEar?.let {
    val rightEarPos = rightEar.position
}
 val leftCheek = face.getLandmark(    
        FirebaseVisionFaceLandmark.LEFT_CHEEK)
 leftCheek?.let {
    val leftCheekPos = leftCheek.position
}
 val rightCheek = face.getLandmark(    
        FirebaseVisionFaceLandmark.RIGHT_CHEEK)
 rightCheek?.let {
    val rightCheekPos = rightCheek.position
}
 val noseBase = face.getLandmark(    
        FirebaseVisionFaceLandmark.NOSE_BASE)
 noseBase?.let {
    val noseBasePos = noseBase.position
}
 if (face.trackingId != FirebaseVisionFace.INVALID_ID) {
    val faceId = face.trackingId
}
 
 
Item 232: 
#import OpenCV module
import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
 #function to detect face
def detect_face (img):
 #convert the test image to gray image
gray = cv2.cvtColor (img, cv2.COLOR_BGR2GRAY)
 #load OpenCV face detector
face_cas = cv2.CascadeClassifier ('-File name.xml-')
faces = face_cas.detectMultiScale (gray, scaleFactor=1.3, minNeighbors=4);
 #if no faces are detected then return image
if (len (faces) == 0):
return None, None
 #extract the face
faces [0]=(x, y, w, h)
 #return only the face part
return gray[y: y+w, x: x+h], faces [0]
 #this function will read all persons' training images, detect face #from each image
#and will return two lists of exactly same size, one list
def prepare_training_data(data_folder_path):
 #------STEP-1--------
#get the directories (one directory for each subject) in data folder
dirs = os.listdir(data_folder_path)
faces = []
labels = []
for dir_name in dirs:
 #our subject directories start with letter 's' so
#ignore any non-relevant directories if any
if not dir_name.startswith("s"):
continue;
 #------STEP-2--------
#extract label number of subject from dir_name
#format of dir name = slabel
#, so removing letter 's' from dir_name will give us label
label = int(dir_name.replace("s", ""))
 #build path of directory containin images for current subject subject
#sample subject_dir_path = "training-data/s1"
subject_dir_path = data_folder_path + "/" + dir_name
 #get the images names that are inside the given subject directory
subject_images_names = os.listdir(subject_dir_path)
 #------STEP-3--------
#go through each image name, read image,
#detect face and add face to list of faces
for image_name in subject_images_names:
 #ignore system files like .DS_Store
if image_name.startswith("."):
continue;
 #build image path
#sample image path = training-data/s1/1.pgm
image_path = subject_dir_path + "/" + image_name
 #read image
image = cv2.imread(image_path)
 #display an image window to show the image
cv2.imshow("Training on image...", image)
cv2.waitKey(100)
 #detect face
face, rect = detect_face(image)
 #------STEP-4--------
#we will ignore faces that are not detected
if face is not None:
 #add face to list of faces
faces.append(face)
 #add label for this face
labels.append(label)
cv2.destroyAllWindows()
cv2.waitKey(1)
cv2.destroyAllWindows()
return faces, labels
 #let's first prepare our training data
#data will be in two lists of same size
#one list will contain all the faces
#and other list will contain respective labels for each face
print("Preparing data...")
faces, labels = prepare_training_data("training-data")
print("Data prepared")
 #print total faces and labels
print("Total faces: ", len(faces))
print("Total labels: ", len(labels))
 #create our LBPH face recognizer
face_recognizer = cv2.face.createLBPHFaceRecognizer()
 #train our face recognizer of our training faces
face_recognizer.train(faces, np.array(labels))
 #function to draw rectangle on image
#according to given (x, y) coordinates and
#given width and heigh
def draw_rectangle(img, rect):
(x, y, w, h) = rect
cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)
 #function to draw text on give image starting from
#passed (x, y) coordinates.
def draw_text(img, text, x, y):
cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)
 #this function recognizes the person in image passed
#and draws a rectangle around detected face with name of the subject
def predict(test_img):
 #make a copy of the image as we don't want to chang original image
img = test_img.copy()
 #detect face from the image
face, rect = detect_face(img)
 #predict the image using our face recognizer
label= face_recognizer.predict(face)
 #get name of respective label returned by face recognizer
label_text = subjects[label]
 #draw a rectangle around face detected
draw_rectangle(img, rect)
 #draw name of predicted person
draw_text(img, label_text, rect[0], rect[1]-5)
return img
 #load test images
test_img1 = cv2.imread("test-data/test1.jpg")
test_img2 = cv2.imread("test-data/test2.jpg")
 #perform a prediction
predicted_img1 = predict(test_img1)
predicted_img2 = predict(test_img2)
print("Prediction complete")
 #create a figure of 2 plots (one for each test image)
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
 #display test image1 result
ax1.imshow(cv2.cvtColor(predicted_img1, cv2.COLOR_BGR2RGB))
 #display test image2 result
ax2.imshow(cv2.cvtColor(predicted_img2, cv2.COLOR_BGR2RGB))
 #display both images
cv2.imshow("Tom cruise test", predicted_img1)
cv2.imshow("Shahrukh Khan test", predicted_img2)
cv2.waitKey(0)
cv2.destroyAllWindows()
cv2.waitKey(1)
cv2.destroyAllWindows()
 
 
Item 234: 
M <- as.matrix(UScitiesD)
 M <- as.matrix(dist(raw_data))
 n <- nrow(M)
C <- diag(n) - (1/n) * matrix(rep(1, n^2), nrow = n)
 B <- -(1/2) * C %*% M %*% C
 E <- eigen(B)$vectors[,1:2]
L <- diag(2) * eigen(B)$values[1:2]
 X <- E %*% L^(1/2)
plot(-X, pch=4)
text(-X, labels = rownames(M), cex = 0.5)
 data <- as.matrix(mtcars)
head(data)
dim(data)
 cor(mtcars$cyl, mtcars$hp)
 cor(data)
 e <- eigen(cor(data))
 e$values
barplot(e$values/sum(e$values),main="Proportion Variance explained")
 cumulative <- cumsum(e$values/sum(e$values))
print(cumulative)
i <- which(cumulative >= 0.9)[1]
print(i)
 W <- e$vectors[1:ncol(data),1:i]
 tD <- data %*% W
head(tD)
 cor(data, tD[,1:i])
 require(dplyr)
languages <- data.frame(HTML = c(22,20,15, 5, 5, 5, 0, 2, 0),
                  JavaScript = c(20,25,25,20,20,15, 5, 5, 0),
                        Java = c(15, 5, 0,15,30,30,10,10,15),
                      Python = c( 5, 0, 2, 5,10, 5,40,35,30),
job = c("Web","Web","Web","App","App","App","Data","Data","Data"))
 View(languages)
 cor(select(languages, -job))
 Web <- as.data.frame(scale(filter(languages, job == "Web") %>% select(., -job),T))
App <- as.data.frame(scale(filter(languages, job == "App") %>% select(., -job),T))
Data <- as.data.frame(scale(filter(languages, job == "Data") %>% select(., -job),T))
 within <- cov(Web) + cov(App) + cov(Data)
 means <- t(data.frame(
  mean_Web <- sapply(Web, mean),
  mean_App <- sapply(App, mean),
  mean_Data <- sapply(Data, mean)
))
 between <- cov(means)
 e <- eigen(solve(within) %*% between)
barplot(e$values/sum(e$values),main='Variance explained')
W <- e$vectors[,1:2]
 LDA <- scale(select(languages, -job), T) %*% W
plot(LDA, pch="", main='Linear Discriminant Analysis')
text(LDA[,1],LDA[,2],cex=0.75,languages$job, col=unlist(lapply(c(2,3,4),rep, 3)))
 cor(select(languages,-job),LDA)
 x <- y <- c(); a <- b <- 1
for(i in 1:1000){
    theta <- 0.01 * i
    x <- append(x,(a+b*theta)*(cos(theta)+runif(1,-1,1))
    y <- append(y,(a+b*theta)*(sin(theta)+runif(1,-1,1))
}
 color <- rainbow(1200)[1:1000]
spiral <- data.frame(x,y,color)
plot(y~x, pch=20, col=color)
 pc <- prcomp(spiral[,1:2])
plot(data.frame(pc$x[,1],1),col=as.character(spiral$color))
 n <- 5
distance <- as.matrix(dist(spiral[,1:2]))
A <- matrix(0,ncol=ncol(distance),nrow=nrow(distance))
for(i in 1:nrow(A)){
   neighbours <- as.integer(names(sort(distance[i,])[2:n+1]))
   A[i,neighbours] <- 1
}
 install.packages('igraph'); require(igraph)
graph <- graph_from_adjacency_matrix(A)
geo <- distances(graph, algorithm = 'dijkstra')
 md <- data.frame('scaled'=cmdscale(geo,1), 'color'=spiral$color)
plot(data.frame(md$scaled,1), col=as.character(md$color), pch=20)
 set.seed(100)
x <- y <- c(); a <- b <- 1
for(i in 1:1000){
    theta <- 0.02 * i
    x <- append(x,(a+b*theta)*(cos(theta)+runif(1,-1,1))
    y <- append(y,(a+b*theta)*(sin(theta)+runif(1,-1,1))
}
 color <- rainbow(1200)[1:1000]
spiral <- data.frame(x,y,color)
plot(y~x, pch=20, col=color)
 n <- 10
distance <- as.matrix(dist(spiral[,1:2]))
A <- matrix(0,ncol=ncol(distance),nrow=nrow(distance))
for(i in 1:nrow(A)){
   neighbours <- as.integer(names(sort(distance[i,])[2:n+1]))
   A[i,neighbours] <- 1
}
 for(j in 1:nrow(A)){
   for(k in 1:ncol(A)){
      if(A[j,k] == 1){
         out[k,j] <- 1
      }
   }
}
 D <- diag(nrow(A))
for(i in 1:nrow(D)){
   D[i,i] = sum(A[,i])
}
 L = D - A
 install.packages('geigen'); require(geigen)
eig <- geigen(L,D)
eig$values[1:10]
 embedding <- eig$vectors[,2]
plot(data.frame(embedding,1), col=spiral$colors, pch=20)
 
 
Item 236: 
Recommend & share..
 
 
Item 240: 
Popular Musical Acts: Acid Mothers Temple, John Maus, Ty Segall, Widowspeak, Acid Baby Jesus
 Popular Musical Acts: Lady Gaga, Rihanna, Katy Perry,Justin Bieber, Trans-Siberian Orchestra
 Popular Musical Acts: Brad Paisley, Toby Keith, Mötley Crüe, Slipknot, Journey
 
 
Item 243: 
Quick note before you dive in:
 This post (largely based on recent McKinsey Global Institute and Executive Office of the President reports) includes some doom and gloom about the negative impacts artificial intelligence may have on your life and career.
 The last thing I want is for you to leave feeling freaked out, so try reading to the end, where I discuss the potential AI has to change your work for the better. If you can’t make it that far, save it for later or share it with a friend so you remember to read it the next time you hear about AI or automation in the news.
 Either way, you’re taking a step towards preparing yourself and a friend to be more successful in the future. You rock!
 If you're on your computer, click one of the images below to jump to a section or keep reading!
 
 
Item 255: 
importance = feat_imp(ens, X_train[cols], y_train); importance
 [('YearMade', -0.21947050888595573),
 ('Coupler_System', -0.21318328275792894),
 ('ProductSize', -0.18353291714217482),
 ('saleYear', -0.045706193607739254),
 ('Enclosure', -0.041566508577359523),
 ('MachineID', -0.01399141076436905),
 ('MachineHoursCurrentMeter', -1.9246700722952426e-05)]
 In above output, YearMade increases prediction RMSE most if it gets shuffled (proxy to getting removed from model). So it must be most important feature. 
 
(above results correspond to data taken from a Kaggle competition. Here is the link - https://www.kaggle.com/c/bluebook-for-bulldozers)
 
 
Item 261: 
Recommend & share..
 
 
Item 262: 
Recommend & share..
 
 
Item 264: 
Recommend & share..
 
 
Item 265: 
Recommend & share..
 
 
Item 269: 
Hi Jane,

Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. 
Also many thanks for your suggestions. We hope to improve this feature in the future. 

In case you experience any further problems with the app, please don't hesitate to contact me again.

Best regards,

John Doe
Customer Support

1600 Amphitheatre Parkway
Mountain View, CA
United States
 Hei

Grunnet manglende dekning på deres kort for månedlig trekk, blir dere nå overført til årlig fakturering.
I morgen vil dere motta faktura for hosting og drift av nettbutikk for perioden 05.03.2018-05.03.2019.
Ta gjerne kontakt om dere har spørsmål.

Med vennlig hilsen
John Doe - SomeCompany.no
04756 | johndoe@somecompany.no

Husk å sjekk vårt hjelpesenter, kanskje du finner svar der: https://support.somecompany.no/

 Ciao John, 

Grazie mille per averci contattato! Apprezziamo molto che abbiate trovato il tempo per inviarci i vostri commenti e siamo lieti che vi piaccia l'App. 

Sentitevi liberi di parlare di con i vostri amici o di sostenerci lasciando una recensione nell'App Store!

Cordiali saluti, 

Jane Doe
Customer Support

One Infinite Loop
Cupertino
CA 95014
 Thank you for keeping me updated on this issue. I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for your suggestions. We hope to improve this feature in the future. In case you experience any further problems with the app, please don't hesitate to contact me again.
 Grunnet manglende dekning på deres kort for månedlig trekk, blir dere nå overført til årlig fakturering. I morgen vil dere motta faktura for hosting og drift av nettbutikk for perioden 05.03.2018-05.03.2019. Ta gjerne kontakt om dere har spørsmål.
 Grazie mille per averci contattato! Apprezziamo molto che abbiate trovato il tempo per inviarci i vostri commenti e siamo lieti che vi piaccia l'App. Sentitevi liberi di parlare di con i vostri amici o di sostenerci lasciando una recensione nell'App Store.
 I'm happy to hear that the issue got resolved after all and you can now use the app in its full functionality again. Also many thanks for your suggestions. In case you experience any further problems with the app, please don't hesitate to contact me again.
 Grunnet manglende dekning på deres kort for månedlig trekk, blir dere nå overført til årlig fakturering. I morgen vil dere motta faktura for hosting og drift av nettbutikk for perioden 05.03.2018-05.03.2019. Ta gjerne kontakt om dere har spørsmål.
 Apprezziamo molto che abbiate trovato il tempo per inviarci i vostri commenti e siamo lieti che vi piaccia l'App. Sentitevi liberi di parlare di con i vostri amici o di sostenerci lasciando una recensione nell'App Store.
 I can assure you that our developers are already aware of the issue and are trying to solve it as soon as possible.
AND
I have already forwarded your problem report to our developers and they will now investigate this issue with the login page in further detail in order to detect the source of this problem.
--------------------------------------------------------------------I am very sorry to hear that.
AND
We sincerely apologize for the inconvenience caused.
--------------------------------------------------------------------Therefore, I would kindly ask you to tell me which operating system you are using the app on.
AND
Can you specify which device you are using as well as the Android or iOS version it currently has installed?
 
 
Item 272: 
item {
 id: 1
 name: ‘toy’
}
 
 
Item 273: 
Recommend & share..
 
 
Item 276: 
findNextMove(board, playerNo) {
  let opponent = 3 - playerNo;
  let tree = new Tree();
  let rootNode = tree.root;
  rootNode.state.board = new Board(board);
  rootNode.state.playerNo = opponent;
   let startTime = Date.now();
  while ((Date.now() - startTime) < 1000) {
    let promisingNode = selectPromisingNode(rootNode);
    
    if (promisingNode.state.board.checkStatus() === IN_PROGRESS) {
      expandNode(promisingNode);
    }
     let nodeToExplore = promisingNode;
    if (nodeToExplore.childArray.length > 0) {
      nodeToExplore = promisingNode.getRandomChild();
    }
     let playoutResult = simRanPlayout(nodeToExplore, opponent)
    backpropagation(nodeToExplore, playoutResult);
  }
   let winnerNode = rootNode.getChildWithMaxScore();
  return winnerNode.state.board;
}
 selectPromisingNode(rootNode) {
  let node = rootNode;
  while (node.childArray.length !== 0) {
    node = UCB.findBestNodeWithUCB(node);
  }
  return node;
}
 UCB = {
   findBestNodeWithUCB: (node) => {
    let parentVisitCt = node.state.visitCount;
    let childUCB = [];
    node.childArray.forEach(child => {
      childUCB.push(UCB.ucbValue(parentVisitCt, child.state.winScore, child.state.visitCount));
    }
     let max = Math.max(...childUCB);
    let idx = childUCB.indexOf(max);
    return node.childArray[idx];
   };
 ucbValue: (totalVisit, nodeWinScore, nodeVisit) => {
     if (nodeVisit === 0) {
      return Number.MAX_SAFE_INTEGER;
    }
    return (nodeWinScore / nodeVisit) + 1.41 * Math.sqrt ( Math.log (totalVisit) / nodeVisit);
  }
 }
 expandNode(node) {
   let possibleStates = node.state.getAllPossibleStates();
   possibleStates.forEach(state => {
    let newNode = new Node(state);
    newNode.parent = node;
    newNode.state.playNo = node.state.getOpponent();
    node.childArray.push(newNode);
  });
 }
 let nodeToExplore = promisingNode;
if (nodeToExplore.childArray.length > 0) {
  nodeToExplore = promisingNode.getRandomChild();
}
 simulateRandomPlayout(node, opponent) {
   let tempNode = new Node(null, node);
  let tempState = tempNode.state;
  let boardStatus = tempState.board.checkStatus();
   if (boardStatus === opponent) {
    tempNode.parent.state.winScore = Number.MIN_SAFE_INTEGER;
    return boardStatus;
  }
   while(boardStatus === board.IN_PROGRESS) {
    tempState.togglePlayer();
    tempState.randomPlay();
    boardStatus = tempState.board.checkStatus();
  }
  return boardStatus;
 }
 backpropagation(nodeToExplore, playoutResult) {
  let tempNode = nodeToExplore;
  while (tempNode !== undefined) {
    tempNode.state.visitCount++;
    if (tempNode.state.playNo === playoutResult) {
      tempNode.state.addScore(10);
    }
    tempNode = tempNode.parent;
  }
}
 simulateAIPlay() {
  let board = new Board();
  let player = 1;
  let totalMove = 9;
  for (var i = 0; i < totalMove; i++) {
    board = findNextMove(board, player);
    if (board.checkStatus() !== -1) {
      break;
    }
    player = 3 - player;
  }
  let winStatus = board.checkStatus();
  return winStatus;
}
 
 
Item 277: 
Recommend & share..
 
 
Item 279: 
implementation 'com.google.firebase:firebase-ml-vision:15.0.0'
 <meta-data
    android:name="com.google.firebase.ml.vision.DEPENDENCIES"
    android:value="barcode, face, other_model_names..." />
 
 
Item 281: 
You flip the fair coin. Every time you get heads, you lose $1, and every time you get tails, you gain $1.
 You flip the fair coin. Every time you get heads, you lose $1, and every time you get tails, you gain $2.
 You flip the fair coin. Every time you get heads, you lose $1, and every time you get tails, you gain $1. Additionally, there is a $0.01 fee for every flip regardless of the outcome.
 
 
Item 282: 
Recommend & share..
 
 
Item 283: 
Recommend & share..
 
 
Item 286: 
//subroutine header for performing cluster analysis:
#include "cluster.h"
 //maximum number of clusters:
#define MAX_CLUSTER 10
 int main(int argc, char **argv) {
  char *infile;            //input file
  char *outfile;           //output file
  FILE *fs;                //file pointer
  double **a;              //matrix of training data/U
  int m;                   //number of rows in matrix
  int n;                   //number of columns in matrix
  int nsv;                 //number of singular values
   if (argc!=4) {
    printf("syntax: cluster_svd nsv train centers\n");
    printf("  where:\n");
    printf("nsv      = number of singular values (0 = use untransformed data)\n");
    printf("infile   = ASCII input file containing training data\n");
    printf("output   = ASCII output file containing cluster centers\n");
    printf("\n");
    printf("file format:\n");
    printf("- one line header containing number of rows and number of columns\n");
    printf("- row major list of each matrix element\n");
    exit(1);
  }
   if (sscanf(argv[1], "%d", &nsv)!=1) {
    fprintf(stderr, "Error parsing first command line argument\n");
    exit(1);
  }
  infile=argv[2];
  outfile=argv[3];
   fs=fopen(infile, "r");
  if (fs==NULL) {
    fprintf(stderr, "Error opening input file, %s\n", infile);
    exit(1);
  }
   if (fscanf(fs, "%d %d", &m, &n)!=2) {
    fprintf(stderr, "Format error in input file: %s line 0", infile);
    exit(1);
  }
  if (nsv>n) {
    fprintf(stderr, "Command line parameter nsv=%d out of range\n", nsv);
    exit(1);
  }
   a=new double *[m];
  a[0]=new double[m*n];
  for (int i=1; i<m; i++) a[i]=a[0]+i*n;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      if (fscanf(fs, "%lg", a[i]+j)!=1) {
        fprintf(stderr, "Format error in input file, %s, line %d\n", infile, i);
        exit(1);
      }
    }
  }
   fclose(fs);
 #ifndef SVD_H
#define SVD_H
 //subroutine for singular value decomposition:
int                       //returns an error code (0 for success)
     svd (double **a,     //input matrix--replaced by U on output
                int m,        //number of rows
                int n,        //number of columns
                double *s,    //singular values
                double **vt); //V--right singular vectors
 #endif
   double *ave;
  double *s;               //singular values
  double **vt;             //right singular vectors
   //first we calculate and remove the arithmetic means:
  ave=new double[n];
  for (int i=0; i<n; i++) ave[i]=0;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      ave[j]+=a[i][j];
    }
  }
  for (int i=0; i<n; i++) ave[i]/=m;
  for (int i=0; i<m; i++) {
    for (int j=0; j<n; j++) {
      a[i][j]-=ave[j];
    }
  }
   if (nsv>0) {
    //make space for singular values:
    s=new double[n];
     //make space for right singular vectors:
    vt=new double *[n];
    vt[0]=new double[n*n];
    for (int i=1; i<n; i++) vt[i]=vt[0]+i*n;
     //perform the decomposition:
    int err=svd(a, m, n, s, vt);
    if (err!=0) {
      fprintf(stderr, "Error in svd subroutine\n");
      exit(err);
    }
  }
 #ifndef CLUSTER_H
#define CLUSTER_H
 int                            //returns number of cluster centers
    cluster (double ** x,      //training vectors
                int m,         //number of training vectors
                int n,         //dimension of each vector
                int max_nc,    //maximum number of cluster centers
                double **mu);  //returned cluster centers
 #endif
 double **mu_p;      //matrix of cluster centers
  int nc;           //number of cluster centers
   //make space for cluster centers:
  mu_p=new double *[MAX_CLUSTER];
  mu_p[0]=new double[MAX_CLUSTER*n];
  for (int i=1; i<MAX_CLUSTER; i++) mu_p[i]=mu_p[0]+i*n;
   if (nsv>0) {
    //make space for cluster centers:
    nc=cluster(a, m, nsv, MAX_CLUSTER, mu_p);
  } else {
    //make space for cluster centers:
    nc=cluster(a, m, n, MAX_CLUSTER, mu_p);
  }
   if (nc <= 0) {
    fprintf(stderr, "Cluster algorithm failed");
    exit(-1);
  }
   double **mu;        //cluster centers in un-transformed coords
   //allocate space for the un-transformed cluster centers:
  mu=new double *[nc];
  mu[0]=new double[nc*n];
  for (int i=1; i<nc; i++) mu[i]=mu[0]+i*n;
   //perform the coordinate transformation:
  for (int i=0; i<nc; i++) {
    for (int j=0; j<n; j++) {
      mu[i][j]=ave[j];
      if (nsv>0) {
        for (int k=0; k<nsv; k++) mu[i][j]+=vt[k][j]*s[k]*mu_p[i][k];
      } else {
        mu[i][j]+=mu_p[i][j];
      }
    }
  }
   //write the results to a file:
  fs=fopen(outfile, "w");
  if (fs==NULL) {
    fprintf(stderr, "Error opening output file, %s\n", outfile);
    exit(1);
  }
   fprintf(fs, "%d %d\n", nc, n);
  for (int i=0; i<nc; i++) {
    for (int j=0; j<n; j++) fprintf(fs, "%16.8lg", mu[i][j]);
    fprintf(fs, "\n");
  }
   fclose(fs);
   //clean up:
  delete [] mu[0];
  delete [] mu;
   delete [] mu_p[0];
  delete [] mu_p;
   delete [] ave;
  delete [] a[0];
  delete [] a;
  if (nsv>0) {
    delete [] s;
    delete [] vt[0];
    delete [] vt;
  }
   return 0;
}
 int svd(double **a,          //matrix -- replaced by u on output
   int m,                     //number of rows
   int n,                     //number of columns
   int k,                     //desired number of singular values
   double*s,                  //singular values
   double **vt);              //right singular vectors
 
 
Item 287: 
Recommend & share..
 
 
Item 288: 
{“user”: “user_0”, “items”: [{“sku”: “sku_0”, “r”: 1},
                             {“sku”: “sku_1”, “r”: 2}]}
{“user”: “user_1”, “items”: [{“sku”: “sku_0”, “r”: 2},
                             {“sku”: “sku_2”, “r”: 1}]}
 (...)
 {“user”: “user_m”, “items”: [{“sku”: “sku_0”, “r”: 2},
                             {“sku”: “sku_n”, “r”: 1}]}
 user,productSku,type
95106786645166913,AG672APF78UCF,1
8897887309145128270,FI911APF10HUZ,2
1153521620412862249,CO515APM42GTT,2
1819629928011287314,FI911SHM06PUF,1
1133082218947946503,JO546APM50KFT,3
 #cron.yaml
cron:
- description: "daily summary job"
  url: /tasks/summary
  schedule: every 24 hours
 gcloud app deploy queue.yaml
gcloud app deploy cron.yaml
 {“user”:”8291743389332496534",”interactions”:[{“item”:”RE499APM08ZTZ”,”score”:0.5},{“item”:”RE499APM85DGC”,”score”:0.5}]}
 {“user”:”5843584611541988560",”interactions”:[{“item”:”SA232SHF47ZZO”,”score”:0.5},{“item”:”SA232SHF89GXI”,”score”:1.0}]}
 {“user”:”6935962925703084781",”interactions”:[{“item”:”DE996ACF83KYE”,”score”:0.5},{“item”:”CR177ACF76BIH”,”score”:0.5},{“item”:”MA318ACF62LRP”,”score”:0.5},{“item”:”DE996ACF23CIQ”,”score”:0.5},{“item”:”QU097ACF76IKL”,”score”:0.5},{“item”:”QU097ACF44IPN”,”score”:0.5},{“item”:”QU097ACF77IOG”,”score”:0.5},{“item”:”DE996ACF50CHP”,”score”:1.5}]}
 {“item”:”LO611APF49EBY”,”similarity_items”:
    [{“item”:”DI944APF23XPS”,”similarity”:0.017922988},
     {“item”:”ME700ACF75HAQ”,”similarity”:0.035410713},
     {“item”:”LO611APF47ECA”,”similarity”:0.1254363},
     {“item”:”TR943APF03NGY”,”similarity”:0.024296477},
     {“item”:”CA700APF63DSU”,”similarity”:0.044455424},
     {“item”:”LO611APF43ECE”,”similarity”:0.33709994},
     {“item”:”LO611APF41ECG”,”similarity”:0.07137738},
     {“item”:”LO611APF80EAT”,”similarity”:0.02228836},
     {“item”:”LO611APF84EAP”,”similarity”:0.03180418},
     {“item”:”DE234APF58ION”,”similarity”:0.013907681}]}
 /make_recommendation?browsed=CA278SHF46UJH
 
 
Item 289: 
tuna isa fish.
herring isa fish.
tuna has fin.
herring has fin.
parrot isa bird.
eagle isa bird.
parrot has wing.
eagle has wing.
fin isa extremity.
wing isa extremity.
fin has scale.
wing has feather.
 A mom is a human.
A dad is a human.
A mom is a parent.
A dad is a parent.
A son is a child.
A daughter is a child.
A son is a human.
A daughter is a human.
A mom likes cake.
A daughter likes cake.
A son likes sausage.
A dad likes sausage.
Cake is a food.
Sausage is a food.
Mom is a human now.
Dad is a human now.
Mom is a parent now.
Dad is a parent now.
Son is a child now.
Daughter is a child now.
Son is a human now.
Daughter is a human now.
Mom likes cake now.
Daughter likes cake now.
Son likes sausage now.
Dad likes sausage now.
Cake is a food now.
Sausage is a food now.
Mom was a daughter before.
Dad was a son before.
Mom was not a parent before.
Dad was not a parent before.
Mom liked cake before.
Dad liked sausage before.
Cake was a food before.
Sausage was a food before.
 
 
Item 294: 
const mind = require('wits')
mind.open()
mind.read(console.log)
 
 
Item 296: 
Recommend & share..
 
 
Item 297: 
Recommend & share..
 
 
Item 299: 
Recommend & share..
 
 
Item 300: 
Recommend & share..
 
 
Item 301: 
Recommend & share..
 
 
Item 302: 
User’s Query <-> DialogFlow Agent <-> Webhook <-> API To Get Details
 
 
Item 303: 
entities:
 - login_problem_type
 slots:
 login_problem_type:
 type: categorical
 values:
 — computer
 — phone
 — tablet
 - I changed my [laptop](login_problem_type:computer) recently and cant login
- cant login from my new [macbook](login_problem_type:computer). Is there any additional software that i need to install?
 {
 “laptop”: “computer”,
 “desktop”: “computer”,
 “machine”: “computer”,
 “pc”: “computer”,
 “macbook”: “computer”,
 “iphone”: “phone”,
 “ipad”: “tablet”
}
 ## help with login (general)
* login_general
 — utter_login_specify
> clarify login problem
 ## help with login (computer)
> clarify login problem
* login_clarification{"login_problem_type":"computer"}
 — utter_login_computer_help
 ## help with login (phone)
> clarify login problem
* login_clarification{"login_problem_type":"phone"}
 — utter_login_phone_help
 ## help with login (tablet)
> clarify login problem
* login_clarification{"login_problem_type":"tablet"}
 — utter_login_tablet_help
 
 
Item 311: 
Recommend & share..
 
 
Item 316: 
Recommend & share..
 
 
Item 317: 
Recommend & share..
 
 
Item 318: 
Grade ~ failures + higher_edu + mother_edu + studytime + father_edu + absences
 pm.traceplot(normal_trace)
 pm.plot_posterior(normal_trace)
 pm.df_summary(normal_trace)
 Grade ~ N(9.20 * Intercept - 1.32 * failures + 1.85 * higher_edu + 0.26 * mother_edu + 0.58 * studytime + 0.03 * father_edu - 0.07 * absences,  2.28^2)
 Test Observation:
failures = 0, higher_edu = 1, mother_edu = 2, studytime = 1,
father_edu = 2, absences = 8
 True Grade = 12
Average Estimate = 11.6763
5% Estimate = 7.7618    95% Estimate = 15.5931
 New Observation:
absences = 1, failures = 0, father_edu = 1
higher_edu = 1, mother_edu = 4, studytime = 3
 Average Estimate = 13.8009
5% Estimate = 10.0696    95% Estimate = 17.4629
 
 
Item 319: 
git clone https://github.com/pjreddie/darknet.git
cd darknet
 GPU=1 
CUDNN=0
OPENCV=1
OPENMP=0
DEBUG=0
...
 ~/darknet$ make -j8
 wget https://pjreddie.com/media/files/yolo.weights
 ./darknet detect cfg/yolo.cfg cfg/yolo.weights data/dog.jpg
 ./darknet detector demo cfg/coco.data cfg/yolo.cfg  cfg/yolo.weights
 ./darknet detector demo cfg/coco.data cfg/yolo.cfg cfg/yolo.weights <video dosya yolu>
 
 
Item 322: 
(define myMedium
(list
(substrate hE-cad-Fc]) (growthFactors (list dfA dfB dfC))
))
 (define MyStartCells (apply (list TFiX mRNAiY) cellLine3))
 (define DifStep1 (cells medium) (incubate cells medium)))
(define DifFinal (cells medium) (incubate cells (add dfQ medium))))
 (measure someCells myPanel)
# 0.3
(define DifCheck (cells medium panel) (if (> (measure cells panel) 0.7)
(incubate cells (add dfP medium))) else
(incubate (apply mRNAiZ cells) medium)
))
 (define myProtocol (cells medium)
(DifStepFinal (DifCheck ( DifStep1 cells medium)))
 (measure (myProtocol myStartCells myMedium) resultPanel)
 
 
Item 323: 
Recommend & share..
 
 
Item 324: 
Recommend & share..
 
 
Item 326: 
Recommend & share..
 
 
Item 331: 
mario_vs_wario/
    training_set/
        mario/
            mario_1.jpg
            mario_2.jpg
            ...
        wario/
            wario_1.jpg
            wario_2.jpg
            ...
    test_set/
        mario/
            mario_1.jpg
            mario_2.jpg
            ...
        wario/
            wario_1.jpg
            wario_2.jpg
            ...
 
 
Item 332: 
Recommend & share..
 
 
Item 336: 
Confidence:
 - Palm 0.75
 - Oak 0.60
 Confidence:
 - USB 0.76
 
 
Item 338: 
Recommend & share..
 
 
Item 339: 
sklearn.feature_selection.f_regression
 sklearn.feature_selection.f_classif
 sklearn.feature_selection.mututal_info_regression 
sklearn.feature_selection.mututal_info_classif
 sklearn.feature_selection.VarianceThreshold
 
 
Item 340: 
let model = try VNCoreMLModel(for: AnimalClassifier().model)
 
 
Item 341: 
|         | Cat 0 | Cat 1 | Dog 0 | Dog 1 |
|---------|-------|-------|-------|-------|
| Topic 0 | *     |       |       | *     |
| Topic 1 |       | *     | *     |       |
 |     | Topic 0 | Topic 1 |
|-----|---------|---------|
| Cat | 1       | 1       |
| Dog | 1       | 1       |
 |            | Topic 0 | Topic 1 |
|------------|---------|---------|
| Document 0 | 1       | 1       |
| Document 1 | 1       | 1       |
 t0 =
(
  (cat emoji with Topic 0 + beta)
  /
  (emoji with Topic 0 + unique emoji * beta)
)
*
(
  (emoji in Document 0 with Topic 0 + alpha)
  /
  (emoji in Document 0 with a topic + number of topics * alpha)
) =
(
  (0 + 0.01)
  /
  (1 + 2 * 0.01)
)
*
(
  (0 + 0.5)
  /
  (1 + 2 * 0.5)
) = 0.0024509803921568627
 t1 = ((1 + 0.01) / (2 + 2 * 0.01)) * ((1 + 0.5) / (1 + 2 * 0.5))
   = 0.375
 p(Cat 0 = Topic 0 | *) = t0 / (t0 + t1) = 0.006493506493506494
p(Cat 0 = Topic 1 | *) = t1 / (t0 + t1) = 0.9935064935064936
 |         | Cat 0 | Cat 1 | Dog 0 | Dog 1 |
|---------|-------|-------|-------|-------|
| Topic 0 | *     | *     |       | *     |
| Topic 1 |       |       | *     |       |
 |     | Topic 0 | Topic 1 |
|-----|---------|---------|
| Cat | 2       | 0       |
| Dog | 1       | 1       |
 |            | Topic 0 | Topic 1 |
|------------|---------|---------|
| Document 0 | 2       | 0       |
| Document 1 | 1       | 1       |
 |         | Cat 0 | Cat 1 | Dog 0 | Dog 1 |
|---------|-------|-------|-------|-------|
| Topic 0 | *     | *     |       |       |
| Topic 1 |       |       | *     | *     |
 |     | Topic 0 | Topic 1 |
|-----|---------|---------|
| Cat | 2       | 0       |
| Dog | 0       | 2       |
 |            | Topic 0 | Topic 1 |
|------------|---------|---------|
| Document 0 | 2       | 0       |
| Document 1 | 0       | 2       |
 Phi row column =
  (emoji row with topic column + beta)
  /
  (all emoji with topic column + unique emoji * beta)
 Theta row column =
  (emoji in document row with topic column + alpha)
  /
  (emoji in document row + number of topics * alpha)
 
 
Item 347: 
y = a_0 + a_1 * x      ## Linear Equation
 
 
Item 351: 
Recommend & share..
 
 
Item 354: 
a_hash(original) = 3c3e0e1a3a1e1e1e
a_hash(modified) = 3c3e0e3e3e1e1e1e
 md5(original) = 8d4e3391a3bca7...
md5(modified) = c27baa59355d10...
 
 
Item 358: 
library(tidyverse)
library(readxl)
library(knitr)
library(ggplot2)
library(lubridate)
library(arules)
library(arulesViz)
library(plyr)
 retail <- read_excel('Online_retail.xlsx')
retail <- retail[complete.cases(retail), ]
retail <- retail %>% mutate(Description = as.factor(Description))
retail <- retail %>% mutate(Country = as.factor(Country))
retail$Date <- as.Date(retail$InvoiceDate)
retail$Time <- format(retail$InvoiceDate,"%H:%M:%S")
retail$InvoiceNo <- as.numeric(as.character(retail$InvoiceNo))
 glimpse(retail)
 retail$Time <- as.factor(retail$Time)
a <- hms(as.character(retail$Time))
retail$Time = hour(a)
 retail %>% 
  ggplot(aes(x=Time)) + 
  geom_histogram(stat="count",fill="indianred")
 detach("package:plyr", unload=TRUE)
 retail %>% 
  group_by(InvoiceNo) %>% 
  summarize(n_items = mean(Quantity)) %>%
  ggplot(aes(x=n_items))+
  geom_histogram(fill="indianred", bins = 100000) + 
  geom_rug()+
  coord_cartesian(xlim=c(0,80))
 tmp <- retail %>% 
  group_by(StockCode, Description) %>% 
  summarize(count = n()) %>% 
  arrange(desc(count))
tmp <- head(tmp, n=10)
tmp
 tmp %>% 
  ggplot(aes(x=reorder(Description,count), y=count))+
  geom_bar(stat="identity",fill="indian red")+
  coord_flip()
 retail_sorted <- retail[order(retail$CustomerID),]
library(plyr)
itemList <- ddply(retail,c("CustomerID","Date"), 
                       function(df1)paste(df1$Description, 
                       collapse = ","))
 itemList$CustomerID <- NULL
itemList$Date <- NULL
colnames(itemList) <- c("items")
 write.csv(itemList,"market_basket.csv", quote = FALSE, row.names = TRUE)
 tr <- read.transactions('market_basket.csv', format = 'basket', sep=',')
tr
summary(tr)
 itemFrequencyPlot(tr, topN=20, type='absolute')
 rules <- apriori(tr, parameter = list(supp=0.001, conf=0.8))
rules <- sort(rules, by='confidence', decreasing = TRUE)
summary(rules)
 inspect(rules[1:10])
 topRules <- rules[1:10]
plot(topRules)
 plot(topRules, method="graph")
 plot(topRules, method = "grouped")
 
 
Item 361: 
keras.layers.Dropout(rate, noise_shape=None, seed=None)
 model=keras.models.Sequential()
model.add(keras.layers.Dense(150, activation="relu"))
model.add(keras.layers.Dropout(0.5))
 keras.layers.BatchNormalization()
 model.add(Conv2D(60,3, padding = "same"))
model.add(BatchNormalization())
model.add(Activation("relu"))
 
 
Item 364: 
Recommend & share..
 
 
Item 365: 
Recommend & share..
 
 
Item 368: 
for i in range(num_epochs):
    grad = compute_gradient(data, params)
    params = params — learning_rate * grad
 for i in range(num_epochs):
    np.random.shuffle(data)
    for batch in radom_minibatches(data, batch_size=32):
        grad = compute_gradient(batch, params)
        params = params — learning_rate * grad
 for i in range(num_epochs):
    np.random.shuffle(data)
    for example in data:
        grad = compute_gradient(example, params)
        params = params — learning_rate * grad
 
 
Item 369: 
Recommend & share..
 
 
Item 370: 
Recommend & share..
 
 
Item 371: 
Recommend & share..
 
 
Item 379: 
Recommend & share..
 
 
Item 381: 
item {
id: 1
Name: santa
}
 
 
Item 382: 
myfiles = !ls
 
 
Item 386: 
index=data['headline_text'].str.match(r'(?=.*\byear\b)(?=.*\bold\b).*$')
texts=data['headline_text'].loc[index].tolist()
noun=[]
verb=[]
for doc in nlp.pipe(texts,n_threads=16,batch_size=10000):
    try:
        for c in doc:
            if c.pos_=="NOUN":
                noun.append(c.text)
            elif c.pos_=="VERB":
                verb.append(c.text)            
    except:
        noun.append("")
        verb.append("")
plt.subplot(1,2,1)
pd.Series(noun).value_counts().head(10).plot(kind="bar",figsize=(20,5))
plt.title("Top 10 Nouns in context of 'Year Old'",fontsize=30)
plt.xticks(size=20,rotation=80)
plt.yticks([])
plt.subplot(1,2,2)
pd.Series(verb).value_counts().head(10).plot(kind="bar",figsize=(20,5))

plt.title("Top 10 Verbs in context of 'Year Old'",fontsize=30)
plt.xticks(size=20,rotation=80)
plt.yticks([]
 ['10-year-old girl missing',
 'Relative kills 9-year-old',
 '59-year-old widow murdered',
 'Spunky 60-year-old woman prevents burglary',
 "75-year-old woman done to death in B'lore",
 'Encroachments threaten 900-year-old temple',
 '3 nabbed for 5-year-old robbery',
 '25-year-old man electrocuted',
 '5-year-old boy run over',
 'Killers of 88-year-old woman arrested',
 '21-year-old held in theft case',
 "60-year-old gets two years' RI for rape attempt",
 'STRAIGHT ANSWERSBRSwati Aneja 13 year old schoolgirl on what I Day means to her',
 'Robbers stab 9-year-old',
 "Eight year old's brush with 'commissions'",
 'By Ganesha; what 81-year-old Deryck does BEST',
 'Six-year-old girl raped; murdered',
 'FBI woos 16-year-old indian author',
 'Six-year old raped murdered in Patiala cantonment',
 'FBI woos 16-year-old Indian author']
 
 
Item 387: 
DATE(DATETRUNC('month', DATEADD('month', [Months Forecast], [Month])))
 LOOKUP(SUM([#Passengers]), [Months Forecast])
 IF LAST() < [Months Forecast]
THEN 'Model Forecast'
ELSE 'Model Prediction'
END
 
 
Item 389: 
Recommend & share..
 
 
Item 390: 
O(n) where n: number of operations
 def factorial(n):
    if n < 1:        #base case
        return 1
    else:             #recursive case
        return n * factorial(n-1)
 
 
Item 391: 
vector(France) — vector(Paris) + vector(Berlin)
 'dependable',
'trustworthy',
'cost-effective',
'efficient',
'economical',
'Reliable',
'accurate',
'reliability',
'trusted',
'reputable'
 'relyable', 
'realible', 
'relable', 
'reliabe', 
'realiable', 
'relaiable', 
'relaible', 
'trustworth', 
'trustfull', 
'consitant'
 [France] - [Paris] + [Berlin] ==> [Germany]
 [reliable] - [relieable] + [foriegn] ==> [foreign]
 'becasue',
'becuase',
'beacuse',
'becuse',
'becouse',
'beacause',
'becaue',
'unfortunatly',
'beause',
'bacause'
 'and',
'annd',
'anf',
'ansd',
'adn',
'abd',
'andd',
'cuase',
'alll',
'becuse'
 'Best' spellings:
U.S.;  officials;  political;  whose;  States;  significant;  percent;  University;  economic;  increasingly;  efforts;  individuals;  government;  former;  York;  such;  their;
 'Worst' spellings:
!!!!!!;  Fav;  THANKS; omg;  EVINRUDE;  ur;  Airbags;  :{;  hes;  ppl;  ps3;  points3;  Mercruiser;  aint; fromServlet;  plz;  awsome;
 
 
Item 394: 
public int get_p1_health()
{
    return _currentDomain.PeekByte(0x000530);
}
 public int get_p2_health()
{
    return _currentDomain.PeekByte(0x000730);
}
 
 
Item 395: 
Abstract
 With this article, I am sharing my personal learnings from creating an AI-enabled UI. I developed four baseline principles which can be applied when designing for AI-enabled user interfaces: 1) discovery and expectation management, 2) design for forgiveness, 3) data transparency and tailoring, and 4) privacy, security and control.
 Next to the applying basic UX knowledge, keep these design principles in mind and you will have a good base for your AI-enabled user interface.
 To avoid confusion, in this article I use the abbreviation AI to refer to narrow artificial intelligence.
 These examples are mere manifestations. For clarity, AI can be implemented anywhere — such as a search engine, behind your photo library, or in a vacuum cleaner. The UI depends on the fit of the AI with the task and channel.
 Developing AI-enabled UI at Deloitte
 The AI tool functions as an agent to the audit experts by processing big data and brute force algorithms at high speed. The AI removes the manual, time-consuming, and tedious tasks that are embedded in auditor’s current jobs such as dossier cross-checking. Moreover, the AI is able to analyze tons of documents, read a text, find trends and is able to generate predictions. It monitors and remembers auditor’s behaviors and choices, and learns from this for next time. The tool is able to convert big data into insightful information for the auditor.
 Nonetheless, being an auditor requires deep professional knowledge and experience in the audit process. The AI tool still needs the auditor to understand the context of data. Therefore, the AI — being an agent — gives the final say/decision to the auditor.
 For example, the only expectation I have of a pet feeder is to have it feed my cat multiple times per day without my physical presence. However, Petnet’s smart feeder offers me more than that. The AI uses my cat’s weight, age, breed, and activity level to match the cat with portion sizes.
 In the previous cat example, the pet feeder only has to be set up once. Additionally, it sends an alert when it’s running low on food. The only task I have to do is to fill it with kibbles when it prompts me to do so.
 For example, I lost my earring which is small in size. I have carpeting, and could not locate the earring. I eventually cleared Roomba’s bin, and let it roam the room. Few moments later, I found my earring in Roomba’s bin.
 For example, if you would ask Siri the following: “Show me the nearest fast food restaurant which is not McDonald’s” — Siri will sadly show you *drumroll* the location of the nearest McDonald’s. This is because Siri is programmed to recognize words such as “nearest”, “fast food restaurant” and “McDonald’s”, however, Apple did not discover to include the possibility that users might ask Siri to not show them something.
 Apple did not design Siri for users to forgive her. Siri is designed as an adult assistant. She sounds like an adult and she speaks like an adult. Therefore, users are less inclined to forgive this adult assistant when she cannot “understand” simple commands.
 Cocorobo is a vacuum cleaner. The delightful feature is that it can talk to people. A result of this example is that it helped a Japanese adult feel less isolated — since the robot greeted him every time he came home (to an empty house). I bet he would forgive his little friend if it smeared his house with puppy-feces.
 The Pet feeder, Google Assistant, Roomba, Spotify, or Netflix are all examples that AI-enabled UIs should be able to function without internet connectivity. Voice assistants Siri and Alexa do not function without internet connectivity.
 It’s not necessary to show the algorithms to the user, but it is important to mention which data is used in order for it to be an agent to the user.
 For example, Amazon Alexa knows when you go to sleep or when you’re out of town. If she gets hacked and this information gets into the wrong hands (like burglars), it’s something to be anxious about. That’s why it’s so important to address the privacy issue here and be transparent about what data the AI has.
 In the Google Translate Community, Google offers users to manually translate and confirm words and sentences. It helps the machine to learn and understand the context and to translate sentences from one language to another. Thus, the AI is continuously “learning” since it is fed with big tailored data at a high speed. This way it keeps getting better in translating.
 In the example of the Roomba and the puppy, the Roomba could ask what the problem is and use this input to learn for next time. This way the interaction would be beneficial for both user and the robot.
 E.g. You own a Netflix account. Your friend does not. Your friend watched movies on your Netflix account which you don’t like — This means the result of the algorithm has been changed due to the behavior of your friend. In this case, you would like to configure the predictions by deleting your friend’s preferences/history.
 What I noticed when jumping on the Crypto-currency train was the importance of privacy and security of the UIs that handle my money. I did not even mind the two-factor authentication setups because I knew it was for my own good.
 Knowing that AI will have crucial, personal and privacy sensitive data of mine — as a user it is important that the UI does its very best to protect my data.
 I did this when I configured my Roomba to automatically vacuum my house every day. The first time I set this up, at 14:46, I scheduled the time for it to run automatically at [14:47]. I wanted to check whether it is indeed cleaning on scheduled time. After I saw that it did, I trust the Roomba to do this for me every day at 12:00 while I am at work.
 I can press two buttons on my pet feeder to output kibbles any time I want. Moreover, I can press the “clean” button to make Roomba start or stop cleaning (on unscheduled time) or physically move the Roomba to clean on a spot I direct it to.
 For example, the Nest thermostat learns what you like by turning it up or down at different times. It remembers your intervenience and creates a personalized schedule for you.
 E.g. Digit is a platform that analyzes your spending habit. It acts as a chatbot and proactively asks consent from you to transfer your money in order for you to save.
 When my Roomba gets stuck, it makes a beeping noise to let me know there’s something wrong. If it did not beep, I would not know it stopped working.
 1. Discovery and expectation management
Set user expectations well in order to avoid false expectations.
 * Users should be aware of what the tool can, and cannot do
 * Users should expect most benefit from minimal input
 * Prepare for undiscovered and unexpected usage
 * Educate the user about the unexpected
 ---
 2. Design for forgiveness
The AI will make mistakes. Design the UI so users are inclined to forgive it.
 * Design the tool in a way that users will forgive it when it makes mistakes
 * Design delightful features to increase the likelihood of forgiveness
 * Design ability to use AI without internet connectivity
 ---
 Data transparency and tailoring
Be transparent about collecting data and offer users the ability to tailor it.
 * The AI should be transparent in what data it has of the user
 * Users should be able to provide input so the AI can learn
 * Users should be able to adjust what AI has learned
 ---
 Privacy, security and control
Gain trust by driving privacy, security and the ability to control the AI.
 * Design top notch security for users to trust AI with personal data
 * Prove delivery on promises by offering test runs
 * Design ability for users to intervene and take over control
 * AI should learn from user’s intervenience
 * AI should not do anything without user’s consent
 * AI should notify users of system errors
 
 
Item 396: 
train['pickup_datetime']=pd.to_datetime(train['pickup_datetime'],format='%Y-%m-%d %H:%M:%S UTC')
train['pickup_date']= train['pickup_datetime'].dt.date
train['pickup_day']=train['pickup_datetime'].apply(lambda x:x.day)
train['pickup_hour']=train['pickup_datetime'].apply(lambda x:x.hour)
train['pickup_day_of_week']=train['pickup_datetime'].apply(lambda x:calendar.day_name[x.weekday()])
train['pickup_month']=train['pickup_datetime'].apply(lambda x:x.month)
train['pickup_year']=train['pickup_datetime'].apply(lambda x:x.year)
 
 
Item 397: 
docker run — rm -ti parrotprediction/docker-ds-python-libs
 
 
Item 399: 
Recommend & share..
 
 
Item 400: 
Ground Truth: 1111111111
Classifier 1: 1111111100 = 80% accuracy
Classifier 2: 1111111100 = 80% accuracy
Classifier 3: 1011111100 = 70% accuracy
 Ensemble Result: 1111111100 = 80% accuracy
 Ground Truth: 1111111111
Classifier 1: 1111111100 = 80% accuracy
Classifier 2: 0111011101 = 70% accuracy
Classifier 3: 1000101111 = 60% accuracy
 Ensemble Result: 1111111101 = 90% accuracy
 
 
Item 403: 
jupyter notebook --no-browser --port=8888
 ssh -i thisIsmyKey.pem -L 8000:localhost:8888 ubuntu@ec2–34–227–222–100.compute-1.amazonaws.com
 
 
Item 407: 
error = (h(x)-y)² --> (Predicted - Actual)²   
error = (6-5)² = 1
 ² is to get rid of negative values (what if Actual y=6 and Py=5)
 
 
Item 409: 
Powerful vanilla scents rise from the glass, but the fruit, even in this difficult vintage, comes out immediately. It's tart and sharp, with a strong herbal component, and the wine snaps into focus quickly with fruit, acid, tannin, herb and vanilla in equal proportion. Firm and tight, still quite young, this wine needs decanting and/or further bottle age to show its best.
Predicted:  46.233624 Actual:  45.0  
 
A good everyday wine. It's dry, full-bodied and has enough berry-cherry flavors to get by, wrapped into a smooth texture.
Predicted:  9.694958 Actual:  10.0  

 Here's a modern, round and velvety Barolo (from Monforte d'Alba) that will appeal to those who love a thick and juicy style of wine. The aromas include lavender, allspice, cinnamon, white chocolate and vanilla. Tart berry flavors backed by crisp acidity and firm tannins give the mouthfeel determination and grit.
Predicted:  41.028854 Actual:  49.0
 
 
Item 412: 
"Algorithms like these, which use random numbers to approximate 
deterministic outcomes, are called Monte Carlo methods, after the 
famous casino city where random numbers produce a deterministic 
outcome (the house wins the gambler's money)."
- Doctor Jubal, The Math Forum
 
 
Item 414: 
<[^>]+>
 model = Sequential()
model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(y_train.shape[1], activation='softmax'))
 Confusion Matrix
[[11  0  0  1  0  0  0  0  0  0]
 [ 0 27  0  0  0  0  1  2  1  1]
 [ 0  2 11  0  0  0  0  0  4  2]
 [ 0  1  0 16  0  0  0  0  0  0]
 [ 0  0  0  0 24  0  1  1  0  0]
 [ 1  0  0  1  0 25  0  0  0  0]
 [ 0  0  0  0  1  0 18  0  0  0]
 [ 0  0  0  0  0  0  0 12  0  0]
 [ 0  0  0  0  0  0  0  0 23  0]
 [ 0  2  0  0  0  0  0  0  0 13]]
 
 
Item 416: 
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
 Number of partitions: 1
Partitioner: None
Partitions structure: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]
 Default parallelism: 2
Number of partitions: 2
Partitioner: None
Partitions structure: [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
 Number of partitions: 15
Partitioner: None
Partitions structure: [[], [0], [1], [], [2], [3], [], [4], [5], [], [6], [7], [], [8], [9]]
 Number of partitions: 2
Partitioner: <pyspark.rdd.Partitioner object at 0x7f97a56fabd0>
Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]
 Element: [0]: 0 % 2 = partition 0
Element: [1]: 1 % 2 = partition 1
Element: [2]: 2 % 2 = partition 0
Element: [3]: 3 % 2 = partition 1
Element: [4]: 4 % 2 = partition 0
Element: [5]: 5 % 2 = partition 1
Element: [6]: 6 % 2 = partition 0
Element: [7]: 7 % 2 = partition 1
Element: [8]: 8 % 2 = partition 0
Element: [9]: 9 % 2 = partition 1
 1
1
4
 Number of partitions: 4
Partitioner: <pyspark.rdd.Partitioner object at 0x7f97a56b7bd0>
Partitions structure: [[('United Kingdom', {'country': 'United Kingdom', 'amount': 100, 'name': 'Bob'}), ('United Kingdom', {'country': 'United Kingdom', 'amount': 15, 'name': 'James'}), ('Germany', {'country': 'Germany', 'amount': 200, 'name': 'Johannes'})], [], [('Poland', {'country': 'Poland', 'amount': 51, 'name': 'Marek'}), ('Poland', {'country': 'Poland', 'amount': 75, 'name': 'Paul'})], []]
 Partitions structure: [[('Poland', {'country': 'Poland', 'amount': 51, 'name': 'Marek'}), ('Germany', {'country': 'Germany', 'amount': 200, 'name': 'Johannes'}), ('Poland', {'country': 'Poland', 'amount': 75, 'name': 'Paul'})], [('United Kingdom', {'country': 'United Kingdom', 'amount': 100, 'name': 'Bob'}), ('United Kingdom', {'country': 'United Kingdom', 'amount': 15, 'name': 'James'})], []]
Total sales for each partition: [326, 115, 0]
 Number of partitions: 2
Partitioner: None
Partitions structure: [[Row(amount=100, country=u'United Kingdom', name=u'Bob'), Row(amount=15, country=u'United Kingdom', name=u'James')], [Row(amount=51, country=u'Poland', name=u'Marek'), Row(amount=200, country=u'Germany', name=u'Johannes'), Row(amount=75, country=u'Poland', name=u'Paul')]]
 After 'repartition()'
Number of partitions: 50
Partitioner: None
Partitions structure: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=200, country=u'Germany', name=u'Johannes')], [], [Row(amount=51, country=u'Poland', name=u'Marek'), Row(amount=75, country=u'Poland', name=u'Paul')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(amount=100, country=u'United Kingdom', name=u'Bob'), Row(amount=15, country=u'United Kingdom', name=u'James')], [], [], [], []]
 Number of partitions: 2
Partitions structure: [[Row(num=0), Row(num=1), Row(num=2), Row(num=3), Row(num=4)], [Row(num=5), Row(num=6), Row(num=7), Row(num=8), Row(num=9)]]
 Number of partitions: 4
Partitions structure: [[Row(num=1), Row(num=6)], [Row(num=2), Row(num=7)], [Row(num=3), Row(num=8)], [Row(num=0), Row(num=4), Row(num=5), Row(num=9)]]
 Number of partitions: 2
Partitioner: <pyspark.rdd.Partitioner object at 0x7f97a5711310>
Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]
 Number of partitions: 2
Partitioner: None
Partitions structure: [[(0, 0), (2, 4), (4, 8), (6, 12), (8, 16)], [(1, 2), (3, 6), (5, 10), (7, 14), (9, 18)]]
 Number of partitions: 2
Partitioner: <pyspark.rdd.Partitioner object at 0x7f97a56b7d90>
Partitions structure: [[(0, 0), (2, 2), (4, 4), (6, 6), (8, 8)], [(1, 1), (3, 3), (5, 5), (7, 7), (9, 9)]]
 Number of partitions: 2
Partitioner: <pyspark.rdd.Partitioner object at 0x7f97a56b7d90>
Partitions structure: [[(0, 0), (2, 4), (4, 8), (6, 12), (8, 16)], [(1, 2), (3, 6), (5, 10), (7, 14), (9, 18)]]
 
 
Item 417: 
Recommend & share..
 
 
Item 422: 
giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca (score = 0.88493)
indri, indris, Indri indri, Indri brevicaudatus (score = 0.00878)
lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens (score = 0.00317)
custard apple (score = 0.00149)
earthstar (score = 0.00127)
 python classify_image.py --image_file images.png
 python classify_image.py --image_file D:/images.png
 
 
Item 427: 
def gen_train(self, seq_len):
    """
    Generates training data
    :param seq_len: length of window
    :return: X_train and Y_train
    """
    for i in range((len(self.stock_train)//seq_len)*seq_len - seq_len - 1):
        x = np.array(self.stock_train.iloc[i: i + seq_len, 1])
        y = np.array([self.stock_train.iloc[i + seq_len + 1, 1]], np.float64)
        self.input_train.append(x)
        self.output_train.append(y)
    self.X_train = np.array(self.input_train)
    self.Y_train = np.array(self.output_train)
 model = tf.keras.models.Sequential()
 model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(100, activation=tf.nn.relu))
model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))

model.compile(optimizer="adam", loss="mean_squared_error")
 model.fit(X_train, Y_train, epochs=100)
 model.evaluate(X_test, Y_test)
 model = tf.keras.Sequential()
model.add(tf.keras.layers.LSTM(20, input_shape=(10, 1), return_sequences=True))
model.add(tf.keras.layers.LSTM(20))
model.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))

model.compile(optimizer="adam", loss="mean_squared_error")

model.fit(X_train, Y_train, epochs=50)

model.evaluate(X_test, Y_test)
 def back_test(strategy, seq_len, ticker, start_date, end_date, dim):
    """
    A simple back test for a given date period
    :param strategy: the chosen strategy. Note to have already formed the model, and fitted with training data.
    :param seq_len: length of the days used for prediction
    :param ticker: company ticker
    :param start_date: starting date
    :type start_date: "YYYY-mm-dd"
    :param end_date: ending date
    :type end_date: "YYYY-mm-dd"
    :param dim: dimension required for strategy: 3dim for LSTM and 2dim for MLP
    :type dim: tuple
    :return: Percentage errors array that gives the errors for every test in the given date range
    """
    data = pdr.get_data_yahoo(ticker, start_date, end_date)
    stock_data = data["Adj Close"]
    errors = []
    for i in range((len(stock_data)//10)*10 - seq_len - 1):
        x = np.array(stock_data.iloc[i: i + seq_len, 1]).reshape(dim) / 200
        y = np.array(stock_data.iloc[i + seq_len + 1, 1]) / 200
        predict = strategy.predict(x)
        while predict == 0:
            predict = strategy.predict(x)
        error = (predict - y) / 100
        errors.append(error)
        total_error = np.array(errors)
    print(f"Average error = {total_error.mean()}")
 
 
Item 433: 
!curl -O "https://s3.amazonaws.com/tripdata/2017[01-12]-citibike-tripdata.csv.zip"
!unzip '*.zip'
!rm *.zip
files = !ls *.csv #For Ipython only
df = concat([read_csv(f, header=None,low_memory=False) for f in files], keys=files) 
 1. Trip Duration (seconds) — How long a trip lasted
2. Start Time and Date - Self explanatory
3. Stop Time and Date - Self explanatory
4. Start Station Name - Self explanatory
5. End Station Name - Self explanatory
6. Station ID - Unique identifier for each station
7. Station Lat/Long - Coordinates
8. Bike ID - unique identifier for each bike
9. User Type (Customer = 24-hour pass or 3-day pass user;  Subscriber = Annual Member) - Customers are usually tourists, subscribers are usually NYC residents
10. Gender (Zero=unknown; 1=male; 2=female) - Usually unknown for customers since they often sign up at a kiosk
11. Year of Birth - Self entered, not validated by an ID.
 trips_df = df.groupby([‘Start Station Name’,’End Station Name’]).size().reset_index(name = ‘Number of Trips’)
 df['Birth Year'] = df.groupby(['Gender','Start Station ID'])['Birth Year'].transform(lambda x: x.fillna(x.median()))
 df = df.drop(df.index[(df['Birth Year'] < df['Birth Year'].mean()-(2*df['Birth Year'].std()))])
 df['Age'] = 2018 - df['Birth Year'];
df['Age'] = df['Age'].astype(int);
 dist = []
for i in range(len(df)):
    dist.append(geopy.distance.vincenty(df.iloc[i]['Start Coordinates'],df.iloc[i]['End Coordinates']).miles)
    if (i%1000000==0):
        print(i)
 df['min_mile'] = round(df['Minutes']/df['Distance'], 2)
 df['mile_hour'] = round(df['Distance']/(df['Minutes']/60),2)
 df = df.drop(df.index[(df['Distance'] == 0)])
 bike_use_df = df.groupby(['Bike ID']).size().reset_index(name = 'Number of Times Used');
bike_use_df = bike_use_df.sort_values('Number of Times Used', ascending = False);
 bike_min_df['Minutes Used'] = df.groupby('Bike ID')['Minutes'].sum()
bike_min_df = bike_min_df.sort_values('Minutes Used', ascending = False)
 df = df.drop(df.index[(df['Trip Duration'] > 2700)])
df_sample = df.sample(frac = 0.1, random_state = 0)
 (Speed * Distance = Trip Duration): Which is why speed is dropped
 def get_date_info(df):
    df['d_week'] = df['Start Time'].dt.dayofweek
    df['m_yr'] = df['Start Time'].dt.month
    df['ToD'] = df['Start Time'].dt.hour
     df['d_week'] = (df['d_week']<5).astype(int)
     df['m_yr'] = df['m_yr'].replace(to_replace=[12,1,2], value = 0)
    df['m_yr'] = df['m_yr'].replace(to_replace=[3,4,5], value = 1)
    df['m_yr'] = df['m_yr'].replace(to_replace=[6,7,8], value = 2)
    df['m_yr'] = df['m_yr'].replace(to_replace=[9,10,11], value = 3)
    
    df['ToD'] = pd.cut(df['ToD'], bins=[-1, 5, 9, 14, 20, 25], labels=['Night','Morning','Afternoon','Evening','Night1'])
    df['ToD'] = df['ToD'].replace(to_replace='Night1', value = 'Night')
    df['ToD'] = df['ToD'].cat.remove_unused_categories()
    
    df['m_yr'] = df['m_yr'].astype('category')
    df['d_week'] = df['d_week'].astype('category')
 return(df)
 def get_speed_distance(df):
     df['Start Station Name'] = df['Start Station Name'].astype(str)
    df['End Station Name'] = df['End Station Name'].astype(str)
    df['Trip'] = df['Start Station Name'] + ' to ' + df['End Station Name']
    df['Trip'] = df['Trip'].astype('category')
    
    df['avg_speed'] = df.groupby(['Trip','User Type'])['mile_hour'].transform('mean')
    df['avg_duration'] = df.groupby(['Trip','User Type'])['Trip Duration'].transform('median')
    
    return df
 def get_weather(df):
    df['DATE'] = to_datetime(df['Start Time'].dt.date)
    df = df.merge(df_weather, on = 'DATE', how = 'left')
    return df
 CV accuracy: 0.874 +/- 0.001
 CV accuracy: 0.852 +/- 0.000
 
 
Item 435: 
go run main.go -teachratio 0.8 -src ./testdata/fakenews/
 
 
Item 439: 
+---------------+-----+
| Classes       |   2 |
+---------------+-----+
| Num Samples   | 569 |
+---------------+-----+
| Num Benign    | 357 |
+---------------+-----+
| Num Malignant | 212 |
+---------------+-----+
 
 
Item 443: 
1::595::5::978824268  
 Movie Nr. : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
 Rating:     5 0 2 4 0 0 2 1 5  1  0  4  5  1  3
 Movie Nr. : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
 Rating:     5 0 2 4 0 0 2 1 5  0  0  0  0  0  0
 Movie Nr. : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
 Rating:     0 0 0 0 0 0 0 0 0  1  0  4  5  1  3
 epoch_nr: 0,  train_loss: 1.169, test_loss: 1.020
epoch_nr: 10, train_loss: 0.936, test_loss: 0.959
epoch_nr: 20, train_loss: 0.889, test_loss: 0.931
epoch_nr: 30, train_loss: 0.873, test_loss: 0.923
epoch_nr: 40, train_loss: 0.859, test_loss: 0.925
epoch_nr: 50, train_loss: 0.844, test_loss: 0.929
 
 
Item 445: 
They refuse to permit us to obtain the refuse permit.
 >>> text = word_tokenize("They refuse to permit us to obtain the refuse permit")
>>> nltk.pos_tag(text)
[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'),
('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]
 
 
Item 454: 
library(bigrquery)
library(plotly)
project <- "your_project_id"
 sql <- "SELECT  
  substr(cast(pickup_datetime as String), 1, 7) as date
  ,payment_type as type 
  ,sum(total_amount) as amount
FROM `nyc-tlc.yellow.trips`
group by 1, 2"
 df <- query_exec(sql, project = project, use_legacy_sql = FALSE)
plot_ly(df, x = ~date, y = ~amount, color = ~type) %>% add_lines() 
 ,sum(case when payment_type in ('CSH', 'CAS') then amount else 0 end) as cash_payments
 total <- aggregate(df$Amount, by=list(Category=df$Date), FUN=sum)
plot_ly(total, x = ~Category, y = ~x) %>% add_lines()
 ---
title: "Business Intelligence"
author: "Ben Weber"
date: "May 21, 2018"
output: html_document
---
## Taxi Payments 
R Markdown can outputs reports as PDF or HTML.
 ```{r echo=FALSE, message=FALSE, warning=FALSE}
library(bigrquery)
library(plotly)
project <- "your_project_id"
 sql <- "SELECT  
  substr(cast(pickup_datetime as String), 1, 7) as date
  ,payment_type as type 
  ,sum(total_amount) as amount
FROM `nyc-tlc.yellow.trips`
group by 1, 2"
 df <- query_exec(sql, project = project, use_legacy_sql = FALSE)
plot_ly(df, x = ~date, y = ~amount, color = ~type) %>% add_lines()
```
 Rscript -e "rmarkdown::render('BI.Rmd')"
 library(shiny)
library(bigrquery)
library(plotly)
project <- "your_project_id"
 sql <- "SELECT  
substr(cast(pickup_datetime as String), 1, 7) as date
,payment_type as type 
,sum(total_amount) as amount
FROM `nyc-tlc.yellow.trips`
group by 1, 2"
df <- query_exec(sql, project = project, use_legacy_sql = FALSE)
 server <- function(input, output) {
  output$plot <-  renderPlotly({
    plot_ly(df[df$date >= input$year, ], x = ~date, 
      y = ~amount, color = ~type) %>% add_lines()
  })
}
 ui <- shinyUI(fluidPage(
  sidebarLayout(
    sidebarPanel(
      sliderInput("year", "Start Year:", 
        min = 2009, max = 2015, value = 2012)
    ),
    mainPanel(plotlyOutput("plot"))
  )
))
 shinyApp(ui = ui, server = server)
 create table tracking.logins as (
  select eventVersion,server_time 
    ,JSON_EXTRACT_SCALAR(message, '$.userID') as userID
    ,JSON_EXTRACT_SCALAR(message, '$.deviceType') as deviceType
  from tracking.raw_events
  where eventType = 'Login'
)
 insert into tracking.logins
    (eventVersion,server_time, userID, deviceType)
with lastUpdate as (
  select max(server_time) as updateTime
  from tracking.logins
)
select eventVersion,server_time 
  ,JSON_EXTRACT_SCALAR(message, '$.userID') as userID
  ,JSON_EXTRACT_SCALAR(message, '$.deviceType') as deviceType
from tracking.raw_events e
join lastUpdate l 
  on e.server_time > updateTime
where eventType = 'Login'
 create table metrics.dau as (
  select substr(server_time, 1, 10) as Date
    ,deviceType, count(distinct userID) as DAU
  from `tracking.logins` 
  group by 1, 2
  order by 1, 2
)
 bq query --flagfile=/etls/login_etl.sql
 
 
Item 455: 
def step_decay(epoch):
    lr_init = 0.001
    drop = 0.5
    epochs_drop = 4.0
    lr_new = lr_init * \
             math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lr_new
 
 
Item 459: 
tf_files/retrained_graph.pb 
  tf_files/retrained_labels.txt
 python -m tensorflow.python.tools.optimize_for_inference \
  --input=tf_files/retrained_graph.pb \
  --output=tf_files/optimized_graph.pb \
  --input_names="input" \
  --output_names="final_result"
 python -m scripts.label_image \
  --graph=tf_files/retrained_graph.pb \
  --image=tf_files/flower_photos/daisy/3475870145_685a19116d.jpg
 python -m scripts/label_image \
    --graph=tf_files/optimized_graph.pb \
    --image=tf_files/flower_photos/daisy/3475870145_685a19116d.jpg
 python -m scripts.quantize_graph \
  --input=tf_files/optimized_graph.pb \
  --output=tf_files/rounded_graph.pb \
  --output_node_names=final_result \
  --mode=weights_rounded
 gzip -c tf_files/rounded_graph.pb > tf_files/rounded_graph.pb.gz

gzip -l tf_files/rounded_graph.pb.gz
 target 'YourProjectName'
pod 'TensorFlow-experimental'
 mkdir -p ~/graphs
curl -o ~/graphs/inception5h.zip \
 https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip \
 && unzip ~/graphs/inception5h.zip -d ~/graphs/inception5h
cp ~/graphs/inception5h/* tensorflow/examples/ios/benchmark/data/
cp ~/graphs/inception5h/* tensorflow/examples/ios/camera/data/
cp ~/graphs/inception5h/* tensorflow/examples/ios/simple/data/
 cd tensorflow/examples/ios/simple
pod install
open tf_simple_example.xcworkspace #note .xcworkspace,not .xcodeproj
                                     
 cp tf_files/rounded_graph.pb android/tfmobile/assets/graph.pb
cp tf_files/retrained_labels.txt android/tfmobile/assets/labels.txt
 private static final String INPUT_NAME = "input";
private static final String OUTPUT_NAME = "final_result";
 
 
Item 465: 
library(plyr)
library(corrplot)
library(ggplot2)
library(gridExtra)
library(ggthemes)
library(caret)
library(MASS)
library(randomForest)
library(party)
 churn <- read.csv('Telco-Customer-Churn.csv')
str(churn)
 sapply(churn, function(x) sum(is.na(x)))
 churn <- churn[complete.cases(churn), ]
 cols_recode1 <- c(10:15)
for(i in 1:ncol(churn[,cols_recode1])) {
        churn[,cols_recode1][,i] <- as.factor(mapvalues
                                              (churn[,cols_recode1][,i], from =c("No internet service"),to=c("No")))
}
 churn$MultipleLines <- as.factor(mapvalues(churn$MultipleLines, 
                                           from=c("No phone service"),
                                           to=c("No")))
 min(churn$tenure); max(churn$tenure)
 group_tenure <- function(tenure){
    if (tenure >= 0 & tenure <= 12){
        return('0-12 Month')
    }else if(tenure > 12 & tenure <= 24){
        return('12-24 Month')
    }else if (tenure > 24 & tenure <= 48){
        return('24-48 Month')
    }else if (tenure > 48 & tenure <=60){
        return('48-60 Month')
    }else if (tenure > 60){
        return('> 60 Month')
    }
}
 churn$tenure_group <- sapply(churn$tenure,group_tenure)
churn$tenure_group <- as.factor(churn$tenure_group)
 churn$SeniorCitizen <- as.factor(mapvalues(churn$SeniorCitizen,
                                      from=c("0","1"),
                                      to=c("No", "Yes")))
 churn$customerID <- NULL
churn$tenure <- NULL
 numeric.var <- sapply(churn, is.numeric)
corr.matrix <- cor(churn[,numeric.var])
corrplot(corr.matrix, main="\n\nCorrelation Plot for Numerical Variables", method="number")
 churn$TotalCharges <- NULL
 p1 <- ggplot(churn, aes(x=gender)) + ggtitle("Gender") + xlab("Gender") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p2 <- ggplot(churn, aes(x=SeniorCitizen)) + ggtitle("Senior Citizen") + xlab("Senior Citizen") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p3 <- ggplot(churn, aes(x=Partner)) + ggtitle("Partner") + xlab("Partner") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p4 <- ggplot(churn, aes(x=Dependents)) + ggtitle("Dependents") + xlab("Dependents") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
grid.arrange(p1, p2, p3, p4, ncol=2)
 p5 <- ggplot(churn, aes(x=PhoneService)) + ggtitle("Phone Service") + xlab("Phone Service") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p6 <- ggplot(churn, aes(x=MultipleLines)) + ggtitle("Multiple Lines") + xlab("Multiple Lines") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p7 <- ggplot(churn, aes(x=InternetService)) + ggtitle("Internet Service") + xlab("Internet Service") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p8 <- ggplot(churn, aes(x=OnlineSecurity)) + ggtitle("Online Security") + xlab("Online Security") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
grid.arrange(p5, p6, p7, p8, ncol=2)
 p9 <- ggplot(churn, aes(x=OnlineBackup)) + ggtitle("Online Backup") + xlab("Online Backup") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p10 <- ggplot(churn, aes(x=DeviceProtection)) + ggtitle("Device Protection") + xlab("Device Protection") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p11 <- ggplot(churn, aes(x=TechSupport)) + ggtitle("Tech Support") + xlab("Tech Support") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p12 <- ggplot(churn, aes(x=StreamingTV)) + ggtitle("Streaming TV") + xlab("Streaming TV") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
grid.arrange(p9, p10, p11, p12, ncol=2)
 p13 <- ggplot(churn, aes(x=StreamingMovies)) + ggtitle("Streaming Movies") + xlab("Streaming Movies") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p14 <- ggplot(churn, aes(x=Contract)) + ggtitle("Contract") + xlab("Contract") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p15 <- ggplot(churn, aes(x=PaperlessBilling)) + ggtitle("Paperless Billing") + xlab("Paperless Billing") + 
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p16 <- ggplot(churn, aes(x=PaymentMethod)) + ggtitle("Payment Method") + xlab("Payment Method") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
p17 <- ggplot(churn, aes(x=tenure_group)) + ggtitle("Tenure Group") + xlab("Tenure Group") +
  geom_bar(aes(y = 100*(..count..)/sum(..count..)), width = 0.5) + ylab("Percentage") + coord_flip() + theme_minimal()
grid.arrange(p13, p14, p15, p16, p17, ncol=2)
 intrain<- createDataPartition(churn$Churn,p=0.7,list=FALSE)
set.seed(2017)
training<- churn[intrain,]
testing<- churn[-intrain,]
 dim(training); dim(testing)
 LogModel <- glm(Churn ~ .,family=binomial(link="logit"),data=training)
print(summary(LogModel))
 anova(LogModel, test="Chisq")
 testing$Churn <- as.character(testing$Churn)
testing$Churn[testing$Churn=="No"] <- "0"
testing$Churn[testing$Churn=="Yes"] <- "1"
fitted.results <- predict(LogModel,newdata=testing,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != testing$Churn)
print(paste('Logistic Regression Accuracy',1-misClasificError))
 print("Confusion Matrix for Logistic Regression"); table(testing$Churn, fitted.results > 0.5)
 library(MASS)
exp(cbind(OR=coef(LogModel), confint(LogModel)))
 tree <- ctree(Churn~Contract+tenure_group+PaperlessBilling, training)
plot(tree)
 pred_tree <- predict(tree, testing)
print("Confusion Matrix for Decision Tree"); table(Predicted = pred_tree, Actual = testing$Churn)
 p1 <- predict(tree, training)
tab1 <- table(Predicted = p1, Actual = training$Churn)
tab2 <- table(Predicted = pred_tree, Actual = testing$Churn)
print(paste('Decision Tree Accuracy',sum(diag(tab2))/sum(tab2)))
 rfModel <- randomForest(Churn ~., data = training)
print(rfModel)
 pred_rf <- predict(rfModel, testing)
caret::confusionMatrix(pred_rf, testing$Churn)
 plot(rfModel)
 t <- tuneRF(training[, -18], training[, 18], stepFactor = 0.5, plot = TRUE, ntreeTry = 200, trace = TRUE, improve = 0.05)
 rfModel_new <- randomForest(Churn ~., data = training, ntree = 200, mtry = 2, importance = TRUE, proximity = TRUE)
print(rfModel_new)
 pred_rf_new <- predict(rfModel_new, testing)
caret::confusionMatrix(pred_rf_new, testing$Churn)
 varImpPlot(rfModel_new, sort=T, n.var = 10, main = 'Top 10 Feature Importance')
 
 
Item 467: 
web_address = 'https://www.fifaindex.com/teams/fifa'
 df = pd.DataFrame()
 for day in range(1,260,1):
     for pag in range(1,30):
     
         source_address = web_address + '05_' + str(day) + '/' + str(pag) + '/' + '?type=1'
         print('Day:', str(day))
         print(pag)
 try:
             soup = get_soup(source_address)
             result_list =  soup.find('div', {'id': 'no-more-tables'})
         except:
             print('Page not found.')
             break
 date = str(soup.find('ol', {'class': 'breadcrumb'}))
 if df.empty:
             df = pd.read_html(str(result_list))[0]
             df['date'] = date
         else:
             temp_df = pd.read_html(str(result_list))[0]
             temp_df['date'] = date
             df = df.append(temp_df)
 df = read_pickle('team_stats.pickle')
df.head()
 results = pd.read_csv('match_results.csv')
results.head()
 results = results.drop(['city', 'tournament', 'country'], axis=1)
results.home_team = results.home_team.apply(text_norm) #lower_case
results.away_team = results.away_team.apply(text_norm) #lower_case
results.index = pd.DatetimeIndex(results.date).year
results = results.drop('date', 1)
results.head()
 results = results.loc[2004:2017]
 df_teams = list(df.name.unique())
results = results.reset_index()
 for index, row in results.iterrows():
    if row.home_team not in df_teams:
        results.loc[index, 'home_team'] = None
    if row.away_team not in df_teams:
        results.loc[index, 'away_team'] = None
        
results = results.dropna()
 results['score'] = results.home_score - results.away_score
results = results.drop(['home_score', 'away_score', 'home_team', 'away_team'], 1)
 results['winner'] = None
results['winner'][results.score > 0] = 1
results['winner'][results.score < 0] = -1
results['winner'][results.score == 0] = 0
 results = results[results.winner != 0]
 results['att'] = results['att1'] - results['att2']
results['def'] = results['def1'] - results['def2']
results['mid'] = results['mid1'] - results['mid2']
results['ovr'] = results['ovr1'] - results['ovr2']
 to_drop = results[results.winner == 1].sample(247)
results = results.drop(labels=to_drop.index, axis=0)
 lr = LogisticRegression()
lr.fit(x_train, y_train)
 rf = RandomForestClassifier()
rf.fit(x_train, y_train)
 svc = SVC(kernel='linear')
svc.fit(x_train, y_train)
 wc = read_pickle('world_cup_teams.pickle')
wc.head()
 def match(wc, team1, team2, model):
    
    match = pd.DataFrame(columns=['att1','def1','mid1','ovr1','att2','def2','mid2','ovr2'], index=[0])
    
    match['att1'] = wc[wc.name == team1]['att'].iloc[0]
    match['def1'] = wc[wc.name == team1]['def'].iloc[0]
    match['mid1'] = wc[wc.name == team1]['mid'].iloc[0]
    match['ovr1'] = wc[wc.name == team1]['ovr'].iloc[0]
 match['att2'] = wc[wc.name == team2]['att'].iloc[0]
    match['def2'] = wc[wc.name == team2]['def'].iloc[0]
    match['mid2'] = wc[wc.name == team2]['mid'].iloc[0]
    match['ovr2'] = wc[wc.name == team2]['ovr'].iloc[0]
    
    match['att'] = match['att1'] - match['att2']
    match['def'] = match['def1'] - match['def2']
    match['mid'] = match['mid1'] - match['mid2']
    match['ovr'] = match['ovr1'] - match['ovr2']
    
    match = match[['att', 'def', 'mid', 'ovr']]
    
    match_array = match.values
    
    prediction = model.predict(match_array)
    
    winner = None
    
    if prediction == 1:
        winner = team1
    elif prediction == -1:
        winner = team2
    
    return winner
 match(wc, 'brazil', 'spain', svc)
 >>> 'spain'
 def match(wc, team1, team2, model, random_scale=5):
    
    match = pd.DataFrame(columns=['att1','def1','mid1','ovr1','att2','def2','mid2','ovr2'], index=[0])
    
    att1 = wc[wc.name == team1]['att'].iloc[0]
    def1 = wc[wc.name == team1]['def'].iloc[0]
    mid1 = wc[wc.name == team1]['mid'].iloc[0]
    ovr1 = wc[wc.name == team1]['ovr'].iloc[0]
 att2 = wc[wc.name == team2]['att'].iloc[0]
    def2 = wc[wc.name == team2]['def'].iloc[0]
    mid2 = wc[wc.name == team2]['mid'].iloc[0]
    ovr2 = wc[wc.name == team2]['ovr'].iloc[0]
    
    match['att1'] = np.random.normal(att1, scale=random_scale)
    match['def1'] = np.random.normal(def1, scale=random_scale)
    match['mid1'] = np.random.normal(mid1, scale=random_scale)
    match['ovr1'] = np.random.normal(ovr1, scale=random_scale)
 match['att2'] = np.random.normal(att2, scale=random_scale)
    match['def2'] = np.random.normal(def2, scale=random_scale)
    match['mid2'] = np.random.normal(mid2, scale=random_scale)
    match['ovr2'] = np.random.normal(ovr2, scale=random_scale)
    
    match['att'] = match['att1'] - match['att2']
    match['def'] = match['def1'] - match['def2']
    match['mid'] = match['mid1'] - match['mid2']
    match['ovr'] = match['ovr1'] - match['ovr2']
    
    match = match[['att', 'def', 'mid', 'ovr']]
    
    match_array = match.values
    
    prediction = model.predict(match_array)
    
    winner = None
    
    if prediction == 1:
        winner = team1
    elif prediction == -1:
        winner = team2
    
    return winner
 def simulate_matches(team1, team2, n_matches=10000):
    
    match_results = []
    for i in range(n_matches):
        match_results.append(match(wc, team1, team2, svc, random_scale=5))
        
    team1_proba = match_results.count(team1)/len(match_results)*100
    team2_proba = match_results.count(team2)/len(match_results)*100
    
    print(team1, str(round(team1_proba, 2)) + '%')
    print(team2, str(round(team2_proba,2)) + '%')
    print('-------------------------')
    print()
    
    if team1_proba > team2_proba:
        overall_winner = team1
    else:
        overall_winner = team2
    
    return {'team1': team1,
            'team2': team2,
            'team1_proba': team1_proba, 
            'team2_proba': team2_proba, 
            'overall_winner': overall_winner,
            'match_results': match_results}
 simulation_test = simulate_matches('croatia', 'denmark', n_matches=10000)
 p_list = []
for i in range(len(simulation_test['match_results'])):
    denmark = simulation_test['match_results'][:i].count('denmark') / (i+1) * 100
    croatia = simulation_test['match_results'][:i].count('croatia') / (i+1) * 100
    p_list.append(denmark - croatia)
 spain_proba = 0.817 * 0.862 * 0.718 * 0.593 * 100 * (0.7 ** 4)
 print('Chance of Spain winning:', str(round(spain_proba,2)) + '%')
 
 
Item 468: 
def se_block(in_block, ch, ratio=16):
    x = GlobalAveragePooling2D()(in_block)
    x = Dense(ch//ratio, activation='relu')(x)
    x = Dense(ch, activation='sigmoid')(x)
    return multiply()([in_block, x])
 
 
Item 477: 
Recommend & share..
 
 
Item 479: 
p = p1 * p2 / (p1 * p2 + (1 - p1) * (1 - p2))
 
 
Item 480: 
#1 Rank of a Matrix. 
#2 Matrix Vector products. 
#3 Column Spaces and Null Spaces of a matrix. 
#4 Eigen Values and Eigen Vectors.
#5 SVD factorization of a matrix. 
 Probability Theory: 
#1 Counting and Combinatorial methods.
#2 Bayes’ Theorem.
#3 Random Variables.
#4 Expection
#5 Variance
#6 Conditional and Joint Distributions. 
#7 Moment Generating Functions.
#8 Exponential Family of Distributions
 Statistics: 
#1 Maximum Likelihood Estimation
#2 MAP
#3 Prior and Posterior
#4 Sampling methods 
#5 Gibbs
#6 Mean, Mode, Medium, Variance
 #1 Vector-valued functions
#2 Partial-Derivatives
#3 Gradient
#4 Directional Gradient
#5 Hessian
#6 Jacobian
#7 Laplacian
#8 Lagrange Multipliers

 #1 Entropy
#2 Mutual Information
#3 Information Gain
#4 KL Divergence
 
 
Item 485: 
 AlexNet on CPU — 7 images/sec
 AlexNet on GPU — 208 images/sec
 Equihash on GPU — 110 Hashes/sec
 AlexNet+Equihash on GPU — 18 images/sec and 110 Hashes/sec
 Hashrate(ZCash) = 650,000,000 H/s
Hashrate(NVIDIA GTX 1080) = 500 H/s
 Number of GPUs = 1,360,000  (GTX 1080 equivalent)
 pip3 install snark
snark login
 pip3 install snark
 snark start
 
 
Item 486: 
def IDA(H):
   A <- random initialization
   repeat:
      A <- Distill(Amplify(H, A))
 def Distill(overseer):
   """
   Returns an AI trained using narrow, robust techniques to perform 
   a task that the overseer already understands how to perform.
   """
 def Amplify(human, AI):
   """
   Interactive process in which human uses many calls to AI to 
   improve on human's native performance at relevant task(s).
   """
 
 
Item 487: 
Plug 'jalvesaq/Nvim-R'
 " in your .vimrc /init.vim
 “ remapping the basic :: send line
nmap , <Plug>RDSendLine
 “ remapping selection :: send multiple lines
vmap , <Plug>RDSendSelection
 “ remapping selection :: send multiple lines + echo lines
vmap ,e <Plug>RESendSelection
 “ install plugin :: using vim-plug
Plug ‘rizzatti/dash.vim’
 “ remap search key
nmap <silent> <leader>d <Plug>DashSearch<CR>
 Plug ‘roxma/nvim-completion-manager’
Plug ‘gaalcaras/ncm-R’
 “ Optional: for snippet support
Plug ‘sirver/UltiSnips’
 Plug ‘w0rp/ale’
 “ settings :: Nvim-R plugin
 “ R output is highlighted with current colorscheme
let g:rout_follow_colorscheme = 1
 “ R commands in R output are highlighted
let g:Rout_more_colors = 1
 
 
Item 490: 
encoded = MaxPooling2D((2, 2), padding='same', name='encoder')(x)
 autoencoder.save('autoencoder.h5')
 
 
Item 492: 
sample_df["Conditions"] = sample_df["Conditions"].fillna('Unknown')

weather_dict = {'Overcast' : 0, 
                'Haze' : 0,
                'Partly Cloudy' : 0, 
                'Mostly Cloudy' : 0, 
                'Scattered Clouds' : 0, 
                'Light Freezing Fog' : 0,
                
                'Unknown' : 1,
                'Clear' : 2, 
                
                'Heavy Rain' : 3, 
                'Rain' : 3, 
                'Light Freezing Rain' : 3,
                'Light Rain' : 3, 
                
                'Heavy Snow' : 4,
                'Light Snow' : 4,
                'Snow' : 4}
 sample_df["Conditions"] = sample_df["Conditions"].apply(lambda x: weather_dict[x])
 bst = lgb.train(params,
                dtrain,
                num_boost_round = nrounds,
                valid_sets = [dtrain, dval],
                valid_names = ['train', 'valid'],
                categorical_feature = [20, 24]
                )
 dtrain = xgb.DMatrix(X_train, np.log(y_train+1))
dval = xgb.DMatrix(X_val, np.log(y_val+1))

watchlist = [(dval, 'eval'), (dtrain, 'train')]
 gbm = xgb.train(params,
                dtrain,
                num_boost_round = nrounds,
                evals = watchlist,
                verbose_eval = True
                )
 
 
Item 494: 
def linear_regression(X, y, m_current=0, b_current=0, epochs=1000, learning_rate=0.0001):
     N = float(len(y))
     for i in range(epochs):
          y_current = (m_current * X) + b_current
          cost = sum([data**2 for data in (y-y_current)]) / N
          m_gradient = -(2/N) * sum(X * (y - y_current))
          b_gradient = -(2/N) * sum(y - y_current)
          m_current = m_current - (learning_rate * m_gradient)
          b_current = b_current - (learning_rate * b_gradient)
     return m_current, b_current, cost
 for i in range(epochs):
     y_current = (m_current * X) + b_current
     cost = sum([data**2 for data in (y-y_current)]) / N
     m_gradient = -(2/N) * sum(X * (y - y_current))
     b_gradient = -(2/N) * sum(y - y_current)
     m_current = m_current - (learning_rate * m_gradient)
     b_current = b_current - (learning_rate * b_gradient)
 
 
Item 497: 
git clone https://github.com/YadiraF/PRNet
cd PRNet
 
 
Item 500: 
|scene|participants:archer,malory,trexler|deaths:0|tags:drinking|
MALORY: Yes, ODIN.
ARCHER: Why would ODIN wanna buy--? Oh, my God, is Len Trexler behind this?
MALORY: He made me a very attractive offer.
ARCHER: Yeah, I bet. That's because he wants to-- Bleh!
MALORY: [SIGHS]
 const stackArea = radialArea()
   .angle((d, i) => xScale(i) / 1200 * Math.PI * 2)
   .innerRadius(d => yScale(d.y1))
   .outerRadius(d => yScale(d.y2))
   .defined(d => d.y1 — d.y2 !== 0)  
   .curve(curveCatmullRom.alpha(0.75))
 const deathArc = arc()
   .innerRadius(d => yScale(d.y2) + 10)
   .outerRadius(d => yScale(d.y2) + 15)
 g.selectAll("g.deaths")
   .each(function (d,i) {
   if (d.deaths > 0) {
      const bumpMax = max(bumpedData.map(p => p.data[i * 3].y2))
      const arcCenter = deathArc.centroid({
         startAngle: xScale(i * 3 + 1) / 1200 * Math.PI * 2,  
         endAngle: xScale(i * 3 + 1) / 1200 * Math.PI * 2,   
         y2: bumpMax, 
         r: d.deaths * 2
      }) 
      
      const skullG = select(this)
         .append("g")
         .attr("class", "skull")
         .attr("transform", "translate(" + (arcCenter[0] - 7.5) + "," + (arcCenter[1] - 7.5) + ") rotate(" + (xScale(i * 3 + 1) / 1200 * Math.PI * 2 * 57.2958) + ",7.5,7.5)")
       skullG
         .append("image")
         .attr("xlink:href", skull)
         .attr("width", 15)
         .attr("height", 15)
         .attr("y", -2)
})
 
 
Item 503: 
Dt0->1 = Predicted delivery time from store to D1.
Dt1->2 = Predicted delivery time from D1 to D2.
Qt0->1 = 90th percentile prediction from store to D1.
Qt1->2 = 90th percentile prediction from D1 to D2.
B0->1 = Qt0->1 - Dt0->1 = Delivery Time Buffer from store to D1.
B1->2 = Qt1->2 - Dt1->2 = Delivery Time Buffer from D1 to D2.
B0->2 = Buffer that we need to use to make sure D2 will not be late.
 
 
Item 504: 
#fastai function
val_idxs = get_cv_idxs(len(ratings)) #get validation indices 
 wd=2e-4 #weight decay
n_factors = 50 #dimension of embedding vector
 #fastai function
cf = CollabFilterDataset.from_csv(path, 'ratings.csv', 'userId', 'movieId', 'rating') #creating a custom data loader
 #create a learner (model) and specify the batch size and optimizer 
learn = cf.get_learner(n_factors, val_idxs, 64, opt_fn=optim.Adam) 
#fastai function
 #training with learning rate as 1e-2 
learn.fit(1e-2, 2, wds=wd, cycle_len=1, cycle_mult=2, use_wd_sched=True) 
#fastai function
 preds = learn.predict() #prediction on validation
math.sqrt(metrics.mean_squared_error(y,preds)) #calculating RMSE
 y=learn.data.val_y #actual ratings for validation
sns.jointplot(preds, y, kind='hex', stat_func=None);
 
 
Item 506: 
token= ‘aiufniqaefncqiuhfencioaeusKJBNfljabicnlkjshniuwnscslkjjndfi’
 graph = facebook.GraphAPI(access_token=token, version = 2.7)
events = graph.request(‘/search?q=Poetry&type=event&limit=10000’)
 eventList = events[‘data’]
 eventid = eventList[1][‘id’]
 event1 = graph.get_object(id=eventid,
 fields=’attending_count,can_guests_invite,category,cover,declined_count,description,end_time,guest_list_enabled,interested_count,is_canceled,is_page_owned,is_viewer_admin,maybe_count,noreply_count,owner,parent_group,place,ticket_uri,timezone,type,updated_time’)
attenderscount = event1[‘attending_count’]
declinerscount = event1[‘declined_count’]
interestedcount = event1[‘interested_count’]
maybecount = event1[‘maybe_count’]
noreplycount = event1[‘noreply_count’]
 attenders = requests.get(“https://graph.facebook.com/v2.7/"+eventid+"/attending?access_token="+token+”&limit=”+str(attenderscount)) 
 attenders_json = attenders.json()
 admins = requests.get(“https://graph.facebook.com/v2.7/"+eventid+"/admins?access_token="+token)
 admins_json = admins.json()
 
 
Item 522: 
x = [23, 40, 6, 74, 38, 1, 70]
 observations = [23, 40, 6, 74, 38, 1, 70]
mean = 36
difference_from_the_mean = [13, 4, 30, 38, 2, 35, 34]
square_of_the_differences = [169, 16, 900, 1444, 4, 1225, 1156]
variance = (169+16+900+1444+4+1225+1156)/7 = 4914/7 = 702
 
 
Item 523: 
df = pd.read_csv('parkinsons.data')
 features = df.loc[:, df.columns != 'status'].values[:, 1:]
labels = df.loc[:, 'status'].values
 scaler = MinMaxScaler((-1, 1))
X = scaler.fit_transform(features)
 X_r, X_s, Y_r, Y_s = train_test_split(X, labels, test_size=0.14)
 model = XGBClassifier()
model.fit(X_r, Y_r)
 Y_hat = [round(yhat) for yhat in model.predict(X_test)]
print(accuracy_score(Y_test, Y_hat))
 
 
Item 524: 
conda env create -f environment.yaml 
 //Instead of using
model.fit()
// Use
model.fit_generator()
 python drive.py model.h5
 
 
Item 526: 
[
  {
    "action": "data",
    "src": [
      "<Your project name>/*"
    ],
    "dst": "$sample",
    "settings": {
      "classes_mapping": "default"
    }
  },
  {
    "action": "tag",
    "src": [
      "$sample"
    ],
    "dst": "$sample2",
    "settings": {
      "tag": "train"
    }
  },
  {
    "action": "background",
    "src": [
      "$sample2"
    ],
    "dst": "$sample3",
    "settings": {
      "class": "bg"
    }
  },
  {
    "action": "segmentation",
    "src": [
      "$sample3"
    ],
    "dst": "Carvana",
    "settings": {
      "gt_machine_color": {
        "car": [255, 255, 255],
        "bg": [0, 0, 0]
      },
      "tag2part": {
        "train": "train"
      },
      "txt_generation": {
        "prefix": "."
      }
    }
  }
]
 
 
Item 527: 
"citi\nREWARDS\nxxxx xxxx xxxx xxxx\nVALID THRU\n04/21\nHARSHIT DWIVEDI\nPlatinum"
 
 
Item 531: 
embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/1")
 embeddings = embed([
    "The quick brown fox jumps over the lazy dog.",
    "I am a sentence for which I would like to get its embedding"])

print session.run(embeddings)
 
 
Item 532: 
bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
  --in_graph=/your/.pb/file \
  --outputs="output_node_name" \
  --out_graph=/the/quantized/.pb/file \
  --transforms='quantize_weights'
 grep "op: " PATH/TO/mygraph.txt | sort | uniq | sed -E 's/^.+"(.+)".?$/\1/g' 
 bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \
--crosstool_top=//external:android/crosstool \
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
--cpu=armeabi-v7a
 bazel build 
//tensorflow/contrib/android:android_tensorflow_inference_java
 bazel-bin/tensorflow/contrib/android/libandroid_tensorflow_inference_java.jar
 
 
Item 533: 
STREET                            325084
RESIDENCE                         223854
APARTMENT                         179444
SIDEWALK                          158478
OTHER                              53474
PARKING LOT/GARAGE(NON.RESID.)     40907
ALLEY                              31239
RESIDENTIAL YARD (FRONT/BACK)      30209
SMALL RETAIL STORE                 28209
SCHOOL, PUBLIC, BUILDING           25474
Name: Location Description, dtype: int64
 plt.figure(figsize = (15, 10))
 sns.countplot(y= 'Location Description', data = df, order = df['Location Description'].value_counts().iloc[:10].index)
 chicago_map_crime = folium.Map(location=[41.895140898, -87.624255632],
                        zoom_start=13,
                        tiles="CartoDB dark_matter")

for i in range(500):
    lat = CR_index['LocationCoord'].iloc[i][0]
    long = CR_index['LocationCoord'].iloc[i][1]
    radius = CR_index['ValueCount'].iloc[i] / 45
    
    if CR_index['ValueCount'].iloc[i] > 1000:
        color = "#FF4500"
    else:
        color = "#008080"
    
    popup_text = """Latitude : {}<br>
                Longitude : {}<br>
                Criminal Incidents : {}<br>"""
    popup_text = popup_text.format(lat,
                               long,
                               CR_index['ValueCount'].iloc[i]
                               )
    folium.CircleMarker(location = [lat, long], popup= popup_text,radius = radius, color = color, fill = True).add_to(chicago_map_crime)
 theft_in_months = pd.DataFrame({"thefts" : df_theft['Month'].value_counts(), "month" : df_theft["Month"].value_counts().index}, index = range(12))
 theft_in_months.fillna(0, inplace=True)
theft_in_months = theft_in_months.sort_values(['month'], ascending=[1])
 theft_in_months.head()
 
 
Item 537: 
network = input_data(shape=[None, 4, 1], name='input')
network = fully_connected(network, 1, activation='linear')
network = regression(network, optimizer='adam', learning_rate=1e-2, loss='mean_square', name='target')
model = tflearn.DNN(network)
 network = input_data(shape=[None, 5, 1], name='input')
network = fully_connected(network, 25, activation='relu')
network = fully_connected(network, 1, activation='linear')
network = regression(network, optimizer='adam', learning_rate=1e-2, loss='mean_square', name='target')
model = tflearn.DNN(network)
 
 
Item 538: 
def normalEqn(X,y):
    """ 
    Computes the closed-form solution to linear regression
    """
    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
    return theta
 
 
Item 548: 
def add_brightness(image):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    image_HLS = np.array(image_HLS, dtype = np.float64) 
    random_brightness_coefficient = np.random.uniform()+0.5 ## generates value between 0.5 and 1.5
    image_HLS[:,:,1] = image_HLS[:,:,1]*random_brightness_coefficient ## scale pixel values up or down for channel 1(Lightness)
    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255
    image_HLS = np.array(image_HLS, dtype = np.uint8)
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB
 def generate_shadow_coordinates(imshape, no_of_shadows=1):
    vertices_list=[]
    for index in range(no_of_shadows):
        vertex=[]
        for dimensions in range(np.random.randint(3,15)): ## Dimensionality of the shadow polygon
            vertex.append(( imshape[1]*np.random.uniform(),imshape[0]//3+imshape[0]*np.random.uniform()))
        vertices = np.array([vertex], dtype=np.int32) ## single shadow vertices 
        vertices_list.append(vertices)
    return vertices_list ## List of shadow vertices
 
def add_shadow(image,no_of_shadows=1):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    mask = np.zeros_like(image) 
    imshape = image.shape
    vertices_list= generate_shadow_coordinates(imshape, no_of_shadows) #3 getting list of shadow vertices
    for vertices in vertices_list: 
        cv2.fillPoly(mask, vertices, 255) ## adding all shadow polygons on empty mask, single 255 denotes only red channel
    
    image_HLS[:,:,1][mask[:,:,0]==255] = image_HLS[:,:,1][mask[:,:,0]==255]*0.5   ## if red channel is hot, image's "Lightness" channel's brightness is lowered 
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB
 def add_snow(image):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    image_HLS = np.array(image_HLS, dtype = np.float64) 
    brightness_coefficient = 2.5 
    snow_point=140 ## increase this for more snow
    image_HLS[:,:,1][image_HLS[:,:,1]<snow_point] = image_HLS[:,:,1][image_HLS[:,:,1]<snow_point]*brightness_coefficient ## scale pixel values up for channel 1(Lightness)
    image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255
    image_HLS = np.array(image_HLS, dtype = np.uint8)
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB
 def generate_random_lines(imshape,slant,drop_length):
    drops=[]
    for i in range(1500): ## If You want heavy rain, try increasing this
        if slant<0:
            x= np.random.randint(slant,imshape[1])
        else:
            x= np.random.randint(0,imshape[1]-slant)
        y= np.random.randint(0,imshape[0]-drop_length)
        drops.append((x,y))
    return drops
        
    
def add_rain(image):
    
    imshape = image.shape
    slant_extreme=10
    slant= np.random.randint(-slant_extreme,slant_extreme) 
    drop_length=20
    drop_width=2
    drop_color=(200,200,200) ## a shade of gray
    rain_drops= generate_random_lines(imshape,slant,drop_length)
    
    for rain_drop in rain_drops:
        cv2.line(image,(rain_drop[0],rain_drop[1]),(rain_drop[0]+slant,rain_drop[1]+drop_length),drop_color,drop_width)
    image= cv2.blur(image,(7,7)) ## rainy view are blurry
    
    brightness_coefficient = 0.7 ## rainy days are usually shady 
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    image_HLS[:,:,1] = image_HLS[:,:,1]*brightness_coefficient ## scale pixel values down for channel 1(Lightness)
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB
 def add_blur(image, x,y,hw):
    image[y:y+hw, x:x+hw,1] = image[y:y+hw, x:x+hw,1]+1
    image[:,:,1][image[:,:,1]>255]  = 255 ##Sets all values above 255 to 255
    image[y:y+hw, x:x+hw,1] = cv2.blur(image[y:y+hw, x:x+hw,1] ,(10,10))
    return image
 def generate_random_blur_coordinates(imshape,hw):
    blur_points=[]
    midx= imshape[1]//2-hw-100
    midy= imshape[0]//2-hw-100
    index=1
    while(midx>-100 or midy>-100): ## radially generating coordinates
        for i in range(250*index):
            x= np.random.randint(midx,imshape[1]-midx-hw)
            y= np.random.randint(midy,imshape[0]-midy-hw)
            blur_points.append((x,y))
        midx-=250*imshape[1]//sum(imshape)
        midy-=250*imshape[0]//sum(imshape)
        index+=1
    return blur_points
    
def add_fog(image):
    image_HLS = cv2.cvtColor(image,cv2.COLOR_RGB2HLS) ## Conversion to HLS
    mask = np.zeros_like(image) 
    imshape = image.shape
    hw=100
    image_HLS[:,:,1]=image_HLS[:,:,1]*0.8
    haze_list= generate_random_blur_coordinates(imshape,hw)
    for haze_points in haze_list: 
        image_HLS[:,:,1][image_HLS[:,:,1]>255]  = 255 ##Sets all values above 255 to 255
        image_HLS= add_blur(image_HLS, haze_points[0],haze_points[1], hw) ## adding all shadow polygons on empty mask, single 255 denotes only red channel
    image_RGB = cv2.cvtColor(image_HLS,cv2.COLOR_HLS2RGB) ## Conversion to RGB
    return image_RGB
 
 
Item 554: 
Bandit #1 = 10% success rate
Bandit #2 = 50% success rate
Bandit #3 = 60% success rate
Bandit #4 = 80% success rate (best)
Bandit #5 = 10% success rate
Bandit #6 = 25% success rate
Bandit #7 = 60% success rate
Bandit #8 = 45% success rate
Bandit #9 = 75% success rate (2nd best)
Bandit #10 = 65% success rate (3rd best)
 
 
Item 555: 
model <- glm(formula = FraudulentClaim ~ ., family = binomial(link = "logit"), 
    data = train)
 model <- glm(formula = FraudulentClaim ~ income + age + X, family = binomial(link = "logit"), 
    data = train)
 
 
Item 556: 
Selective Search:
1. Generate initial sub-segmentation, we generate many candidate     regions
2. Use greedy algorithm to recursively combine similar regions into larger ones 
3. Use the generated regions to produce the final candidate region proposals 
 
 
Item 558: 
1.4796491688889395 0.10148121494753726
 
 
Item 561: 
0x2910543Af39abA0Cd09dBb2D50200b3E800A63D2
 Address                                     Nonce   View Tx
A: 0x580992b51e3925e23280efb93d3047c82f17e038  17      Link
B: 0x480992b51e3925e23280efb93d3047c82f17e038  0       Link
 
 
Item 563: 
library(dplyr)
 mtcars %>% group_by(cyl) %>% summarize(mean_mpg = mean(mpg))
 require(dplyr)
 mtcars %>% group_by(cyl) %>% summarize(mean_mpg = mean(mpg))
 
 
Item 566: 
#define HEIGHT 25
#define WIDTH 25
 enum struct direction {L,R,U,D};
 struct point {
  int x,y;
  direction d;
  point(int x_ = -1, int y_ = -1):x(x_),y(y_){};
  point(int x_, int y_, direction d_):x(x_),y(y_),d(d_){};
  bool inBounds() { return ( x>=0 && x<WIDTH && y>=0 && y<HEIGHT ) ? 1 : 0; }
  void print() { std::cout<<"{x: "<<x<<", y: "<<y<<"}\n"; };
  const bool operator==(const point& rhs) const { return ( x == rhs.x && y == rhs.y) ? 1 : 0; };
};
 float euclideanDistance(point a, point b) {
  return (pow( pow( a.x - b.x, 2.0) + pow( a.y - b.y, 2.0), 0.5));
}
 void randomMaze(int maze[HEIGHT][WIDTH], point p) {
  point rn[4] = {
      point(p.x-2, p.y, direction::L),
      point(p.x+2, p.y, direction::R),
      point(p.x, p.y+2, direction::U),
      point(p.x, p.y-2, direction::D)
  };
  std::random_shuffle(&rn[0], &rn[4]);
  
  for(point cn : rn) {
    if(cn.inBounds() && !maze[cn.y][cn.x]) {
      if(cn.d == direction::L)
        maze[cn.y][cn.x+1] = 1;
      else if(cn.d == direction::R)
        maze[cn.y][cn.x-1] = 1;
      else if(cn.d == direction::U)
        maze[cn.y-1][cn.x] = 1;
      else if(cn.d == direction::D)
        maze[cn.y+1][cn.x] = 1;
      maze[cn.y][cn.x] = 1;
      randomMaze(maze, cn);
    }
  }
}
 std::vector<points> astar(int maze[HEIGHT][WIDTH],point s,point g) {
  //initialize sets//
  std::vector<point> paths[HEIGHT][WIDTH];
  float dist[HEIGHT][WIDTH] = { 0 };
  bool visited[HEIGHT][WIDTH] = { 0 };
  for(int i=0; i<HEIGHT; i++)
    for(int j=0; j<WIDTH; j++)
      dist[i][j] = INT_MAX;
   //initialize starting point//
  point cur = s;
  dist[cur.y][cur.x] = euclideanDistance(s,g);
   //best-fit search algorithm//
  while( !(cur == g) ) {
    
    //update current point to being visited//
    visited[cur.y][cur.x] = 1;
     //neighbors of the current point//
    point nb[4] = {
      point(cur.x-1,cur.y,direction::L),
      point(cur.x+1,cur.y,direction::R),
      point(cur.x,cur.y-1,direction::U),
      point(cur.x,cur.y+1,direction::D)
    };
     //calculate distances//
    for(point cn : nb )
      if( cn.inBounds() && maze[cn.y][cn.x] &&
        (euclideanDistance(cn,g) + dist[cur.y][cur.x] + maze[cn.y][cn.x] < dist[cn.y][cn.x]) ) {
          dist[cn.y][cn.x] = euclideanDistance(cn,g) + dist[cur.y][cur.x] + maze[cn.y][cn.x];
          paths[cn.y][cn.x] = paths[cur.y][cur.x], paths[cn.y][cn.x].push_back(cur);
      } 
     //select point of next iteration//
    cur = point(-1,-1);
    float md = INT_MAX;
    for(int i=0; i<HEIGHT; i++)
      for(int j=0; j<WIDTH; j++)
        if(!visited[i][j] && dist[i][j]!=INT_MAX && dist[i][j] < md) { cur = point(j,i), md = dist[i][j]; }
  
  }
   //return path from start to goal//
  paths[g.y][g.x].push_back(g);
  return paths[g.y][g.x];
 }
 
 
Item 568: 
struct node {
  int val;
  node* left;
  node* right;
  node(int v_) : val(v_), left(NULL), right(NULL) {};
};
 class BinaryTree {
  private:
    struct node {
      ...
    };
    node* root;
    void print(node* temp) {
      ...
    } 
  public:
    void addNode(int v_) {
      ...
    }
    int findNodeCount(int v_) {
      ...
    }
    void print() {
      ...
    } 
    BinaryTree() { ... }
};
 void addNode(int v_) {
      if(!root)
        root = new node(v_);
      else {
        node* walker = root;
        while(walker) {
          if(v_ < walker->val) {
            if(walker->left)
              walker = walker->left;
            else {
              walker->left = new node(v_);
              walker = NULL;
            }
          }
          else {
            if(walker->right)
              walker = walker->right;
            else {
              walker->right = new node(v_);
              walker = NULL;
            }
          }  
        }
      }
}
 int findNodeCount(int v_) {
      int count = 0;
      node* walker = root;
      while(walker) {
        if(v_ == walker->val)
          return count;
          walker = (v_ < walker->val) ? walker->left : walker->right;
        count++;
      }
      return -1;
    }
 void print() {
      cout<<"Printing binary tree:"<<endl<<"[ ";
      print(root);
      cout<<"]"<<endl;
    }
 void print(node* temp) {
      if(temp->left)
        print(temp->left);
      cout<<temp->val<<" ";
      if(temp->right)
        print(temp->right);
    }
 BinaryTree() { root = NULL; }
 
 
Item 571: 
data_x = np.zeros([n_samples, 256])
data_y = np.zeros([n_samples, len(unique_chars)])
for i in range(n_iterations):
    for j, val in enumerate(secret_chars):
        data_x[i * n_chars + j, :] = deep_spectre.train(j)
        data_y[i * n_chars + j, char_to_idx_map[val]] = 1
 scaler = StandardScaler()
data_x = scaler.fit_transform(data_x)
x_train, x_test, y_train, y_test = train_test_split(data_x, data_y)
 model = Sequential()
model.add(Dense(200, input_shape = (256,), activation = ‘relu’))
model.add(Dense(150, activation = ‘relu’))
model.add(Dense(100, activation = ‘relu’))
model.add(Dense(len(unique_chars), activation = ‘softmax’))
 model.compile(loss = ‘categorical_crossentropy’,
              optimizer = ‘adam’,
              metrics = [‘accuracy’])
 model.fit(x_train, y_train, 
          batch_size = 32,
          epochs = 10,
          validation_data = (x_test, y_test))
 Train on 48000 samples, validate on 16000 samples
Epoch 1/10
48000/48000 [==============================] — 4s 83us/step — loss: 2.9168 — acc: 0.3363 — val_loss: 0.7985 — val_acc: 0.8276
Epoch 2/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.4543 — acc: 0.9007 — val_loss: 0.3505 — val_acc: 0.9204
Epoch 3/10
48000/48000 [==============================] — 4s 75us/step — loss: 0.2802 — acc: 0.9367 — val_loss: 0.2825 — val_acc: 0.9335
Epoch 4/10
48000/48000 [==============================] — 3s 73us/step — loss: 0.2516 — acc: 0.9441 — val_loss: 0.2948 — val_acc: 0.9293
Epoch 5/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2368 — acc: 0.9451 — val_loss: 0.2640 — val_acc: 0.9361
Epoch 6/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2320 — acc: 0.9460 — val_loss: 0.2765 — val_acc: 0.9360
Epoch 7/10
48000/48000 [==============================] — 3s 73us/step — loss: 0.2405 — acc: 0.9458 — val_loss: 0.2588 — val_acc: 0.9376
Epoch 8/10
48000/48000 [==============================] — 4s 74us/step — loss: 0.2324 — acc: 0.9468 — val_loss: 0.2502 — val_acc: 0.9403
Epoch 9/10
48000/48000 [==============================] — 4s 73us/step — loss: 0.2269 — acc: 0.9474 — val_loss: 0.2452 — val_acc: 0.9408
Epoch 10/10
48000/48000 [==============================] — 3s 72us/step — loss: 0.2277 — acc: 0.9467 — val_loss: 0.2663 — val_acc: 0.9392
 secret_len = 40
x_message = np.zeros([secret_len, 256])
for i in range(secret_len):
    x_message[i, :] = deep_spectre.read(i)
 y_pred = model.predict(scaler.transform(x_message))
pred_chars = np.argmax(y_pred, axis=1)
message = ‘’.join(list(map(lambda x: chr(idx_to_char_map[x]), pred_chars)))
print(“The secret message is:”, message)
 The secret message is: The Magic Words are Squeamish Ossifrage.
 
 
Item 572: 
if: (2 eyes & pitbullmix_snout 
     + pitbullmix_wet_nose & mouth)
then: pitbullmix_face
 if: 2 eyes 
& BELOW: pitbullmix_snout 
     & pitbullmix_wet_nose 
& BELOW: mouth 
then: pitbullmix_face
 
 
Item 573: 
/training-data
    /sports
        sports-example1.txt
        sports-example2.txt
        sports-example3.txt
    /politics
        politics-example1.txt
 
 
Item 575: 
def residual_block(x, squeeze=16, expand=64):
  m = Conv2D(squeeze, (1,1), activation='relu')(x)
  m = Conv2D(squeeze, (3,3), activation='relu')(m)
  m = Conv2D(expand, (1,1), activation='relu')(m)
  return Add()([m, x])
 def inverted_residual_block(x, expand=64, squeeze=16):
  m = Conv2D(expand, (1,1), activation='relu')(x)
  m = DepthwiseConv2D((3,3), activation='relu')(m)
  m = Conv2D(squeeze, (1,1), activation='relu')(m)
  return Add()([m, x])
 def inverted_linear_residual_block(x, expand=64, squeeze=16):
  m = Conv2D(expand, (1,1), activation='relu')(x)
  m = DepthwiseConv2D((3,3),  activation='relu')(m)
  m = Conv2D(squeeze, (1,1))(m)
  return Add()([m, x])
 def relu(x):
  return max(0, x)
 def relu6(x):
  return min(max(0, x), 6)
 def bottleneck_block(x, expand=64, squeeze=16):
  m = Conv2D(expand, (1,1))(x)
  m = BatchNormalization()(m)
  m = Activation('relu6')(m)
  m = DepthwiseConv2D((3,3))(m)
  m = BatchNormalization()(m)
  m = Activation('relu6')(m)
  m = Conv2D(squeeze, (1,1))(m)
  m = BatchNormalization()(m)
  return Add()([m, x])
 
 
Item 576: 
jupyter notebook
 
 
Item 579: 
[
  ...
  {
    "text": "I'd like to get a beard trim on Saturday.",
    "intent": "schedule",
    "entities": [
      {
        "start": 18,
        "end": 28,
        "value": "beard trim",
        "entity": "service_name"
      },
    ]
  },
  ...
]
 ## Schedule intent with valid service name
* schedule{"service_name":"beard trim"}
- action_service_lookup               
- slot{"service_id": 12345}
- action_confirm_appointment
 ## Schedule intent with invalid service name
* schedule{"service_name":"jelly beans"}
- action_service_lookup
[note the lack of slot setting here compared to above]
- action_apologize_for_misunderstanding
- action_prompt_for_service
 ## Schedule intent with no service name
* schedule
- action_prompt_for_service
 
 
Item 582: 
n_estimators: np.arange(10,200,10)
 min_samples_leaf: np.arange(1,100,10)
 max_features: ['auto','sqrt','log2']
 n_estimators: 80
 min_samples_leaf: 1
 max_features: 'sqrt'
 
 
Item 583: 
(S are.v (NP the zebras.n) (ADJP fat.a) ?)
 (SatisfactionLink
   (TypedVariable (VariableNode “$X”) (Type “ConceptNode”))
   (AndLink
     (InheritanceLink (VariableNode “$X”) 
                     (ConceptNode “BoundingBox”))
     (EvaluationLink
       (GroundedPredicateNode “py:recognize”)
       (ListLink (VariableNode “$X”) (ConceptNode “zebra”)))
     (EvaluationLink
       (GroundedPredicateNode “py:recognize”)
       (ListLink (VariableNode “$X”) (ConceptNode “fat”)))))
 _det(color, _$qVar);_obj(be, plane);_subj(be, color)
 (BindLink
   (VariableList
     (TypedVariable (Variable”$B”) (Type “ConceptNode”))
     (TypedVariable (Variable “$X”) (Type “ConceptNode”)))
   (AndLink
     (InheritanceLink (VariableNode “$B”) 
                     (ConceptNode “BoundingBox”))
     (InheritanceLink (VariableNode “$X”) (ConceptNode “color”))
     (EvaluationLink
       (GroundedPredicateNode “py:recognize”)
       (ListLink (VariableNode “$B”) (ConceptNode “plane”)))
     (EvaluationLink
       (GroundedPredicateNode “py:recognize”)
       (ListLink (VariableNode “$B”) (VariableNode “$X”))))
   (ListLink (VariableNode “$B”) (VariableNode “$X”)))
 
 
Item 584: 
[['he', 'is', 'a', 'king'],
 ['she', 'is', 'a', 'queen'],
 ['he', 'is', 'a', 'man'],
 ['she', 'is', 'a', 'woman'],
 ['warsaw', 'is', 'poland', 'capital'],
 ['berlin', 'is', 'germany', 'capital'],
 ['paris', 'is', 'france', 'capital']]
  0: 'he',
 1: 'is',
 2: 'a',
 3: 'king',
 4: 'she',
 5: 'queen',
 6: 'man',
 7: 'woman',
 8: 'warsaw',
 9: 'poland',
 10: 'capital',
 11: 'berlin',
 12: 'germany',
 13: 'paris',
 14: 'france'
 array([[ 0,  1],
       [ 0,  2],
       ...
 he is
he a
is he
is a
is king
a he
a is
a king
 log_softmax = F.log_softmax(a2, dim=0)
 loss = F.nll_loss(log_softmax.view(1,-1), y_true)
 loss.backward()
 W1.data -= 0.01 * W1.grad.data
W2.data -= 0.01 * W2.grad.data
 W1.grad.data.zero_()
W2.grad.data.zero_()
 Loss at epo 0: 4.241989389487675
Loss at epo 10: 3.8398486052240646
Loss at epo 20: 3.5548086541039603
Loss at epo 30: 3.343840673991612
Loss at epo 40: 3.183084646293095
Loss at epo 50: 3.05673006943294
Loss at epo 60: 2.953996729850769
Loss at epo 70: 2.867735825266157
Loss at epo 80: 2.79331214427948
Loss at epo 90: 2.727727291413716
Loss at epo 100: 2.6690095041479385
 
 
Item 585: 
• Feature: the string representing the input text
 • Target: the text’s polarity (0 or 1)
 Text: This is a cat.  -->  Word Sequence: [this, is, a, cat]
 <div>This is not a sentence.<\div> --> [this, is, not, a, sentence]
 Input string: <a>bcd>
 Difference between greedy and non-greedy search :
 greedy: <.*>   -->   <a>bcd>
non-greedy: <.*?>   -->   <a>
 Training texts: ["This is a good cat", "This is a bad day"]
 => vocabulary: [this, cat, day, is, good, a, bad]
 New text: "This day is a good day"   -->   [1, 0, 2, 1, 1, 1, 0]
 wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
 tar -zxvf aclImdb_v1.tar.gz
 This is a cat. --> [this, is, a, cat, (this, is), (is, a), (a, cat)]
 
 
Item 588: 
A SIMULATION RUN
 Initialize 
- Randomly initialize a population of individuals
- Determine the fittest individual so far "CHAMPION"
 Repeat for a number of iterations
- Generate the next population of individuals based on the previous
    - Select parents
    - Perform genetic operations such as crossover or clone 
    - to produce children and possibly mutate
- Get the fittest individual in this population "challenger" 
- Replace the "champion" with the "challenger" if it is more fit
 The champion's DNA is the best solution we've seen in this simulation run. You might need to make multiple runs to get a better solution. 
 1. Number of iterations
2. Population size
3. Crossover probability
4. Mutation probability
 1. DNA representing the solution
2. The fitness function
3. The selection function
4. Genetic operations such as crossover, clone, and mutation
 
 
Item 592: 
client.recognize_celebrities(Image={'Bytes': imgBytes,})
 {
    "CelebrityFaces": [
        {
            "Face": {
                "BoundingBox": {
                    "Height": 0.6766666769981384,
                    "Left": 0.273333340883255,
                    "Top": 0.09833333641290665,
                    "Width": 0.4511111080646515
                },
                "Confidence": 99.9476089477539,
                "Landmarks": [
                ],
                "Pose": {
                },
                "Quality": {
                    "Brightness": 56.59690475463867,
                    "Sharpness": 99.9945297241211
                }
            },
            "Id": "1SK7cR8M",
            "MatchConfidence": 100,
            "Name": "Jeff Bezos",
            "Urls": [
                "www.imdb.com/name/nm1757263"
            ]
        }
    ],
    "UnrecognizedFaces": []
}
 {
    "Labels": [
        {
            "Confidence": 99.25341796875,
            "Name": "Skateboard"
        },
        {
            "Confidence": 99.25341796875,
            "Name": "Sport"
        },
        {
            "Confidence": 99.24723052978516,
            "Name": "People"
        },
],
}
 
 
Item 593: 
[2] -> 4
[2, 4] -> 17
[2, 4, 17] -> 3
[2, 4, 17, 3] -> 5
 
 
Item 594: 
predicted = np.zeros(len(test_df))
print accuracy_score(predicted, test_df['Survived'])
 X, y = get_some_data()
 X = SomeFeaturesTransformer().fit_transform(X)
 X_train, X_test, y_train, y_test = train_test_split(X, y)
 classifier = SomeClassifier().fit(X_train, y_train)
 
 
Item 595: 
>library(stringr) 
>str_trim(" hello people") 
[1] "hello people"

 > airline_data <- read.csv("airline.csv")
> head(airline_data)
  YEAR     Y     W      R     L     K
1 1948 1.214 0.243 0.1454 1.415 0.612
2 1949 1.354 0.260 0.2181 1.384 0.559
3 1950 1.569 0.278 0.3157 1.388 0.573
4 1951 1.948 0.297 0.3940 1.550 0.564
5 1952 2.265 0.310 0.3559 1.802 0.574
6 1953 2.731 0.322 0.3593 1.926 0.711
> tail(airline_data)
       YEAR      Y     W      R     L     K
27 1974.001 19.305 1.131 0.2664 5.729 9.508
28 1975.000 18.721 1.247 0.2301 5.722 9.062
29 1976.000 19.250 1.375 0.3452 5.762 8.262
30 1977.000 20.647 1.544 0.4508 5.877 7.474
31 1978.000 22.726 1.703 0.5877 6.108 7.104
32 1979.000 23.619 1.779 0.5346 6.852 6.874
> dim(airline_data)
[1] 32  6
> names(airline_data)
[1] "YEAR" "Y"    "W"    "R"    "L"    "K"Technically correct data set to Consistent data set
> summary(airline_data)
      YEAR            Y                W         
 Min.   :1948   Min.   : 1.214   Min.   :0.2430  
 1st Qu.:1956   1st Qu.: 3.875   1st Qu.:0.3583  
 Median :1964   Median : 7.349   Median :0.5220  
 Mean   :1964   Mean   :10.050   Mean   :0.6913  
 3rd Qu.:1971   3rd Qu.:16.400   3rd Qu.:0.9295  
 Max.   :1979   Max.   :23.619   Max.   :1.7790  
       R                L               K         
 Min.   :0.1454   Min.   :1.384   Min.   :0.5590  
 1st Qu.:0.2667   1st Qu.:2.355   1st Qu.:0.8203  
 Median :0.3505   Median :3.337   Median :2.0190  
 Mean   :0.3549   Mean   :3.793   Mean   :3.7363  
 3rd Qu.:0.4064   3rd Qu.:5.513   3rd Qu.:7.1965  
 Max.   :0.5879   Max.   :6.852   Max.   :9.5080
 > x[is.na(x)] <- mean(x, na.rm = TRUE)
 >set.seed(7)
 
 
Item 597: 
class doublylinkedlist {
  private:
    struct node {
      int val;
      node* next;
      node* prev;
      node(int v_) : val(v_),next(NULL),prev(NULL) {};
    };
    node* head;
    node* tail;
 ...
 ...
 public:
    doublylinkedlist() {
      head = NULL;
      tail = NULL;
    }
 ...
 ...
     void printFromHead() {
      node* temp = head;
      cout<<”[ ";
      while(temp) {
        cout<<temp->val<<" ";
        temp = temp->next;
      }
      cout<<"]"<<endl;
    }
    void printFromTail() {
       node* temp = tail;
       cout<<”[ ";
       while(temp) {
         cout<<temp->val<<" ";
         temp = temp->prev;
       }
       cout<<"]"<<endl;
    }
 ...
 ...
 void addFromHead(int v_) {
      if(head == NULL && tail == NULL) { //case 1//
        head = new node(v_);
        tail = head;
      }
      else {
        node* cur = head;
        while(cur && v_ > cur->val) {
          cur = cur->next;
        }
        if(cur == head) { //case 2//
          cur->prev = new node(v_);
          cur->prev->next = head;
          head = cur->prev;
        }
        else if(cur) { //case 3//
          cur->prev->next = new node(v_);
          cur->prev->next->next = cur;
          cur->prev->next->prev = cur->prev;
          cur->prev = cur->prev->next;        
        }
        else if(!cur) { //case 4//
          tail->next = new node(v_);
          tail->next->prev = tail;
          tail = tail->next;
        }
      }
    }
 ...
 void addFromTail(int v_) {
      if(head == NULL && tail == NULL) {
        tail = new node(v_);
        head = head;
      }
      else {
        node* cur = tail;
        while(cur && v_ <= cur->val) {
          cur = cur->prev;
        }
        if(cur == tail) {
          cur->next = new node(v_);
          cur->next->prev = tail;
          tail = cur->next;
        }
        else if(cur) {
          cur->next->prev = new node(v_);
          cur->next->prev->prev = cur;
          cur->next->prev->next = cur->next;
          cur->next = cur->next->prev;
        }
        else if(!cur) {          
          head->prev = new node(v_);
          head->prev->next = head;
          head = head->prev;
        }
      }
    }
 
 
Item 603: 
>install.packages("<packagename")
 >library("<package name>")
 mydata <- read.table("tabulardata.txt")
 > filename <-read.csv("/Users/saigayatri/name.csv")
> filename
                                                                         John
1                                                                        Jack
2                                                               John "Da Man"
3                                                                     Stephen
4                                                                            
5                                                       Joan "the bone", Anne
6                                                                            
     Doe               X120.jefferson.st.
1 McGinnis                     220 hobo Av.
2   Repici                120 Jefferson St.
3    Tyler 7452 Terrace "At the Plaza" road
4 Blankman                                 
5      Jet              9th, at Terrace plc
6                                          
    Riverside  NJ X08075
1       Phila  PA   9119
2   Riverside  NJ   8075
3    SomeTown  SD  91234
4    SomeTown  SD    298
5 Desert City  CO    123
6                     NA
 >library(readr)
>my_data <- read_csv("filename.csv",col_types="ccccnnnnn")
 >library(XLConnect)
>df <- readWorksheetFromFile("<file name and extension>", 
                            sheet=1, 
                            startRow = 4,
                            endCol = 2)
 > library(xlsx)
>df <- read.xlsx("<filename >", 
                sheetIndex = 1)
 >df <- read.xlsx2("<filename>", 
                 sheetIndex = 1, 
                 startRow=2, 
                 colIndex = 2)
 >mydatabase = dbConnect(MySQL(), user='user', password='password', dbname='database_name', host='host')
 dbWriteTable(mydatabase, name='table1', value=data.frame.name)
 rs = dbSendQuery(mydatabase, 'select * from the table1')

 data = fetch(rs, n=-1)
 #connect with the database students
mydatabase = dbConnect(MySQL(), user='root', password='1234', dbname='students', host='localhost')
 #get all tables in the database using dbListTables(mydatabase)
dbListTables(mydatabase)
[1] "mytable" "student"
 #retrieve data from the student table
rs = dbSendQuery(mydatabase, 'select * from student')
data = fetch(rs, n=-1)
print(data)
     user       pwd       email  phno
1     123  temp1234    kesh@123  1234
2  123456      1234        1234 22222
3  123456 121232343 12312342342 22222
4  123456  12312432      jahsdf 22222
5  123456      ftgy      jahsdf 22222
6  123456   dfefvgf   ahsdf@123 22222
7  123456     svfsf   ahsdf@123 22222
8  123456  rgaehetH   ahsdf@123 22222
9  123456     hdhgb   ahsdf@123 22222
10    123  12345678        fadf   222
11    123  12345678        fadf   222
12  11111      ngdj     thweweh  1111
 #let's convert a dataframe into mysql table
#Glimpse of dataframe
head(df)
 YEAR     Y     W      R     L     K
1 1948 1.214 0.243 0.1454 1.415 0.612
2 1949 1.354 0.260 0.2181 1.384 0.559
3 1950 1.569 0.278 0.3157 1.388 0.573
4 1951 1.948 0.297 0.3940 1.550 0.564
5 1952 2.265 0.310 0.3559 1.802 0.574
6 1953 2.731 0.322 0.3593 1.926 0.711
 dbWriteTable(mydatabase, name='Newtable', value=df)
[1] TRUE
dbListTables(mydatabase)
[1] "Newtable" "mytable"  "student"
 #successfully added

 > install.packages("rjson")
 >library("rjson")
> json_R_object <- fromJSON(file="/Users/saigayatri/Documents/r30.json")
> print(json_R_object)
 > text_data <-read.table("/Users/saigayatri/Desktop/rprogramming.txt",header=FALSE)
> text_data
     V1      V2      V3 V4   V5        V6
1 Hello people, welcome to this tutorial-
                                                                V7
1 Getting started with data science in 30 days using R programming
  V8
1  .
> class(text_data)
[1] "data.frame"
 > DATA_DIR <- system.file("extdata/", package = "readtext")
> library(readtext)
> readtext(paste0(DATA_DIR, "/txt/UDHR/*"))
readtext object consisting of 13 documents and 0 docvars.
# data.frame [13 x 2]
             doc_id
              <chr>
1  UDHR_chinese.txt
2    UDHR_czech.txt
3   UDHR_danish.txt
4  UDHR_english.txt
5   UDHR_french.txt
6 UDHR_georgian.txt
# ... with 7 more rows, and 1 more variables:
#   text <chr>
 > read_text <-readtext("/Users/saigayatri/Desktop/rprogramming.txt")
> print(read_text)
readtext object consisting of 1 document and 0 docvars.
# data.frame [1 x 2]
            doc_id                text
             <chr>               <chr>
1 rprogramming.txt "\"Hello peop\"..."
> class(read_text)
[1] "readtext"   "data.frame"
 >install.packages("XML")
>library(XML)
 >doc <- htmlTreeParse("https://medium.com/@TheDataGyan/getting-started-with-data-science-in-30-days-with-r-programming-day-1-e59dd8d377e7",useInternal =TRUE)
 > class(doc)
[1] "HTMLInternalDocument" "HTMLInternalDocument"
[3] "XMLInternalDocument"  "XMLAbstractDocument"
 >doc.text = unlist(xpathApply(doc, '//p', xmlValue))
 > class(doc.text)
[1] "character"
> df <- data.frame(doc.text)
 > url <-"/Users/saigayatri/Desktop/notes.xml"
> data_df <- xmlToDataFrame(url)
> data_df
                 text
1             Readers
2         TheDATAGyan
3       R PROGRAMMING
4 It is interesting!!
 > mySASDATA <- read.sas7bdat("airline.sas7bdat")
> mySASDATA
   YEAR      Y     W      R     L     K
1  1948  1.214 0.243 0.1454 1.415 0.612
2  1949  1.354 0.260 0.2181 1.384 0.559
3  1950  1.569 0.278 0.3157 1.388 0.573
4  1951  1.948 0.297 0.3940 1.550 0.564
5  1952  2.265 0.310 0.3559 1.802 0.574
6  1953  2.731 0.322 0.3593 1.926 0.711
7  1954  3.025 0.335 0.4025 1.964 0.776
8  1955  3.562 0.350 0.3961 2.116 0.827
9  1956  3.979 0.361 0.3822 2.435 0.800
10 1957  4.420 0.379 0.3045 2.707 0.921
11 1958  4.563 0.391 0.3284 2.706 1.067
12 1959  5.385 0.426 0.3856 2.846 1.083
13 1960  5.554 0.441 0.3193 3.089 1.481
14 1961  5.465 0.460 0.3079 3.122 1.736
15 1962  5.825 0.485 0.3783 3.184 1.926
16 1963  6.876 0.506 0.4180 3.263 2.041
17 1964  7.823 0.538 0.5163 3.412 1.997
18 1965  9.120 0.564 0.5879 3.623 2.257
19 1966 10.512 0.586 0.5369 4.074 2.742
20 1967 13.020 0.622 0.4443 4.710 3.564
21 1968 15.261 0.666 0.3052 5.217 4.767
22 1969 16.313 0.731 0.2332 5.569 6.511
23 1970 16.002 0.831 0.1883 5.495 7.627
24 1971 15.876 0.906 0.2023 5.334 8.673
25 1972 16.662 1.000 0.2506 5.345 8.331
26 1973 17.014 1.056 0.2668 5.662 8.557
27 1974 19.305 1.131 0.2664 5.729 9.508
28 1975 18.721 1.247 0.2301 5.722 9.062
29 1976 19.250 1.375 0.3452 5.762 8.262
30 1977 20.647 1.544 0.4508 5.877 7.474
31 1978 22.726 1.703 0.5877 6.108 7.104
32 1979 23.619 1.779 0.5346 6.852 6.874
 > library(foreign)
> mydata <- read.dta("airline.dta")
> mydata
   year      y     w      r     l     k
1  1948  1.214 0.243 0.1454 1.415 0.612
2  1949  1.354 0.260 0.2181 1.384 0.559
3  1950  1.569 0.278 0.3157 1.388 0.573
4  1951  1.948 0.297 0.3940 1.550 0.564
5  1952  2.265 0.310 0.3559 1.802 0.574
6  1953  2.731 0.322 0.3593 1.926 0.711
7  1954  3.025 0.335 0.4025 1.964 0.776
8  1955  3.562 0.350 0.3961 2.116 0.827
9  1956  3.979 0.361 0.3822 2.435 0.800
10 1957  4.420 0.379 0.3045 2.707 0.921
11 1958  4.563 0.391 0.3284 2.706 1.067
12 1959  5.385 0.426 0.3856 2.846 1.083
13 1960  5.554 0.441 0.3193 3.089 1.481
14 1961  5.465 0.460 0.3079 3.122 1.736
15 1962  5.825 0.485 0.3783 3.184 1.926
16 1963  6.876 0.506 0.4180 3.263 2.041
17 1964  7.823 0.538 0.5163 3.412 1.997
18 1965  9.120 0.564 0.5879 3.623 2.257
19 1966 10.512 0.586 0.5369 4.074 2.742
20 1967 13.020 0.622 0.4443 4.710 3.564
21 1968 15.261 0.666 0.3052 5.217 4.767
22 1969 16.313 0.731 0.2332 5.569 6.511
23 1970 16.002 0.831 0.1883 5.495 7.627
24 1971 15.876 0.906 0.2023 5.334 8.673
25 1972 16.662 1.000 0.2506 5.345 8.331
26 1973 17.014 1.056 0.2668 5.662 8.557
27 1974 19.305 1.131 0.2664 5.729 9.508
28 1975 18.721 1.247 0.2301 5.722 9.062
29 1976 19.250 1.375 0.3452 5.762 8.262
30 1977 20.647 1.544 0.4508 5.877 7.474
31 1978 22.726 1.703 0.5877 6.108 7.104
32 1979 23.619 1.779 0.5346 6.852 6.874
 
 
Item 608: 
(x1 + dx1, y1 + dy1, x2 + dx2, y2 + dy2)
 tf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold)
 
 
Item 610: 
[  0.         -13.99989315 -19.99984167 -21.99982282 -13.99989315
 -17.99986052 -19.99984273 -19.99984167 -19.99984167 -19.99984273
 -17.99986052 -13.99989315 -21.99982282 -19.99984167 -13.99989315
   0.        ]
[  0 -14 -20 -22 -14 -18 -20 -20 -20 -20 -18 -14 -22 -20 -14   0]
 Policy Probability Distribution:
[[1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]]
 Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):
[[0 3 3 2]
 [0 0 0 2]
 [0 0 1 2]
 [0 1 1 0]]
 Value Function:
[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]
 Reshaped Grid Value Function:
[[ 0. -1. -2. -3.]
 [-1. -2. -3. -2.]
 [-2. -3. -2. -1.]
 [-3. -2. -1.  0.]]
 Policy Probability Distribution:
[[1. 0. 0. 0.]
 [0. 0. 0. 1.]
 [0. 0. 0. 1.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 1. 0. 0.]
 [1. 0. 0. 0.]]
 Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):
[[0 3 3 2]
 [0 0 0 2]
 [0 0 1 2]
 [0 1 1 0]]
 Value Function:
[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]
 Reshaped Grid Value Function:
[[ 0. -1. -2. -3.]
 [-1. -2. -3. -2.]
 [-2. -3. -2. -1.]
 [-3. -2. -1.  0.]]
 
 
Item 611: 
Confidence   Correct  
 ------------ --------- 
          50%   52%      
          60%   55%      
          70%   57%      
          80%   61%      
          90%   66%      
         100%   78%
 n_users ~ 11,000
n_responses ~ 550,000
 OS              Mean Overconfidence    N
                        
Linux                   7.69%           157
Windows                 9.46%          4979
Macintosh               9.95%          1658
iOS                    10.12%          2842
Android                11.13%          1287
 Country         Mean Overconfidence    N
                             
United States           9.33%          6235
Canada                  9.39%           828
Germany                 9.43%           145
Brazil                 10.10%           207
Australia              10.20%           609
United Kingdom         10.58%           788
Sweden                 11.18%           176
Switzerland            11.57%           115
Netherlands            12.11%           372
India                  12.94%           257
 
 
Item 615: 
lasso.fit.outcome <- cv.glmnet(data.matrix(X), df$Y.ab, alpha=1)
 lasso.fit.outcome <- cv.glmnet(data.matrix(X), df$Y.ab, alpha=1)
coef <- predict(lasso.fit.outcome, type = "nonzero")
H <- X.colnames[unlist(coef)]
 lasso.fit.propensity <- cv.glmnet(data.matrix(X), df$T, alpha=1)
coef <- predict(lasso.fit.propensity, type = "nonzero")
K <- X.colnames[unlist(coef)]
 eq.H_union_K <- paste("Y.ab ~ T + ", sum.H_union_K)
fit.double <- lm(eq.H_union_K, data = df)
 fit.rdd <- RDestimate(eq.H_union_K, data = df)
 
 
Item 617: 
epoch_nr: 0, batch: 50/188, acc_train: 0.721, acc_test: 0.709
epoch_nr: 1, batch: 50/188, acc_train: 0.767, acc_test: 0.764
epoch_nr: 2, batch: 50/188, acc_train: 0.772, acc_test: 0.773
epoch_nr: 3, batch: 50/188, acc_train: 0.767, acc_test: 0.725
epoch_nr: 4, batch: 50/188, acc_train: 0.768, acc_test: 0.717
epoch_nr: 5, batch: 50/188, acc_train: 0.772, acc_test: 0.769
epoch_nr: 6, batch: 50/188, acc_train: 0.774, acc_test: 0.771
epoch_nr: 7, batch: 50/188, acc_train: 0.779, acc_test: 0.780
 
 
Item 619: 
library(tidyverse)
library(lubridate)
library(XLConnect)
library(dplyr)
library(ggplot2)
theme_set(theme_minimal())
 raw.data <- readWorksheet(loadWorkbook("Online_Retail.xlsx"), sheet=1)
data <- raw.data
 data$Total <- data$Quantity * data$UnitPrice
 txns <- data %>% 
  mutate(CustomerID = as.factor(CustomerID),
         InvoiceDate = InvoiceDate) %>%
  group_by(CustomerID, InvoiceNo, InvoiceDate) %>% 
  summarise(Spend = sum(Total)) %>%
  ungroup() %>% 
  filter(Spend>0)
 time_between <- txns %>% 
  arrange(CustomerID, InvoiceDate) %>% 
  group_by(CustomerID) %>% 
  mutate(dt = as.numeric(InvoiceDate - lag(InvoiceDate), unit=  'days')) %>% 
  ungroup() %>% 
  na.omit()
 Ntrans = txns %>% 
  group_by(CustomerID) %>% 
  summarise(N = n()) %>%
  filter(N>20)
 sample_n_groups = function(tbl, size, replace = FALSE, weight = NULL) {
  grps = tbl %>% groups %>% lapply(as.character) %>% unlist
  keep = tbl %>% summarise() %>% ungroup() %>% sample_n(size, replace, weight)
  tbl %>% right_join(keep, by=grps) %>% group_by_(.dots = grps)
}
 ecdf_df <- time_between %>% group_by(CustomerID) %>% arrange(dt) %>% mutate(e_cdf = 1:length(dt)/length(dt))
sample_users <- ecdf_df %>% inner_join(Ntrans) %>% sample_n_groups(20)
 ggplot(data = time_between %>% inner_join(Ntrans) %>% filter(CustomerID %in% sample_users$CustomerID), aes(dt)) + 
  geom_histogram(aes(y = ..count../sum(..count..)), bins = 15) + 
  facet_wrap(~CustomerID) +
  labs(x = 'Time Since Last Purchase (Days)',y = 'Frequency')
 ggplot(data = ecdf_df %>% inner_join(Ntrans) %>% filter(CustomerID %in% sample_users$CustomerID), aes(dt,e_cdf) ) + 
  geom_point(size =0.5) +
  geom_line() + 
  geom_hline(yintercept = 0.9, color = 'red') + 
  facet_wrap(~CustomerID) +
  labs(x = 'Time Since Last Purchase (Days)')
 getq <- function(x,a = 0.9){
  if(a>1|a<0){
    print('Check your quantile')
  }
  X <- sort(x)
  e_cdf <- 1:length(X) / length(X)
  aprx = approx(e_cdf, X, xout = c(0.9))
  return(aprx$y)
}
 percentiles = time_between %>% 
  inner_join(Ntrans) %>% 
  filter(N>5) %>% 
  group_by(CustomerID) %>% 
  summarise(percentile.90= getq(dt)) %>% 
  arrange(percentile.90)
 percentiles[ which(percentiles$CustomerID==12748), ]
 txns[ which(txns$CustomerID==12748), ]
 percentiles[ which(percentiles$CustomerID==13102), ]
 txns[ which(txns$CustomerID==13102), ]
 
 
Item 620: 
###############
# Bubble Plot #
###############
library(data.table)
library(ggplot2)
library(ggrepel)
 bubble_data <-fread("https://raw.githubusercontent.com/zhendata/Medium_Posts/c007346db1575aca391a6623c87bb5a31a60b365/bubble_plot_merged_city_data.csv",sep=",")
 bubble_plot <- ggplot(bubble_data, 
               aes(x = Unemployment_Rate, y = Home_Price/1000)) + 
 geom_point(aes(size = Population, fill = Total_Crime),shape=21) +
# Create 'Bubble' by assigning size a variable #
 scale_fill_continuous(low = "#33FFFF", high ="#FF6699" ) +
scale_size_area(max_size = 20)+
# Select bubble color scale and bubble maximum size #
 geom_text_repel(
          aes(label = City),nudge_x = 0,nudge_y = 0.75,size = 6) +
# Use geom_text_repel to repel the labels away from each other #
 theme_bw()+
# Use white background instead of the default grey one #
 ggtitle("Best Cities in US to Live in") +labs(x = "Unemployment Rate%", y = "Home Price",
       size = "Population",fill="Crime") +theme(plot.title = element_text(size=25, hjust = 0.5),
        axis.title=element_text(size=20, face = "bold"),
        axis.text=element_text(size=15)) +
# Style title and axis #
 scale_y_continuous(name="Home Price", breaks = seq(0, 1500, by=250), 
                      labels=c("0", "250K", "500K", "750K", "1000k",    "1250k", "1500K"))
# Make y-axis more readable by replacing scientific number by "K" #
 print(bubble_plot)
 ################
# Motion Chart #
################
library(data.table)
library(googleVis)
 motion_data <-fread("https://raw.githubusercontent.com/zhendata/Medium_Posts/c007346db1575aca391a6623c87bb5a31a60b365/motion_chart_merged_city_data.csv",sep=",")
 motion_chart <- gvisMotionChart(motion_data, 
idvar = "City", 
timevar = "Year",
xvar = "Unemployment Rate",
yvar= "Home Price",
sizevar="Population")
 plot(motion_chart)
# R automatically opens a tab in the browser for you
# The flash player needs to be enabled in browser 
 ########
# Data #
########
"""
The datasets I used are from Zillow (medium housing), FBI’s UCR program, census.gov (population), Bureau of Labor (unemployment). 
I did some data cleaning and joining for the format I needed in this article, and you can click the links below to download.
"""
bubble_plot_merged_city_data.csv, motion_chart_merged_city_data.csv
 
 
Item 621: 
importance %                        feature
7      24.932740                       latitude
0      21.772927                   accommodates
8      19.621701                      longitude
6       7.702141                 minimum_nights
5       6.356207                   extra_people
1       4.105430                      bathrooms
4       3.769556                guests_included
2       2.893417                       bedrooms
13      2.039581      room_type_Entire home/apt
3       1.844825                           beds
20      0.956488      topics_description_Luxury
19      0.914949    topics_description_Location
18      0.900623      topics_description_Budget
12      0.477287            property_type_Other
11      0.442333            property_type_House
9       0.430975        property_type_Apartment
14      0.396504         room_type_Private room
10      0.152781  property_type_Bed & Breakfast
17      0.107925              bed_type_Real Bed
15      0.098952          room_type_Shared room
 
 
Item 628: 
library(syuzhet)
 sentence <- "i love cats such a bundle of joy."
new_sentence <- as.character(strsplit(sentence," "))
 get_nrc_sentiment(new_sentence)
 #This is the output
 anger anticipation disgust fear joy sadness surprise trust negative
   0          0       0    0   2       0        0     0        0
positive
     2
 #load the libraries
library(syuzhet)
library(tm)
library(ggplot2)
 #mydataCopy is a term document,generated from cleaning #transcripts.csv 
 mydataCopy <- mydata
 #carryout sentiment mining using the get_nrc_sentiment()function #log the findings under a variable result
 result <- get_nrc_sentiment(as.character(mydataCopy))
 #change result from a list to a data frame and transpose it 
 result1<-data.frame(t(result))
 #rowSums computes column sums across rows for each level of a #grouping variable.
 new_result <- data.frame(rowSums(result1))
 #name rows and columns of the dataframe
 names(new_result)[1] <- "count"
new_result <- cbind("sentiment" = rownames(new_result), new_result)
rownames(new_result) <- NULL
 #plot the first 8 rows,the distinct emotions
qplot(sentiment, data=new_result[1:8,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
 
#plot the last 2 rows ,positive and negative
qplot(sentiment, data=new_result[9:10,], weight=count, geom="bar",fill=sentiment)+ggtitle("TedTalk Sentiments")
 
 
Item 633: 
FemaleAuthors <- data.frame(Publication=c("The Advertiser", "The Age", "Australian Book Review",
                                                 "The Australian Financial Review", "Books+Publishing",
                                                 "The Courier-Mail","The Daily Telegraph", "Good Reading",
                                                 "The Monthly","Sunday Age","Sunday Tasmanian",
                                                 "The Sydney Morning Herald","The Weekend Australian",
                                                "The West Australian"),
                            PropOfFemales=c(49,42,47,15,61,41,46,49,41,49,49,43,35,58))
FemaleAuthors <- FemaleAuthors[order(-FemaleAuthors$PropOfFemales, -FemaleAuthors$Publication),]
FemaleAuthors$FillValues <- c(rep(c("A","B"),5),"B","A","A","B")
 library("ggplot2")
library("ggridges")
ggplot(data=FemaleAuthors,aes(x=reorder(Publication, PropOfFemales), y=PropOfFemales, fill=FillValues)) +
  geom_bar(stat="identity", 
           colour="black", width=1) +
  scale_y_continuous(breaks=seq(0, 70, by=5),
                     limits=c(0,70),
                     expand=c(0,0)) +
  scale_fill_cyclical(values=c("plum3","orchid2"))+
  labs(x="Publication", y="Proportion of books reviewed \nwith female authors")+
  coord_flip() +
    theme(panel.grid.minor.y=element_blank(), 
        panel.grid.major.x=element_line(color="gray"),
        panel.background=element_blank(), 
        axis.line = element_line(color="gray", size = 1),
        axis.text=element_text(size=10),
        axis.title=element_text(size=15),
        plot.margin=margin(5,5,5,5),
        legend.position = "none")
 GeneExpressionProfile <- data.frame(AlteredGenes=factor(c("Apotosis-associated","Cellular Maintenance & Signalling",
                                                          "Chitin Binding","Detoxification","Insect Digestion-related",
                                                          "Insect Growth","Insect Immunity", "Insect Metabolism",
                                                          "Miscellaneous Function","Nucleic Acid Regulation",
                                                          "Stress Response","Virus Replication / Altered Host Physiology",
                                                          "Unknown")),
                                    PercentAltered=c(1,10,2,4,25,2,4,10,0,5,1,2,34))
GeneExpressionProfile <- GeneExpressionProfile[order(-GeneExpressionProfile$PercentAltered),]
library("ggplot2")
library("ggridges")
library("RColorBrewer")
ggplot(data=GeneExpressionProfile,aes(x=reorder(AlteredGenes, PercentAltered), y=PercentAltered, fill=AlteredGenes)) +
  geom_bar(stat="identity", 
           colour="black", width=1) +
  scale_y_continuous(breaks=seq(0, 50, by=5),
                     limits=c(0,50),
                     expand=c(0,0)) +
  scale_fill_manual(values=colorRampPalette(brewer.pal(12,"Set3"))(13)) +
  labs(x="Gene Group", y="Proportion of altered genes \nacross the genes studied")+
  coord_flip() +
  theme(panel.grid.minor.y=element_blank(), 
        panel.grid.major.x=element_line(color="gray"),
        panel.background=element_blank(), 
        axis.line = element_line(color="gray", size = 1),
        axis.text=element_text(size=10),
        axis.title=element_text(size=15),
        plot.margin=margin(5,5,5,5),
        legend.position = "none")
 TaxExpenditure <- data.frame(Expenditure.Type=c(factor("Industry & workforce", "Defence", "Social security & welfare",
                                                "Community services & culture", "Health", "Infrastructure, transport & energy",
                                                "Education", "General government services")),
                             Expenditure.Amount=c(14.843, 21.277, 121.907, 8.044, 59.858, 13.221, 29.870, 96.797))
 library("ggplot2")
library("stringr")
ggplot(data=TaxExpenditure,aes(x=reorder(Expenditure.Type, Expenditure.Amount), y=Expenditure.Amount, 
                               fill=Expenditure.Type)) +
  geom_bar(stat="identity") +
  scale_y_continuous(breaks=seq(0, 125, by=25),
                     limits=c(0,125),
                     expand=c(0,0)) +
  scale_x_discrete(labels=function(x) str_wrap(x, width=20))+
  labs(x="Expenditure type", y="Expenditure ($millions)")+
  scale_fill_brewer(palette="Set3") +
  coord_flip() +
  theme(panel.grid.minor.y=element_blank(), 
        panel.grid.major.x=element_line(color="gray"),
        panel.background=element_blank(), 
        axis.line = element_line(color="gray", size = 1),
        axis.text=element_text(size=10),
        axis.title=element_text(size=15),
        plot.margin=margin(5,15,5,5),
        legend.position = "none")
 
 
Item 645: 
#Install if the package doesn't exist 
#install.packages('DataExplorer) 
library(DataExplorer)
 choco = read.csv('../flavors_of_cacao.csv', header = T, stringsAsFactors = F)
 choco$Cocoa.Percent = as.numeric(gsub('%','',choco$Cocoa.Percent))
choco$Review.Date = as.character(choco$Review.Date)
 plot_str(choco)
 plot_missing(choco)
 plot_histogram(choco)
 plot_density(choco)
 plot_correlation(choco, type = 'continuous','Review.Date')
 plot_bar(choco)
 create_report(choco)
 
 
Item 653: 
tf.scalar(4.5).print();
 Tensor
  4.5
 tf.tensor1d([3, 7, 8]).print();
 Tensor
  [3, 7, 8]
 tf.tensor(4.5).print();
tf.tensor([3, 7, 8]).print();
 [[1, 5],
 [4, 7]]
 tf.tensor([1, 5, 4, 7], [2, 2]).print();
 tf.tensor([[1, 5], [4, 7]]).print();
 const a = tf.tensor1d([4, 7, 2, 1]);
const b = tf.tensor1d([20, 30, 40, 50]);
 a.add(b).print();
 tf.add(a, b);
 Tensor
  [24, 37, 42, 51]
 tf.sub(a, b).print();
 Tensor //output
  [-16, -23, -38, -49]
 tf.mul(a, b).print();
 Tensor //output
  [80, 210, 80, 50]
 tf.div(a, b).print();
 Tensor //output
  [0.2, 0.2333333, 0.05, 0.02]
 
 
Item 654: 
X = x1 … xi … xn
 Y = y1 … yj … ym
 AC-GCTGAT
 | ||  ||
-CAGC-TAT
 Score = nMatch x 1 + nMismatch x 0 + nGap x 0
      = 6x1 + 1x0 + 2x0
      = 6
 CTGAT (in X)
| |||
C-GAT (in Y)
 Score = nMatch x 1 + nMismatch x 0 + nGap x 0
      = 4x1 + 0x0 + 1x0
      = 4
 <alignment type>XX 
 CODE  DESCRIPTION
x     No parameters. Identical characters have score of 1, else 0.
m     A match score is the score of identical chars, else mismatch
      score.
d     A dictionary returns the score of any pair of characters.
c     A callback function returns scores.
 CODE  DESCRIPTION
x     No gap penalties.
s     Same open and extend gap penalties for both sequences.
d     The sequences have different open and extend gap penalties.
c     A callback function returns the gap penalties.
 X = ACGGGT
Y = ACG
 
 
Item 660: 
android {
   ...
   compileOptions {
    sourceCompatibility JavaVersion.VERSION_1_8
    targetCompatibility JavaVersion.VERSION_1_8
   }
}
 dependencies {
   ...
   implementation “com.google.ar.sceneform.ux:sceneform-ux:1.4.0”
}
 
 
Item 673: 
I'd like to travel
trip
travel
I leave from Paris tomorrow
 npm run train
 isFulfilled: (ages = [], { dialogEntities = {} } = {}) => (
 dialogEntities.passengerNumber &&
 ages.length >= dialogEntities.passengerNumber.values[0].value
)
 reducer: (oldAges, newAge) => [...(oldAges || []), newAge]
 BOTFUEL_APP_TOKEN=<YOUR_APP_TOKEN> BOTFUEL_APP_ID=<YOUR_APP_ID> BOTFUEL_APP_KEY=<YOUR_APP_KEY> npm start
 
 
Item 676: 
idenprof//train//chef// 900 images of chefs
 idenprof//train//doctor// 900 images of doctors
 idenprof//train//engineer// 900 images of engineer
 idenprof//train//farmer// 900 images of farmers
 idenprof//train//firefighter// 900 images of firefighters
 idenprof//train//judge// 900 images of judges
 idenprof//train//mechanic// 900 images of mechanics
 idenprof//train//pilot// 900 images of pilots
 idenprof//train//chef// 900 images of chef
 idenprof//train//police// 900 images of police
 idenprof//train//waiter// 900 images of waiters
 idenprof//test//chef// 200 images of chefs
 idenprof//test//doctor// 200 images of doctors
 idenprof//test//engineer// 200 images of engineer
 idenprof//test//farmer// 200 images of farmers
 idenprof//test//firefighter// 200 images of firefighters
 idenprof//test//judge// 200 images of judges
 idenprof//test//mechanic// 200 images of mechanics
 idenprof//test//pilot// 200 images of pilots
 idenprof//test//chef// 200 images of chef
 idenprof//test//police// 200 images of police
 idenprof//test//waiter// 200 images of waiters
 =====================================

Total params: 23,608,202

Trainable params: 23,555,082

Non-trainable params: 53,120

______________________________________

Using Enhanced Data Generation

Found 4000 images belonging to 4 classes.

Found 800 images belonging to 4 classes.

JSON Mapping for the model classes saved to  C:\Users\User\PycharmProjects\FirstTraining\idenprof\json\model_class.json

Number of experiments (Epochs) :  200
 Epoch 1/100

 1/280 [>.............................] - ETA: 52s - loss: 2.3026 - acc: 0.2500
 2/280 [>.............................] - ETA: 52s - loss: 2.3026 - acc: 0.2500
 3/280 [>.............................] - ETA: 52s - loss: 2.3026 - acc: 0.2500
 ..............................,
..............................,
..............................,
 279/280 [===========================>..] - ETA: 1s - loss: 2.3097 - acc: 0.0625Epoch 00000: saving model to C:\Users\User\PycharmProjects\FirstTraining\idenprof\models\model_ex-000_acc-0.100000.h5


280/280 [==============================] - 51s - loss: 2.3095 - acc: 0.0600 - val_loss: 2.3026 - val_acc: 0.1000
 waiter  :  99.99997615814209
 chef  :  1.568847380895022e-05
 judge  :  1.0255866556008186e-05
 
 
Item 681: 
Recommend & share..
 
 
Item 683: 
False
 654
61
6
 599 61
 Train on 599 samples, validate on 59 samples
Epoch 1/100
599/599 [==============================] - 2s 3ms/step - loss: 0.0074 - val_loss: 0.1025
Epoch 2/100
599/599 [==============================] - 1s 2ms/step - loss: 0.0644 - val_loss: 0.2629
Epoch 3/100
599/599 [==============================] - 1s 2ms/step - loss: 0.0107 - val_loss: 0.0181
Epoch 4/100
599/599 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0070
Epoch 5/100
599/599 [==============================] - 1s 2ms/step - loss: 5.3863e-04 - val_loss: 0.0017
Epoch 6/100
599/599 [==============================] - 1s 2ms/step - loss: 4.1020e-04 - val_loss: 0.0027
Epoch 7/100
599/599 [==============================] - 1s 2ms/step - loss: 2.1977e-04 - val_loss: 0.0022
Epoch 8/100
599/599 [==============================] - 1s 2ms/step - loss: 2.5272e-04 - val_loss: 0.0022
Epoch 9/100
599/599 [==============================] - 1s 2ms/step - loss: 2.4554e-04 - val_loss: 0.0020
Epoch 10/100
599/599 [==============================] - 1s 2ms/step - loss: 2.6365e-04 - val_loss: 0.0019
Epoch 11/100
599/599 [==============================] - 1s 2ms/step - loss: 2.5525e-04 - val_loss: 0.0018
Epoch 12/100
599/599 [==============================] - 1s 2ms/step - loss: 2.6679e-04 - val_loss: 0.0018
Epoch 13/100
599/599 [==============================] - 1s 2ms/step - loss: 2.5337e-04 - val_loss: 0.0017
Epoch 14/100
599/599 [==============================] - 1s 2ms/step - loss: 2.5953e-04 - val_loss: 0.0017
Epoch 15/100
599/599 [==============================] - 1s 2ms/step - loss: 2.4082e-04 - val_loss: 0.0016
Epoch 16/100
599/599 [==============================] - 1s 2ms/step - loss: 2.4312e-04 - val_loss: 0.0016
Epoch 17/100
599/599 [==============================] - 1s 2ms/step - loss: 2.2189e-04 - val_loss: 0.0016
Epoch 18/100
599/599 [==============================] - 1s 2ms/step - loss: 2.2231e-04 - val_loss: 0.0016
Epoch 19/100
599/599 [==============================] - 1s 2ms/step - loss: 2.0289e-04 - val_loss: 0.0016
Epoch 20/100
599/599 [==============================] - 1s 2ms/step - loss: 2.0255e-04 - val_loss: 0.0016
Epoch 21/100
599/599 [==============================] - 1s 2ms/step - loss: 1.8815e-04 - val_loss: 0.0016
Epoch 22/100
599/599 [==============================] - 1s 2ms/step - loss: 1.8700e-04 - val_loss: 0.0016
Epoch 23/100
599/599 [==============================] - 1s 2ms/step - loss: 1.7834e-04 - val_loss: 0.0016
Epoch 24/100
599/599 [==============================] - 1s 2ms/step - loss: 1.7617e-04 - val_loss: 0.0016
Epoch 25/100
599/599 [==============================] - 1s 2ms/step - loss: 1.7182e-04 - val_loss: 0.0016
Epoch 26/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6926e-04 - val_loss: 0.0016
Epoch 27/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6698e-04 - val_loss: 0.0016
Epoch 28/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6496e-04 - val_loss: 0.0016
Epoch 29/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6336e-04 - val_loss: 0.0016
Epoch 30/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6200e-04 - val_loss: 0.0016
Epoch 31/100
599/599 [==============================] - 1s 2ms/step - loss: 1.6081e-04 - val_loss: 0.0016
Epoch 32/100
599/599 [==============================] - 1s 2ms/step - loss: 1.5982e-04 - val_loss: 0.0016
Epoch 33/100
599/599 [==============================] - 1s 2ms/step - loss: 1.5899e-04 - val_loss: 0.0016
Epoch 34/100
599/599 [==============================] - 1s 2ms/step - loss: 1.5830e-04 - val_loss: 0.0016
Epoch 35/100
599/599 [==============================] - 1s 2ms/step - loss: 1.5775e-04 - val_loss: 0.0016
Epoch 36/100
599/599 [==============================] - 1s 2ms/step - loss: 1.5735e-04 - val_loss: 0.0016
Epoch 00036: early stopping
 Test RMSE: 18.724
 Test GRU model RMSE: 32.764
 Iteration: 1
Test RMSE: 9.233
Iteration: 2
Test RMSE: 16.251
Iteration: 3
Test RMSE: 12.337
Iteration: 4
Test RMSE: 38.239
Iteration: 5
Test RMSE: 49.088
Iteration: 6
Test RMSE: 3.908
Iteration: 7
Test RMSE: 7.206
Iteration: 8
Test RMSE: 38.290
Iteration: 9
Test RMSE: 4.388
Iteration: 10
Test RMSE: 9.347
Average RMSE:  18.8287473079
RMSE list: [9.233330864072622, 16.25122406236244, 12.3374370718704, 38.2387143974303, 49.08764082707623, 3.908100289970251, 7.206358361324355, 38.29018303096499, 4.387561412580847, 9.346922761677483]
 Test GRU model RMSE_new: 14.223
 Test SMAPE (percentage): 0.304
 
 
Item 686: 
Recommend & share..
 
 
Item 691: 
A  B : 1+A*B
0  0   1+0*0 = 1
0  1   1+0*1 = 1
1  0   1+1*0 = 1
1  1   1+1*1 = 2 mod(2) = 0
 func (m *model) encrypt() {
  for _, weight := range m.weights {
    p := new(big.Int).SetInt64(weight)
    c, err := paillier.Encrypt(&m.enc.PublicKey, p.Bytes())
    m.encryptedWeights = append(m.encryptedWeights, c)
  }
}
 
 
Item 693: 
analytic_function_name ( [ argument_list ] )
  OVER (
    [ PARTITION BY partition_expression_list ]
    [ ORDER BY expression [{ ASC | DESC }] [, ...] ]
    [ window_frame_clause ]
  )
 
 
Item 695: 
VAE Learning via Stein Variational Gradient Descent
Yuchen Pu (Duke University) · zhe Gan (duke) · Ricardo Henao (Duke University) · Chunyuan Li (Duke University) · Shaobo Han (Duke University) · Lawrence Carin (Duke University)
 Efficient Use of Limited-Memory Resources to Accelerate Linear Learning
Celestine Dünner (IBM Research) · Thomas Parnell (IBM Research) · Martin Jaggi (EPFL)
 Temporal Coherency based Criteria for Predicting Video Frames using Deep Multi-stage Generative Adversarial Networks
Prateep Bhattacharjee (Indian Institute of Technology Madras) · Sukhendu Das (IIT Madras)
 Total papers:
 1. lawrence carin (duke university): 10
2. alexander schwing (university of illinois at urbana-champaign): 6
3. nicolas heess (deepmind): 5
3. michael jordan (university of california, berkeley): 5
3. andreas krause (eth zurich): 5
3. razvan pascanu (deepmind): 5
3. le song (georgia institute of technology): 5
8. 22 tied with 4
 Last-author:
 1. lawrence carin (duke university): 7
2. david blei (columbia university): 4
2. volkan cevher (epfl): 4
2. yoshua bengio (université de montréal): 4
5. 31 tied with 3
 First-author:
 1. arya mazumdar (university of massachusetts amherst): 3
1. eric balkanski (harvard university): 3
1. simon du (carnegie mellon university): 3
23 tied with 2
 Total papers:
 1. google: 60 (8.8%)
2. carnegie mellon university: 48 (7.1%)
3. massachusetts institute of technology: 43 (6.3%)
4. microsoft: 40 (5.9%)
5. stanford university: 39 (5.7%)
6. university of california, berkeley: 35 (5.2%)
7. deepmind: 31 (4.6%)
8. university of oxford: 22 (3.2%)
9. university of illinois at urbana-champaign: 20 (2.9%)
10. georgia institute of technology: 18 (2.7%)
11. princeton: 17 (2.5%)
11. eth zurich: 17 (2.5%)
13. ibm: 16 (2.4%)
14. inria: 15 (2.2%)
14. harvard university: 15 (2.2%)
15. cornell university: 15 (2.2%)
17. duke university: 14 (2.1%)
17. columbia university: 14 (2.1%)
17. university of cambridge: 14 (2.1%)
17. epfl: 14 (2.1%)
21. university of michigan: 13 (1.9%)
22. university of toronto: 12 (1.8%)
22. university of southern california: 12 (1.8%)
22. tsinghua university: 12 (1.8%)
25. facebook: 11 (1.6%)
25. riken: 11 (1.6%)
27. university of washington: 10 (1.5%)
27. university of california, los angeles: 10 (1.5%)
27. university of texas at austin: 10 (1.5%)
30. new york university: 10 (1.5%)
30. university college london: 10 (1.5%)
32. université de montréal: 9 (1.3%)
32. tencent ai lab: 9 (1.3%)
34. openai: 8 (1.2%)
34. adobe: 8 (1.2%)
34. university of california, san diego: 8 (1.2%)
37. university of tokyo: 7 (1.0%)
37. university of pittsburgh: 7 (1.0%)
37. peking university: 7 (1.0%)
37. university of minnesota: 7 (1.0%)
41. university of california, davis: 6 (0.9%)
41. technion: 6 (0.9%)
41. university of pennsylvania: 6 (0.9%)
41. nanjing university: 6 (0.9%)
41. johns hopkins university: 6 (0.9%)
41. university of wisconsin-madison: 6 (0.9%)
47. australian national university: 5 (0.7%)
47. tel aviv university: 5 (0.7%)
47. ohio state university: 5 (0.7%)
57. national university of singapore: 5 (0.7%)
 Total first-author papers:
 1. carnegie mellon university: 36
2. massachusetts institute of technology: 30
3. stanford university: 25
4. google: 24
5. university of california, berkeley: 21
6. duke university: 14
7. deepmind: 14
8. eth zurich: 13
9. microsoft: 12
10. harvard university: 11
 Total institution authors:
 1. carnegie mellon university: 89
2. google: 78
3. massachusetts institute of technology: 69
4. deepmind: 68
5. stanford university: 66
6. university of california, berkeley: 60
7. microsoft: 59
8. eth zurich: 31
9. university of oxford: 29
10. duke university: 28
11. princeton: 28
 'google brain resident': 'google',
'google brain': 'google',
'google inc': 'google',
'google inc.':'google',
'google research nyc': 'google',
'google research': 'google',
'google, inc.': 'google’,
'deepmind @ google': 'deepmind',
'deepmind technologies': 'deepmind',
'google deepmind': 'deepmind’,
 'ibm research - china':'ibm',
'ibm research':'ibm',
'ibm research, ny':'ibm',
'ibm research, usa':'ibm',
'ibm t. j. watson research center':'ibm',
'ibm t. j. watson research':'ibm',
'ibm t.j watson research center':'ibm',
'ibm t.j. watson research center':'ibm',
'ibm t.j.watson research center':'ibm',
'ibm thomas j. watson research center':'ibm',
'ibm tj watson research center':'ibm',
 'microsoft research cambridge':'microsoft',
'microsoft research india':'microsoft',
'microsoft research maluuba':'microsoft',
'microsoft research new england':'microsoft',
'microsoft research':'microsoft',
'microsoft research, redmond, w':'microsoft',
'microsoft research, redmond, wa':'microsoft',
'miicrosoft research':'microsoft',
 'university of wisconsin - madison': 'university of wisconsin-madison',
'university of wisconsin madison': 'university of wisconsin-madison',
'university of wisconsin': 'university of wisconsin-madison',
'university of wisconsin, madison': 'university of wisconsin-madison',
'university of wisconsion-madison': 'university of wisconsin-madison',
'uw-madison': 'university of wisconsion-madison’,
 
 
Item 696: 
youtube.search().list(q=name, type="playlist",part="id",maxResults=1).execute()
 youtube.playlistItems().list(part='contentDetails',maxResults=50, playlistId=playlist_id).execute()
 
 
Item 697: 
image-scraper 'https://www.fashionsite.com/mens-sneakers/'
 image-scraper 'https://www.fashionsite.com/mens-sneakers/?p=2'
 convert -crop 256x256+1+50 pic_to_be_cropped.jpg cropped_pic_name.jpg
 mogrify -crop 256x256+0+50 -path ./cropped/ *.jpg
 python imagerater.py -i 1 -x 5-f ‘./cropped/*.jpg' -q “Rate shoe here: “
 
 
Item 698: 
git clone --recursive https://github.com/tensorflow/tensorflow
 
 
Item 700: 
image_lists = create_image_lists(FLAGS.image_dir, FLAGS.testing_percentage,cFLAGS.validation_percentage)
 with open(ALL_LABELS_FILE) as f:
    labels = f.read().splitlines()
class_count = len(labels)
 ground_truth = np.zeros(class_count, dtype=np.float32)
 ground_truth[label_index] = 1.0
 labels_file = get_image_labels_path(image_lists,label_name,image_index, IMAGE_LABELS_DIR, category)
 with open(labels_file) as f:
   true_labels = f.read().splitlines()
 ground_truth = np.zeros(class_count, dtype=np.float32)
 idx = 0
for label in labels:
   if label in true_labels:
      ground_truth[idx] = 1.0
   idx += 1
 final_tensor = tf.nn.softmax(logits, name=final_tensor_name)
 final_tensor = tf.nn.sigmoid(logits, name=final_tensor_name)
 cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits,ground_truth_input)
 correct_prediction = tf.equal(tf.argmax(result_tensor, 1), tf.argmax(ground_truth_tensor, 1))
 |        | label1  | label2 | label3  |
| image1 |    0    |    1   |    0    |
| image2 |    1    |    0   |    0    |
 tf.argmax(result_tensor, 1)
 correct_prediction = tf.equal(tf.round(result_tensor), ground_truth_tensor)
 python retrain.py \
--bottleneck_dir=bottlenecks \
--how_many_training_steps 500 \
--model_dir=model_dir \
--output_graph=retrained_graph.pb \
--output_labels=retrained_labels.txt \
--summaries_dir=retrain_logs \
--image_dir=images
 python label_image.py <image_name>
 tensorboard --logdir retrain_logs
 
 
Item 701: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 706: 
validate {
 !(state.cabbage = Client &&
   state.goat    = Client &&
   state.wolf    = Client)
}
 
 
Item 707: 
nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)
 nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)
 output = output.view(-1, 16 * 16 * 24)
 self.net = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6, self.unit7, self.pool2, self.unit8, self.unit9, self.unit10, self.unit11, self.pool3,self.unit12, self.unit13, self.unit14, self.avgpool)
 self.avgpool = nn.AvgPool2d(kernel_size=4)
 self.fc = nn.Linear(in_features=128,out_features=num_classes)
 output = output.view(-1,128)
 for i, (images,labels) in enumerate(train_loader):
 if cuda_avail:
    images = Variable(images.cuda())
    labels = Variable(labels.cuda())
 optimizer.zero_grad()
 train_loss += loss.cpu().data[0] * images.size(0)
_, prediction = torch.max(outputs.data, 1)

train_acc += torch.sum(prediction == labels.data)
 
 
Item 710: 
mingw32-make.exe  -j5 > buildLogs.txt
 
 
Item 718: 
for (int i=0; i<n; i++) {
   y[i]=0;
   for (int j=0; j<n; j++) y[i]+=a[i][j]*x[j]
 }
 
 
Item 721: 
x = [23, 40, 6, 74, 38, 1, 70]
 sorted_x = [1, 6, 23, 38, 40, 70, 74]
 
 
Item 722: 
cd ~ && mkdir merkletrees && cd merkletrees
 touch a.txt
 md5 a.txt
 echo “hashing is awesome” >> a.txt && md5 a.txt
 rm a.txt
 echo "Hello from a.txt" >> a.txt && md5 a.txt &&
echo "Hello from b.txt" >> b.txt && md5 b.txt &&
echo "Hello from c.txt" >> c.txt && md5 c.txt &&
echo "Hello from d.txt" >> d.txt && md5 d.txt
 MD5 (a.txt) = c95c68d91441ba192ada81c9cfb2abe7
MD5 (b.txt) = 80262782d5961c452eae5de9991af0fd
MD5 (c.txt) = 0bfe75f719adf6450bb8be8e10126383
MD5 (d.txt) = 10fa7f973f6ec6682e1a6f4570a89861
 md5 <<< "c95c68d91441ba192ada81c9cfb2abe780262782d5961c452eae5de9991af0fd"
 a3d71b58e08759e6245667fb6f4770b6
 md5 <<< "0bfe75f719adf6450bb8be8e1012638310fa7f973f6ec6682e1a6f4570a89861"
 078791272b0b48b7bb7004ebc1b22123
 md5 <<< "a3d71b58e08759e6245667fb6f4770b6078791272b0b48b7bb7004ebc1b22123"
 eaad1d42b754612a4249bcbd95f0b734
 
 
Item 725: 
pipeline: 
  - name: "intent_featurizer_count_vectors" 
  - name: "intent_classifier_tensorflow_embedding" 
    intent_tokenization_flag: true 
    intent_split_symbol: "+"
 ## intent: meetup 
- I am new to the area. What meetups I could join in Berlin? 
- I have just moved to Berlin. Can you suggest any cool meetups for 
  me? 
 ## intent: affirm+ask_transport
- Yes. How do I get there? 
- Sounds good. Do you know how I could get there from home?
 python -m rasa_nlu.train -c config.yml --data data/nlu_data.md -o models --fixed_model_name nlu_model --project current --verbose
 interpreter = Interpreter.load('models/current/nlu_model') print(interpreter.parse("Yes. Can you give me suggestions on how to get there?"))
 { 
  'intent':{
    'name':'affirm+ask_transport',
    'confidence':0.918471097946167 
  }, 
  'entities':[ 
   ], 
  'intent_ranking':[ 
    {
      'name':'affirm+ask_transport', 
      'confidence':0.918471097946167 
    },
    {
      'name':'ask_transport',
      'confidence':0.32662829756736755 
    }, 
    { 
      'name':'thanks+goodbye', 
      'confidence':0.004435105249285698
    }, 
    { 
      'name':'meetup', 
      'confidence':-0.06692223995923996 
    },
    { 
      'name':'deny', 
      'confidence':-0.07153981924057007 
    }, 
    { 
      'name':'goodbye', 
      'confidence':-0.08976072818040848
    }, 
    {
      'name':'greet',
      'confidence':-0.09185336530208588 
    }, 
    { 
      'name':'thanks', 
      'confidence':-0.1762458086013794 
    }, 
    { 
      'name':'affirm', 
      'confidence':-0.3415682911872864 
    } 
  ], 
  'text':'Yes. Can you give me suggestions on how to get there?'  
}
 templates: 
  utter_greet: 
    - "Hey, how can I help you?" 
  utter_goodbye: 
    - "Talk to you later!"
    - "Goodbye :(" 
    - "Bye!"
    - "Have a great day!"
  utter_confirm: 
    - "Done - I have just booked you a spot at the Bots Berlin  
       meetup." 
    - "Great, just made an RSVP for you." 
  utter_meetup: 
    - "Rasa Bots Berlin meetup is definitely worth checking out!  
       They are having an event today at Behrenstraße 42. Would you 
       like to join?" 
  utter_affirm_suggest_transport:
    - "Great, I have just booked a spot for you. The venue is close
       to the Berlin Friedrichstraße station, you can get there by 
       catching U-Bahn U6." 
  utter_suggest_transport:
    - "The venue is close to the Berlin Friedrichstraße station, 
       so the best option is to catch a U-Bahn U6." 
  utter_thanks: 
    - "You are very welcome."
    - "Glad I could help!"
  utter_deny:
    - "That's a shame. Let me know if you change your mind."
 python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue --epochs 200
 python -m rasa_core.run -d models/current/dialogue -u models/current/nlu_model
 User: Hello 
Bot:  Hey, how can I help you?
User: I am new to Berlin and I would like to join some meetups. Any
      suggestions? 
Bot:  Rasa Bots Berlin meetup is definitely worth checking out! They 
      are having an event today at Behrenstraße 42. Would you like 
      to join? 
User: Sure! Can you also tell me how can I get to the venue? 
Bot:  Great, just made an RSVP for you.
Bot:  The venue is close to the Berlin Friedrichstraße station, so  
      the best option is to catch a U-Bahn U6. 
User: Thanks a lot. Talk to you later. 
Bot:  Glad I could help!
Bot:  Talk to you later!
 
 
Item 727: 
   size  bedroom  price
0  2104        3  399900
1  1600        3  329900
2  2400        3  369000
3  1416        2  232000
4  3000        4  539900
    size      bedroom   price
0  0.130010 -0.223675  0.475747
1 -0.504190 -0.223675 -0.084074
2  0.502476 -0.223675  0.228626
3 -0.735723 -1.537767 -0.867025
4  1.257476  1.090417  1.595389
 g,cost = gradientDescent(X,y,theta,iters,alpha)
print(g) #[[ -1.03191687e-16   8.78503652e-01  -4.69166570e-02]]
 finalCost = computeCost(X,y,g)
print(finalCost) #0.13070336960771892
 
 
Item 728: 
- name: "nlp_spacy"
  case_sensitive: true
 - name: "intent_featurizer_count_vectors"
  token_pattern: '(?u)\b\w+\b'
  max_ngram: 2
 ## intent:give_name
- [John](name)
...
 - name: ner_crf
  features: [
    ["low", "title", "upper"],
    ["bias", "low", "prefix5", "prefix2", "suffix5", "suffix3", "suffix2", "upper", "title", "digit", "pattern", "pos", "pos2"],
    ["low", "title", "upper"]]
 - name: "intent_classifier_tensorflow_embedding"
  intent_tokenization_flag: false
 
 
Item 732: 
export GRAALVM_HOME=/path/to/GraalVM
export JAVA_HOME=$GRAALVM_HOME
export PATH=$GRAALVM_HOME/bin:$PATH
 git clone https://github.com/graalvm/graalvm-demos
cd spring-r
 gu -c install org.graalvm.R
 Rscript -e "install.packages(\"ggplot2\")"
 mvn spring-boot:run
 @Autowired
private Function<Double, String> plotFunction;
 public static class DataHolder {
  public double value;
  public DataHolder(double v) {
    value = v;
  }
}
 function(dataHolder) {
  svg()
  data <<- c(data[2:100], dataHolder$value)
  ...
}
 
 
Item 734: 
self.u = K.random_normal_variable([1, units], 0, 1, dtype=self.dtype)
 
 
Item 735: 
library(ggplot2)
library(ggthemes)
library(dplyr)
library(viridis)
library(tidyr)
library(cluster)
library(ggmap)
library(maps)
 toronto <- read.csv('toronto_crime.csv')
toronto <- subset(toronto, !duplicated(toronto$event_unique_id))
unique(toronto$occurrenceyear)
unique(toronto$reportedyear)
 year_group <- group_by(toronto, occurrenceyear)
crime_by_year <- summarise(year_group,
                          n = n())
crime_by_year
 drops <- c("X", "Y", "Index_", "ucr_code", "ucr_ext", "reporteddate", "reportedmonth", "reportedday", "reporteddayofyear", "reporteddayofweek", "reportedhour", "occurrencedayofyear", "reportedyear", "Division", "Hood_ID", "FID")
 toronto <- toronto[, !(names(toronto) %in% drops)]
toronto <- toronto[toronto$occurrenceyear == 2016, ]
toronto <- toronto[complete.cases(toronto), ]
 indicator_group <- group_by(toronto, MCI)
crime_by_indicator <- summarise(indicator_group, n=n())
crime_by_indicator <- crime_by_indicator[order(crime_by_indicator$n, decreasing = TRUE),]
 ggplot(aes(x = reorder(MCI, n), y = n), data = crime_by_indicator) +
  geom_bar(stat = 'identity', width = 0.5) +
  geom_text(aes(label = n), stat = 'identity', data = crime_by_indicator, hjust = -0.1, size = 3.5) +
  coord_flip() +
  xlab('Major Crime Indicators') +
  ylab('Number of Occurrences') +
  ggtitle('Major Crime Indicators Toronto 2016') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 assault <- toronto[toronto$MCI == 'Assault', ]
assault_group <- group_by(assault, offence)
assault_by_offence <- summarise(assault_group, n=n())
assault_by_offence <- assault_by_offence[order(assault_by_offence$n, decreasing = TRUE), ]
 ggplot(aes(x = reorder(offence, n), y = n), data = assault_by_offence) +
  geom_bar(stat = 'identity', width = 0.6) +
  geom_text(aes(label = n), stat = 'identity', data = assault_by_offence, hjust = -0.1, size = 3) +
  coord_flip() +
  xlab('Types of Assault') +
  ylab('Number of Occurrences') +
  ggtitle('Assault Crime Toronto 2016') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 offence_group <- group_by(toronto, offence)
crime_by_offence <- summarise(offence_group, n=n())
crime_by_offence <- crime_by_offence[order(crime_by_offence$n, decreasing = TRUE), ]
 ggplot(aes(x = reorder(offence, n), y = n), data = crime_by_offence) +
  geom_bar(stat = 'identity', width = 0.7) +
  geom_text(aes(label = n), stat = 'identity', data = crime_by_offence, hjust = -0.1, size = 2) +
  coord_flip() +
  xlab('Types of Offence') +
  ylab('Number of Occurrences') +
  ggtitle('Offence Types Toronto 2016') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 hour_group <- group_by(toronto, occurrencehour)
crime_hour <- summarise(hour_group, n=n())
 ggplot(aes(x=occurrencehour, y=n), data = crime_hour) + geom_line(size = 2.5, alpha = 0.7, color = "mediumseagreen", group=1) + 
  geom_point(size = 0.5) + 
  ggtitle('Total Crimes by Hour of Day in Toronto 2016') +
  ylab('Number of Occurrences') +
  xlab('Hour(24-hour clock)') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 hour_crime_group <- group_by(toronto, occurrencehour, MCI)
hour_crime <- summarise(hour_crime_group, n=n())
 ggplot(aes(x=occurrencehour, y=n, color=MCI), data = hour_crime) + 
  geom_line(size=1.5) + 
  ggtitle('Crime Types by Hour of Day in Toronto 2016') +
  ylab('Number of Occurrences') +
  xlab('Hour(24-hour clock)') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 location_group <- group_by(toronto, Neighbourhood)
crime_by_location <- summarise(location_group, n=n())
crime_by_location <- crime_by_location[order(crime_by_location$n, decreasing = TRUE), ]
crime_by_location_top20 <- head(crime_by_location, 20)
 ggplot(aes(x = reorder(Neighbourhood, n), y = n), data = crime_by_location_top20) +
  geom_bar(stat = 'identity', width = 0.6) +
  geom_text(aes(label = n), stat = 'identity', data = crime_by_location_top20, hjust = -0.1, size = 3) +
  coord_flip() +
  xlab('Neighbourhoods') +
  ylab('Number of Occurrences') +
  ggtitle('Neighbourhoods with Most Crimes - Top 20') +
  theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 tail(crime_by_location, 5)
 offence_location_group <- group_by(toronto, Neighbourhood, offence)
offence_type_by_location <- summarise(offence_location_group, n=n())
offence_type_by_location <- offence_type_by_location[order(offence_type_by_location$n, decreasing = TRUE), ]
offence_type_by_location_top20 <- head(offence_type_by_location, 20)
 ggplot(aes(x = Neighbourhood, y=n, fill = offence), data=offence_type_by_location_top20) +
  geom_bar(stat = 'identity', position = position_dodge(), width = 0.8) +
  xlab('Neighbourhood') +
  ylab('Number of Occurrence') +
  ggtitle('Offence Type vs. Neighbourhood Toronto 2016') + theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = .4))
 crime_count <- toronto %>% group_by(occurrencemonth, MCI) %>% summarise(Total = n())
crime_count$occurrencemonth <- ordered(crime_count$occurrencemonth, levels = c('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'))
 ggplot(crime_count, aes(occurrencemonth, MCI, fill = Total)) +
  geom_tile(size = 1, color = "white") +
  scale_fill_viridis()  +
  geom_text(aes(label=Total), color='white') +
  ggtitle("Major Crime Indicators by Month 2016") +
  xlab('Month') +
  theme(plot.title = element_text(size = 16), 
        axis.title = element_text(size = 12, face = "bold"))
 day_count <- toronto %>% group_by(occurrencedayofweek, MCI) %>% summarise(Total = n())
 ggplot(day_count, aes(occurrencedayofweek, MCI, fill = Total)) +
  geom_tile(size = 1, color = "white") +
  scale_fill_viridis()  +
  geom_text(aes(label=Total), color='white') +
  ggtitle("Major Crime Indicators by Day of Week 2016") +
  xlab('Day of Week') +
  theme(plot.title = element_text(size = 16), 
        axis.title = element_text(size = 12, face = "bold"))
 homicide <- read.csv('homicide.csv', stringsAsFactors = F)
homicide$Occurrence_Date <- as.Date(homicide$Occurrence_Date)
 year_group <- group_by(homicide, Occurrence_year, Homicide_Type)
homicide_by_year <- summarise(year_group, n=n())
 ggplot(aes(x = Occurrence_year, y=n, fill = Homicide_Type), data=homicide_by_year) +
  geom_bar(stat = 'identity', position = position_dodge(), width = 0.8) +
  xlab('Year') +
  ylab('Number of Homicides') +
  ggtitle('Homicide 2004-2016') + theme_bw() +
  theme(plot.title = element_text(size = 16),
        axis.title = element_text(size = 12, face = "bold"))
 homicide$month <- format(as.Date(homicide$Occurrence_Date) , "%B")
 homicide_count <- homicide %>% group_by(Occurrence_year, month) %>% summarise(Total = n())
homicide_count$month <- ordered(homicide_count$month, levels = c('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'))
ggplot(homicide_count, aes(Occurrence_year, month, fill = Total)) +
  geom_tile(size = 1, color = "white") +
  scale_fill_viridis()  +
  geom_text(aes(label=Total), color='white') +
  ggtitle("Homicides in Toronto (2004-2016)") +
  xlab('Year') +
  theme(plot.title = element_text(size = 16), 
        axis.title = element_text(size = 12, face = "bold"))
 by_groups <- group_by(toronto, MCI, Neighbourhood)
groups <- summarise(by_groups, n=n())
groups <- groups[c("Neighbourhood", "MCI", "n")]
groups_wide <- spread(groups, key = MCI, value = n)
 z <- groups_wide[, -c(1,1)]
 z <- z[complete.cases(z), ]
 m <- apply(z, 2, mean)
s <- apply(z, 2, sd)
z <- scale(z, m, s)
 wss <- (nrow(z)-1) * sum(apply(z, 2, var))
for (i in 2:20) wss[i] <- sum(kmeans(z, centers=i)$withiness)
plot(1:20, wss, type='b', xlab='Number of Clusters', ylab='Within groups sum of squares')
 kc <- kmeans(z, 2)
kc
 z1 <- data.frame(z, kc$cluster)
clusplot(z1, kc$cluster, color=TRUE, shade=F, labels=0, lines=0, main='k-Means Cluster Analysis')
 z2 <- data.frame(z)
distance <- dist(z2)
hc <- hclust(distance)
 plot(hc, labels = groups_wide$Neighbourhood, main='Cluster Dendrogram', cex=0.65)
 counts = sapply(2:6,function(ncl)table(cutree(hc,ncl)))
names(counts) = 2:6
counts
 member <- cutree(hc, 3)
aggregate(z, list(member), mean)
 plot(silhouette(cutree(hc, 3), distance))
 lat <- toronto$Lat
lon <- toronto$Long
crimes <- toronto$MCI
to_map <- data.frame(crimes, lat, lon)
colnames(to_map) <- c('crimes', 'lat', 'lon')
sbbox <- make_bbox(lon = toronto$Long, lat = toronto$Lat, f = 0.01)
my_map <- get_map(location = sbbox, maptype = "roadmap", scale = 2, color="bw", zoom = 10)
ggmap(my_map) +
  geom_point(data=to_map, aes(x = lon, y = lat, color = "#27AE60"), 
             size = 0.5, alpha = 0.03) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Location of Major Crime Indicators Toronto 2016') +
  guides(color=FALSE)
 ggmap(my_map) +
  geom_point(data=to_map, aes(x = lon, y = lat, color = "#27AE60"), 
             size = 0.5, alpha = 0.05) +
  xlab('Longitude') +
  ylab('Latitude') +
  ggtitle('Location of Major Crime Indicators Toronto 2016') +
  guides(color=FALSE) +
  facet_wrap(~ crimes, nrow = 2)
 
 
Item 736: 
Count Institute
-----------------
51 Google
29 UC Berkeley
29 Carnegie Mellon University
28 MIT
28 Deepmind
26 Stanford
25 Microsoft
21 Facebook
21 University of California
19 University of Oxford
17 Princeton
16 Cornell University
15 University of Toronto
15 UT Austin
14 ETH Zurich
14 École polytechnique fédérale de Lausanne
14 University of Cambridge
11 Georgia Tech
11 Duke University
11 Columbia University
10 University of Southern California
10 Amazon
10 Uber
10 Tsinghua University
10 Harvard
9 INRIA
9 Purdue University
8 Johns Hopkins University
8 Yale University
8 Madison
 
 
Item 737: 
y = a*x + b
 na_if(father_age, 99)
 model = lm(Gestation_Week ~ Father_Age, data = us_baby)
 
 
Item 738: 
Clap, share if you like it and follow me for the updates.
 
 
Item 741: 
visited = {SD: false, LA: false, NY: false, LDN: false}
distance = {SD: 0, LA: INT_MAX, NY: INT_MAX, LDN: INT_MAX} 
 visited = {SD: true, LA: false, NY: false, LDN: false}
distance = {SD: 0, LA: 1, NY: 7, LDN: INT_MAX}
 visited = {SD: true, LA: true, NY: false, LDN: false}
distance = {SD: 0, LA: 1, NY: 5, LDN: 11}
 visited = {SD: true, LA: true, NY: true, LDN: false}
distance = {SD: 0, LA: 1, NY: 5, LDN: 8} 
 visited = {SD: true, LA: true, NY: true, LDN: true}
distance = {SD: 0, LA: 1, NY: 5, LDN: 8}
 void dijkstrasAlgorithm(int graph[SIZE][SIZE], int cur) {
 
  //create distance and visited arrays//
  int distance[SIZE];
  bool visited[SIZE];
   //initialize distance and visited to unreachable and unvisited//
  for(int i=0; i<SIZE; i++)
    distance[i] = INT_MAX, visited[i] = false;

  //at the starting node assign the distance to be zero//
  distance[cur] = 0;
   //iterate as long as a node is reachable and unvisited//
  while(cur!=-1) {
    
    //for neighbor nodes compare a tentative and assigned dist//
    for(int i=0; i<SIZE; i++)
      if((graph[cur][i]+distance[cur]<distance[i])&&graph[cur][i])
        distance[i] = graph[cur][i]+distance[cur];

    //mark the current node as visited and find the next node//
    visited[cur] = true, cur = -1;
    int mindistance = INT_MAX;
    for(int i=0; i<SIZE; i++)
      if(!visited[i] && (distance[i] < mindistance))
        cur = i, mindistance = distance[i];
  }
}
 
 
Item 742: 
tokens = ['Rome', 'was', 'founded', 'in', '753BC', 'by', 'its', 'first', 'king', 'Romulus']
 clean_tokens = ['Rome', 'founded', '753BC', 'first', 'king', 'Romulus']
 ['Rome', 'founded', '753BC', 'first', 'king', 'Romulus']
 ['rome', 'found', '753bc', 'first', 'king', 'romulu']
 
 
Item 745: 
def review_to_words(raw_review):
      #Remove html
      review_text=BeautifulSoup(raw_review).get_text()
      #Remove non-letters - Utilized Regex Library
      letters_only = re.sub("[^a-zA-Z]", " ", review_text)
      #Conver to lowercase and split into individual words
      words = letters_only.lower().split()
      #Convert Stop-Words to a set and remove them
      stops = set(stopwords.words("english"))
      meaningful_words = [w for w in words if not w in    stops]
      #Join the Words back into one string seperated by a  space and return the result.
      return( " ".join( meaningful_words ))
 
 
Item 746: 
Coffee consumption as an AR(1) process:
 Coffee today = Coffee yesterday + 1 cup of coffee
 Coffee consumption as an MA(3) process:
 Coffee today = (Coffee consumed over the last 3 days) / 3
 drinks = [4, 3, 4, 2, 5, 3, 8]
 
 
Item 748: 
            Tag Ratio Fcount Mcount
          Women  16.8     16      0
       The List  15.9     15      0
        Russian  10.9     10      0
 Fat Acceptance   6.9      6      0
  Body Positive   6.9      6      0
       Feminism   5.5     21      3
         Sexism   5.0      4      0
     Brain Body   5.0      4      0
           Rape   4.0      3      0
        Mueller   4.0      3      0
                Tag Ratio Mcount Fcount
         UX Design  16.8     16      0
             Apple  15.0     44      2
          Business  13.2     52      3
            iPhone  12.9     12      0
      Productivity  11.8     82      6
  Entrepreneurship  10.2    121     11
 Self Driving Cars   9.9      9      0
   Web Development   9.0     53      5
    Product Design   9.0     17      1
 Augmented Reality   8.9      8      0
                    Domain Ratio MCount FCount
 journal.thriveglobal.com  21.9     43      1
   medium.com/the-mission  10.0     59      5
              uxdesign.cc   8.9      8      0
          thinkgrowth.org   8.9      8      0
           mondaynote.com   8.9      8      0
                rantt.com   8.9      8      0
               500ish.com   8.9      8      0
       medium.com/incerto   8.9      8      0
    betterhumans.coach.me   6.9      6      0
             uxplanet.org   6.5     12      1
                                   Domain Ratio FCount MCount
                 medium.com/@Amy_Siskind  15.9     15      0
                  medium.com/the-hairpin   5.0      4      0
                medium.com/@thefatshadow   4.0      3      0
               medium.com/@girlziplocked   4.0      3      0
                             thelily.com   4.0      3      0
              medium.com/@caityjohnstone   4.0      3      0
              medium.com/behavior-design   3.0      2      0
 medium.com/conquering-corporate-america   3.0      2      0
             medium.com/code-for-america   3.0      2      0
                    medium.com/@krisgage   3.0      2      0
 
 
Item 750: 
Remix.mp3
Train set: 64
Test set: 1
[[158, 64, 125, <TEMPOS.RHYTHMIC: 'Rhythmic'>], [154, 63, 112, <TEMPOS.RHYTHMIC: 'Rhythmic'>], [167, 72, 145, <TEMPOS.RHYTHMIC: 'Rhythmic'>]]
Is Remix.mp3 rhythmic? True
Remix.mp3 is added successfully
 
 
Item 753: 
cnn1 = Sequential([
    Conv2D(8, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Conv2D(16, kernel_size=(5, 5), activation='relu'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Flatten(),
    Dense(120, activation='relu'),
    Dense(84, activation='relu'),
    Dense(10, activation='softmax')
])
 Train loss: 0.18204240553701917
Train accuracy: 0.932125
-------------------------------
Validation loss: 0.24995902764300507
Validation accuracy: 0.9090833333333334
-------------------------------
 cnn1 = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Conv2D(128, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])
 Train loss: 0.18507329044366877
Train accuracy: 0.9320625
-------------------------------
Validation loss: 0.287726696540912
Validation accuracy: 0.8989166666666667
-------------------------------
 cnn1 = Sequential([
    Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),
    Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(10, activation='softmax')
])
 Train loss: 0.05137992699308476
Train accuracy: 0.9810625
-------------------------------
Validation loss: 0.30437974256711703
Validation accuracy: 0.923
-------------------------------
 cnn1 = Sequential([
    Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),
    Conv2D(16, kernel_size=(3, 3), activation='relu',padding='same'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(10, activation='softmax')
])
 def data():
    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12345)
    X_train = X_train.astype('float32')
    X_val = X_val.astype('float32')
    X_train /= 255
    X_val /= 255
    nb_classes = 10
    Y_train = np_utils.to_categorical(y_train, nb_classes)
    Y_val = np_utils.to_categorical(y_val, nb_classes)
    return X_train, Y_train, X_val, Y_val
 def model(X_train, Y_train, X_val, Y_val):
    
    model = Sequential()
    model_choice = {{choice(['one', 'two'])}}
    if model_choice == 'one':
        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same'))
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
         model.add(Conv2D(32, kernel_size=3, activation='relu'))
        model.add(Conv2D(32, kernel_size=3, activation='relu'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
    elif model_choice == 'two':
        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
         model.add(Conv2D(64, kernel_size=3, activation='relu'))
        model.add(Conv2D(64, kernel_size=3, activation='relu'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
    
    model.add(Flatten())
    model.add(Dense({{choice([256, 512,1024])}}, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout({{uniform(0, 1)}}))
    choiceval = {{choice(['one', 'two'])}}
    if choiceval == 'two':
        model.add(Dense({{choice([256, 512,1024])}}, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout({{uniform(0, 1)}}))
    
    model.add(Dense(10, activation='softmax'))
    
    adam = keras.optimizers.Adam(lr=0.001)
    
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],
                  optimizer=adam)
     model.fit(X_train, Y_train,
              batch_size=256,
              nb_epoch=15,
              verbose=2,
              validation_data=(X_val, Y_val))
    score, acc = model.evaluate(X_val, Y_val, verbose=0)
    print('Val accuracy:', acc)
    return {'loss': -acc, 'status': STATUS_OK, 'model': model}
     model.add(Dense({{choice([256, 512,1024])}}, activation='relu'))
    model.add(BatchNormalization())
    model.add(Dropout({{uniform(0, 1)}}))
    choiceval = {{choice(['one', 'two'])}}
    if choiceval == 'two':
        model.add(Dense({{choice([256, 512,1024])}}, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout({{uniform(0, 1)}}))
 model.add(Dropout({{uniform(0, 1)}}))
 if model_choice == 'one':
        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
        model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same'))
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
        model.add(Conv2D(32, kernel_size=3, activation='relu'))
        model.add(Conv2D(32, kernel_size=3, activation='relu'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
    
elif model_choice == 'two':
        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
        model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
        model.add(Conv2D(64, kernel_size=3, activation='relu'))
        model.add(Conv2D(64, kernel_size=3, activation='relu'))
        model.add(BatchNormalization())
        model.add(MaxPooling2D(pool_size=2,strides=2))
        model.add(Dropout({{uniform(0, 1)}}))
 X_train, Y_train, X_val, Y_val = data()
 best_run, best_model = optim.minimize(model=model,
                                      data=data,
                                      algo=tpe.suggest,
                                      max_evals=30,
                                      trials=Trials(),
                                      notebook_name='Fashion_MNIST')
 model = Sequential()
model_choice = space['model_choice']
if model_choice == 'one':
   model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
   model.add(Conv2D(16, kernel_size=3, activation='relu',padding='same'))
   model.add(MaxPooling2D(pool_size=2,strides=2))
   model.add(Dropout(space['Dropout']))
   model.add(Conv2D(32, kernel_size=3, activation='relu'))
   model.add(Conv2D(32, kernel_size=3, activation='relu'))
   model.add(BatchNormalization())
   model.add(MaxPooling2D(pool_size=2,strides=2))
   model.add(Dropout(space['Dropout_1']))
elif model_choice == 'two':
   model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(1,28,28), data_format='channels_first'))
   model.add(Conv2D(32, kernel_size=3, activation='relu',padding='same'))
   model.add(MaxPooling2D(pool_size=2,strides=2))
   model.add(Dropout(space['Dropout_2']))
   model.add(Conv2D(64, kernel_size=3, activation='relu'))
   model.add(Conv2D(64, kernel_size=3, activation='relu'))
   model.add(BatchNormalization())
   model.add(MaxPooling2D(pool_size=2,strides=2))
   model.add(Dropout(space['Dropout_3']))
      
 model.add(Flatten())
 model.add(Dense(space['Dense'], activation='relu'))
 model.add(BatchNormalization())
 model.add(Dropout(space['Dropout_4']))
 choiceval = space['model_choice_1']
 if choiceval == 'two':
   model.add(Dense(space['Dense_1'], activation='relu'))
   model.add(BatchNormalization())
   model.add(Dropout(space['Dropout_5']))
   model.add(Dense(10, activation='softmax'))
adam = keras.optimizers.Adam(lr=0.001)
 {'Dense': 1, 'Dense_1': 2, 'Dropout': 0.2799579955710103, 'Dropout_1': 0.8593089514091055, 'Dropout_2': 0.17434082481320767, 'Dropout_3': 0.2839296185815494, 'Dropout_4': 0.7087321230411557, 'Dropout_5': 0.3273210014856124, 'model_choice': 1, 'model_choice_1': 0}
 cnn1 = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu',padding='same',input_shape=input_shape),
    Conv2D(32, kernel_size=(3, 3), activation='relu',padding='same'),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Dropout(0.2),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2),strides=2),
    Dropout(0.3),
    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.7),
    Dense(10, activation='softmax')
])
 Train loss: 0.14321659112690638
Train accuracy: 0.9463333333333334
-------------------------------
Validation loss: 0.17808779059847196
Validation accuracy: 0.93525
-------------------------------
Test loss: 0.20868439328074456
Test accuracy: 0.9227
 
 
Item 755: 
yv=[1:4] == y;   %creating logical vectors of y values
 X = [ones(m,1),X];
 1:  m = size(X, 1);
2:  X = [ones(m,1),X]; 
3:  a1 = X;
4:  z2 = Theta1*a1';
5:  a2 = sigmoid(z2);
6:  a2 = [ones(1,m);a2];
7:  z3 = Theta2*a2;
8:  a3 = sigmoid(z3);
9:  yv=[1:4] == y;
10: J = (1/m) * (sum(-yv’ .* log(a3) — ((1 — yv’) .* log(1 — a3))));
11: J = sum(J);
 z2 = Theta1 * a1;      %WRONG! THIS DOESN'T GIVE US WHAT WE WANT
 z2 = Theta1*a1';
 a2 = sigmoid(z2);
 z3 = Theta2*a2;
 a2 = [ones(1,m);a2];
 z3 = Theta2*a2;
 a3 = sigmoid(z3);
 J = (1/m) * (sum(-yv’ .* log(a3) — ((1 — yv’) .* log(1 — a3))));
J = sum(J);
 
 
Item 761: 
{
 "token": "1242ab77d1202aff87bd0b9e96e5a436",
 "usertoken": ["1242ab77d1202aff87bd0b9e96e5a436"],
 "database": "foodmart_demo",
 "params": "total sales"
 }
 {
  "status": "OK",
  "data": {
    "trash_list": []
  },
  "result": "SELECT SUM(sales_fact.store_sales) as `sum.sales_fact.store_sales`\nFROM sales_fact\n ",
  "method": "server.getSql"
}
 
 
Item 766: 
income = ($5,000 * years_of_education) + baseline_income
 
 
Item 767: 
>>> [1. 4. 5. 8.]  
>>>[[1. 2. 3.]
    [4. 5. 6.]]
 >>> [1. 4. 5. 8.]  
>>> 4.0
>>> [1. 4.] 
>>> 5.0
 >>> [[1. 2. 3.]  
     [4. 5. 6.]]  
>>> 5.0  
>>> [4. 5. 6.]  
>>> [3. 6.]
 >>> [6. 4. 9.]  
>>> [-4.  0. -3.]  
>>> [ 5.  4. 18.]
 >>> [[ 6.  8.]  [10. 12.]]  
>>> [[-4. -4.]  [-4. -4.]]  
>>> [[ 5. 12.]  [21. 32.]]
 >>> 2.0 
>>> 7.0  
>>> [44.]
 0           Dave 
1      Cheng-Han 
2        Udacity 
3             42 
4    -1789710578
 >>>
Instructor               Dave 
Curriculum Manager    Cheng-Han 
Course Number               359 
Power Level                9001
 >>> Dave
 >>>
Instructor                 Dave 
Curriculum Manager    Cheng-Han 
Course Number               359
 >>>
Cockroach    1 
Fish         2 
Mini Pig     3 
Puppy        4 
Kitten       5 
 >>>
Cockroach    False 
Fish         False 
Mini Pig     False 
Puppy         True 
Kitten        True 
 >>>
Puppy     4 
Kitten    5 
 >>>
   losses     team  wins  year 
0       5    Bears    11  2010 
1       8    Bears     8  2011 
2       6    Bears    10  2012 
3       1  Packers    15  2011 
4       5  Packers    11  2012 
5      10    Lions     6  2010 
6       6    Lions    10  2011 
7      12    Lions     4  2012
 >>>
losses     int64 
team      object 
wins       int64 
year       int64 
dtype: object
 >>>
          losses       wins         year 
count   8.000000   8.000000     8.000000 
mean    6.625000   9.375000  2011.125000 
std     3.377975   3.377975     0.834523 
min     1.000000   4.000000  2010.000000 
25%     5.000000   7.500000  2010.750000 
50%     6.000000  10.000000  2011.000000 
75%     8.500000  11.000000  2012.000000 
max    12.000000  15.000000  2012.000000
 >>>
    losses     team  wins  year 
0       5    Bears    11  2010 
1       8    Bears     8  2011 
2       6    Bears    10  2012 
3       1  Packers    15  2011 
4       5  Packers    11  2012
 >>>
    losses     team  wins  year 
3       1  Packers    15  2011 
4       5  Packers    11  2012 
5      10    Lions     6  2010 
6       6    Lions    10  2011 
7      12    Lions     4  2012
 >>>
0    2010 
1    2011 
2    2012 
3    2011 
4    2012 
5    2010 
6    2011 
7    2012 
Name: year, dtype: int64  
 >>> ... Same as above
>>>
    year  wins  losses 
0  2010    11       5 
1  2011     8       8 
2  2012    10       6 
3  2011    15       1 
4  2012    11       5 
5  2010     6      10 
6  2011    10       6 
7  2012     4      12
 >>>
   losses   team  wins  year 
0       5  Bears    11  2010     
>>>
   losses   team  wins  year 
0       5  Bears    11  2010     
>>>
   losses     team  wins  year 
3       1  Packers    15  2011 
4       5  Packers    11  2012
 >>>
   losses     team  wins  year 
0       5    Bears    11  2010 
3       1  Packers    15  2011 
4       5  Packers    11  2012
 >>>
   losses     team  wins  year 
3       1  Packers    15  2011 
4       5  Packers    11  2012
 
 
Item 768: 
i make ten thousand dollars per week just surfing the web! (spam)
are you free for a meeting early next week? (not spam)
 i: 0
make: 1
ten: 2
thousand: 3
dollars: 4
per: 5
week: 6
just: 7
surfing: 8
the: 9
web: 10
are: 11
you: 12
free: 13
for: 14
a: 15
meeting: 16
early: 17
next: 18
 i make ten thousand dollars per week just surfing the web! (spam)
-> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 are you free for a meeting early next week? (not spam)
-> [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
 i make ten thousand dollars per week just surfing the web! (spam)
-> [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]
 are you free for a meeting early next week? (not spam)
-> [0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0]
 ii mayke are you th0usands of free for a $$$s surf1ing teh webz meeting early next week
 ii mayke are you th0usands of free for a $$$s surf1ing teh webz meeting early next week
-> [0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 1 1 0]
 "John Doe" -> hash function -> 34
"Jane Doe" -> hash function -> 48
 i make ten thousand dollars per week just surfing the web! (spam)
-> [0 0 0 0 ... 0 0 0 0] (2^28 elements)
 are you free for a meeting early next week? (not spam)
-> [0 0 0 0 ... 0 0 0 0] (2^28 elements)
 i make ten thousand dollars per week just surfing the web! (spam)
-> [0 ... 0 1 1 1 0 1 1 0 ... 0 1 1 1 1 0 1 1 0] (2^28 elements)
 are you free for a meeting early next week? (not spam)
-> [0 1 0 1 0 ... 0 1 0 ... 0 1 0 ... 0 1 10 1 1 0 1] (2^28 elements)
 
 
Item 769: 
[('stoke', 0.8392074108123779), ('non-cardioembolic', 0.789894700050354), ('noncardioembolic', 0.7787829041481018), ('TIA', 0.7507107257843018), ('Stroke', 0.7489094734191895), ('strokes', 0.747207760810852), ('Cardioembolic', 0.7449108362197876), ('TIA/minor', 0.7419171929359436), ('infarction', 0.7395045161247253), ('non-lacunar', 0.7321403622627258)]
 [('palpitations', 0.9567112922668457),
 ('cardiomegaly', 0.9465205073356628),
 ('cyanosis', 0.9343518018722534),
 ('diaphoresis', 0.9291665554046631),
 ('muffled', 0.9271412491798401),
 ('orthopnea', 0.9218621850013733),
 ('palpitation', 0.9127764105796814),
 ('near-syncope', 0.9108157753944397),
 ('pre-syncope', 0.9049088954925537),
 ('bradycardia', 0.9047020077705383)]
 [('SK-N-SH', 0.8899902105331421), Sk-N-SH: neuroblastoma cells
 ('IMR-32', 0.887042224407196), IMR-32:human neuroblastoma cell line
 ('glioma', 0.8706138134002686),
 ('astrocyte', 0.8669655919075012),
 ('Hs683', 0.8647631406784058), Hs683: cell line astrocytoma, glioma
 ('Neuro-2A', 0.8643868565559387), Neuro-2A: neuroblastoma cell
 ('astrocytes', 0.862960696220398),
 ('U87-MG', 0.8623798489570618), U87-MG: Malignant Glioma
 ('U373', 0.8599711656570435), U373: Human glioblastoma astrocytoma
 ('U-87', 0.8597809672355652)], U-87: primary glioblastoma cell line
 
 
Item 784: 
jupyter nbconvert *.ipynb --to slides --post serve
 jupyter nbconvert dino_viz.ipynb --to slides --post serve 
--SlidesExporter.reveal_theme=serif 
--SlidesExporter.reveal_scroll=True 
--SlidesExporter.reveal_transition=none
 git submodule add https://github.com/hakimel/reveal.js.git reveal.js
 jupyter nbconvert --to slides index.ipynb --reveal-prefix=reveal.js 
 jupyter nbconvert --to slides index.ipynb --reveal-prefix=reveal.js --SlidesExporter.reveal_theme=serif 
--SlidesExporter.reveal_scroll=True 
--SlidesExporter.reveal_transition=none
 
 
Item 788: 
#Build the model on training data -
lmMod <- lm(DependentVariable ~ IndependentVariable, data=trainingData)  
 #Run prediction on test data -
testPred <- predict(lmMod, testData)
 
 
Item 790: 
Artist: black lips, Mean Score: 7.48, Genres: ['rock', 'rock', 'rock', 'rock', 'rock']
 Artist: crookers, Mean Score: 5.5, Genres: ['electronic']
 Artist: guided by voices, Mean Score: 7.23043478261, Genres: ['rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock', 'rock']
 
 
Item 792: 
Hi there
 What's the name of the medication?
 'best price on Adderall is $40.56 (cash) at Costco, 3250 Northlake Blvd Palm Beach Gardens'
 'use this coupon: https://www.goodrx.com/coupon/dispatch/8BI34djdRjZfhn6VCTT8TcV_AueHH5T6ZLelmRA0TCbB ...
 {'city': 'JUPITER', 'state': 'FL'}
 [{'address': '2144 W Indiantown Rd',
  'city': 'Jupiter',
  'state': 'FL',
  'zip': '33458'}]
 'Lipitor'
 ['Price range: $10.55 - $24.56',
 {'cash-price': '40',
  'href': 'https://www.goodrx.com/coupon/dispatch/Dl2epbMbnu2CusdTSgaD6nsC1sMQraY_5p7wBeHyjuItgHTq9EqokDpPM_L6HfFJanBo7inaungbesjKyDPy9f7diGNNAkmzxI8bh0bM79gbGQ%3D%3D',
  'price': '10.55',
  'qualifier': 'with free Coupon',
  'store': 'Safeway'},
 {'cash-price': '35',
  'href': 'https://www.goodrx.com/coupon/dispatch/ta3cubPJ-Sy6w2XvkNJhgHNISZoEkj92wMXhtpcKbBKe_eKKaHfxXyl1Ktc-42-0Emrxa2CV4HJ-MG1JfoU2Qsi3Qyau1GgDK4CZEV14',
  'price': '12.80',
  'qualifier': 'with free Coupon',
  'store': 'Kroger Pharmacy'},
 ...
 {'address': '3250 Northlake Blvd',
 'city': 'Palm Beach Gardens',
 'price': '40.56',
 'state': 'FL',
 'store': 'Costco',
 'type': 'cash'}
 
 
Item 795: 
{
 “id”: “G024BE91L”,
 “name”: “secretplans”,
 “is_group”: “true”,
 “created”: 1360782804,
 “creator”: “U024BE7LH”,
 “is_archived”: false,
 “is_mpim”: false,
 “members”: [
 “U024BE7LH”
 ],
 “topic”: {
 “value”: “Secret plans on hold”,
 “creator”: “U024BE7LV”,
 “last_set”: 1369677212
 },
 “purpose”: {
 “value”: “Discuss secret plans that no-one else should know”,
 “creator”: “U024BE7LH”,
 “last_set”: 1360782804
 },
  “last_read”: “1401383885.000061”,
 “latest”: { … }
 “unread_count”: 0,
 “unread_count_display”: 0
}
 private final Pattern MARKUP = Pattern 
.compile(“(?:(?<markupTag>\\*|_|~|(?:```)|`)(?<markupContent>.*?)\\1)|(?::(?<emojiName>[a-z_0–9-+]*?):)|(?:^\\s*&gt;(?<quoteContent>.*)$)|(?:<(?<link>(?<linkType>#|@|mailto:|!|http|https|).*?)(?:\\|(?<linkText>.*?))?>)”);
 
 
Item 797: 
[9.79903930e-02 1.18781873e-09]
 
 
Item 803: 
my_variable = tf.get_variable("my_variable", [1, 2, 3])
 session.run(tf.global_variables_initializer())
 session.run(z3)
 
 
Item 804: 
1. What previous research and ideas were cited that this paper is building off of? (this info tends to live in the introduction)
2. Was there reasoning for performing this research, if so what was it? (introduction section)
3. Clearly list out the objectives of the study
4. Was any equipment/software used? (methods section)
5. What variables were measured during experimentation? (methods)
6. Were any statistical tests used? What were their results? (methods/results section)
7. What are the main findings? (results section)
8. How do these results fit into the context of other research and their 'field'? (discussion section)
9. Explain each figure and discuss their significance.
10. Can the results be reproduced and is there any code available?
11. Name the authors, year, and title of the paper!
12. Are any of the authors familiar, do you know their previous work? 
13. What key terms and concepts do I not know and need to look up in a dictionary, textbook, or ask someone?
14. What are your thoughts on the results? Do they seem valid?
 
 
Item 808: 
pcm.!default {
    type asym
    playback.pcm {
        type plug
        slave.pcm "hw:0,0"
    }
    capture.pcm {
        type dsnoop
        ipc_key 5432
        slave {
            pcm "hw:1,0"
            buffer_size 96000
        }
    }
}
 
 
Item 810: 
cs.* (Computer Science - All subcategories)
cond-mat.dis-nn (Physics - Disordered Systems and Neural Networks)
q-bio.NC (Quantitative Biology - Neurons and Cognition)
stat.CO (Statistics - Computation)
stat.ML (Statistics - Machine Learning)
 1. learning
2. mdps
3. neural
4. adversarial
5. training
6. deep
7. tasks
8. fairness
9. recurrent
10. generative
11. human
12. answering
13. dataset
14. reinforcement
15. rl
16. artificial intelligence
17. variational
18. sgd
19. neural networks
20. dnn
21. humans
22. trained
23. shot
24. bias
25. generate
 1. investigate
2. stereo
3. proposed
4. svm
5. skin
6. registration
7. presented
8. analysis
9. lesion
10. cameras
11. extracted
12. matching
13. used
14. regression
15. reconstruction
16. time series
17. using
18. breast
19. nlp
20. clustering
21. f1
22. 2d
23. food
24. events
25. edge
 
 
Item 814: 
matrix:
  lr:
    logspace: 0.01:0.1:5

  loss:
    values: [MeanSquaredError, AbsoluteDifference]
 settings:
   grid_search:
    n_experiments: 10
   concurrency: 2
  
  matrix:
    learning_rate:
      linspace: 0.001:0.1:5
    dropout:
      values: [0.25, 0.3]
    activation:
      values: [relu, sigmoid]
 settings:
   random_search:
    n_experiments: 30
   concurrency: 2
  
  matrix:
    learning_rate:
      uniform: 0.001:0.01
    dropout:
      values: [0.25, 0.3, 0.5]
    activation:
      pvalues: [(relu, 0.2), (sigmoid, 0.8)]
 resource:
  name: num_steps
  type: int
 metric:
  name: loss
  optimization: minimize
 metric:
  name: accuracy
  optimization: maximize
 settings:
   hyperband:
    max_iter: 81
    eta: 3
    resource:
      name: num_steps
      type: int
    metric:
      name: loss
      optimization: minimize
    resume: False
   concurrency: 2
  
  matrix:
    learning_rate:
      uniform: 0.001:0.01
    dropout:
      values: [0.25, 0.3, 0.5]
    activation:
      pvalues: [(relu, 0.2), (sigmoid, 0.8)]
 settings:

  bo:
    n_iterations: 5
    n_initial_trials: 10
    metric:
      name: loss
      optimization: minimize
    utility_function:
      acquisition_function: ucb
      kappa: 2.576
      gaussian_process:
        kernel: matern
        length_scale: 1.0
        nu: 1.9
        n_restarts_optimizer: 0
   concurrency: 2
  
  matrix:
    learning_rate:
      uniform: [0.001, 0.01]
    dropout:
      values: [0.25, 0.3]
    activation:
      values: [relu, sigmoid]
 early_stopping:
    - metric: accuracy
      value: 0.9
      optimization: maximize
    - metric: loss
      value: 0.05
      optimization: minimize
 
 
Item 822: 
User
Alexa, set a timer for 5 minutes.
 Alexa
Setting a timer for 5 minutes.
 User
Alexa, set a timer for 5 minutes.
 Alexa
Setting a timer for 5 minutes.
 Alexa
OK, timer for 5 minutes set successfully.
 Barge-in: Interrupt the conversation at any time
 In case of a hybrid model of command-and-control and conversational modes, the system could allow the user to interrupt the conversation at any time. This is called barge-in. Enabling barge-in is critical if the user wants to get out of the conversational flow at any time. For example, in the middle of the game of Jeopardy, the user can barge in and say “Alexa, stop the game.”
 Assistant
Did you get a good night’s sleep?
 User
Yes
 Assistant
Did you read before sleeping?
 User
Yes
 Assistant
What did you read?
 User
The Rise of the Robots
 Assistant
Ok
 Assistant
Hi there! I’m going to ask you a few questions about your sleep. Did you get a good night’s sleep?
 User
Yes
 Assistant
That’s awesome. Did you read before sleeping?
 User
Yes
 Assistant
Good job. What did you read?
 User
The Rise of the Robots
 Assistant
That’s a nice book. Have a great day!
 Priming
 In psychology, priming is a technique whereby exposure to one stimulus influences a response to a subsequent stimulus, without conscious guidance or intention. For example, you are more likely to answer ‘Mongolia’ when asked to name a place that starts with the letter ‘M’ if you’ve just seen a documentary about Mongolia.
 Letting the user know what to expect is also a form of priming. It informs users on how to prepare themselves. Priming however can be subtle, if the VUI responds to the query “Can you play me ‘Fix you’ by ‘Coldplay’?” as “Playing ‘Fix you’ by ‘Coldplay.’” Next time the user can simply say “Play ‘Fix you’ by ‘Coldplay’”
 
 
Item 824: 
torch.Tensor(x, y)
 torch.Tensor(5, 3).uniform_(-1, 1)
 print(x.size())
 y = torch.rand(5, 3)
print(x + y)
 result = torch.Tensor(5, 3)
torch.add(x, y, out=result)
 y.add_(x)
 print(x[:, 1])
 y = torch.randn(5, 10, 15)
print(y.size())
print(y.view(-1, 15).size())
 a = torch.ones(5)
b = a.numpy()
 if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
 x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = Variable(torch.Tensor([1.0]),  requires_grad=True)
 l = loss(x_val, y_val)
l.backward()
 
 
Item 829: 
library(tidyverse)
library(rvest)
 
 
Item 831: 
{
    "x": [1, 2, 3, 4],
    "y": [3, 1, 2, 5],
    "mode": "markers",
    "marker": {
        "size": 18
    },
     "selectedpoints": [0, 2],
     "selected": {
        "marker": {
            "color": "blue"
        }
    },
    "unselected": {
        "marker": {
            "color": "aqua"
        }
    }
}
 
 
Item 836: 
“save_classes”: “__all__”,
 “save_classes”: [“person”],
 curl -XPOST -H ‘X-API-KEY: YourToken’ -F ‘image=@dl_heroes.jpg;type=image/jpeg’ YourUrl
 [{"tags":[],"description":"","objects":[{"bitmap":{"origin":[],"np":[]},"type":"rectangle","classTitle":"person","description":"","tags":[],"points":{"exterior":[[343,87],[493,375]],"interior":[]},"score":0.999502420425415},{"bitmap":{"origin":[],"np":[]},"type":"rectangle","classTitle":"person","description":"","tags":[],"points":{"exterior":[[0,94],[149,375]],"interior":[]},"score":0.9994213581085205},{"bitmap":{"origin":[],"np":[]},"type":"rectangle","classTitle":"person","description":"","tags":[],"points":{"exterior":[[247,96],[367,377]],"interior":[]},"score":0.9987866282463074},{"bitmap":{"origin":[],"np":[]},"type":"rectangle","classTitle":"person","description":"","tags":[],"points":{"exterior":[[138,96],[256,378]],"interior":[]},"score":0.99868243932724},{"bitmap":{"origin":[],"np":[]},"type":"rectangle","classTitle":"person","description":"","tags":[],"points":{"exterior":[[100,133],[129,177]],"interior":[]},"score":0.9136056900024414}],"size":{"width":506,"height":380}}]
 
 
Item 845: 
person, bicycle, car, motorcycle, airplane,
bus, train, truck, boat, traffic light, fire hydrant, stop_sign,
parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra,
giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard,
sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket,
bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange,
broccoli, carrot, hot dog, pizza, donot, cake, chair, couch, potted plant, bed,
dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave,
oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair dryer,
toothbrush.
 
 
Item 847: 
1. Pick the network architecture(initialize with random weights)
2. Do a forward pass (Forward propagation)
3. Calculate the total error(we need to minimize this error)
4. Back propagate the error and Update weights(Back propagation)
5. Repeat the process(2-4)for no of epochs/until error is minimum.
 1. Pick the network architecture 
 2.Forward propagation
 dataset (XOR table) 
 X     y
1 1    0 --> X1=1 and X2=1 
1 0    1     H1 = Sigmoid(X1*w1+X2*w2) = 0.5(assume with random                                  
0 1    1                                                 weights)
0 0    0     similarly H2, H3 and O1, O2
 3.Calculate the total error.
 4. Back propagation
 5. Repeat the process(2-4)for no of epochs/until error is minimum.
 
 
Item 848: 
pip3 install tensorflow
 pip3 install keras
 pip3 install h5py
 
 
Item 853: 
adventure characters epic fantasy world fighting photography Action adventure atmospheric based on a book based on book beautifully filmed ensemble cast fantasy fantasy world high fantasy imdb top 250 magic music nature nothing at all Oscar (Best Cinematography) Oscar (Best Effects - Visual Effects) scenic stylized Tolkien wizards adventure atmospheric ensemble cast fantasy fantasy world magic stylized wizards Watched adapted from:book author:J. R. R. Tolkein based on book epic fantasy middle earth faithful to book fantasy good versus evil high fantasy joseph campbell's study of mythology influenced magic atmospheric boring high fantasy Action adventure atmospheric based on a book beautifully filmed fantasy high fantasy magic music mythology romance stylized time travel Viggo Mortensen wizards Peter Jackson Peter Jackson music must see Tolkien high fantasy Myth Tolkien wizards Ian McKellen bast background universe
 
 
Item 855: 
f(x, y, z) = h(g(x, y) , z)
g(i, j) = i + j
h(p, q) = p*q
 f(1, 2, 3) = h(g(1, 2), 3)
g(1, 2) = 1 + 2 = 3
f(1, 2, 3) = h(3, 3)
h(3, 3) = 3*3 = 9
f(1, 2, 3) = 9
 f(age, gender, city) = predicted_relationship_status
 f(x, y) = ax² + bxy + cy²; where a, b, and c are scalars
 
 
Item 856: 
IN:
['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']
 OUT:
['try', 'navigate', 'first', 'bold', 'flight', ',', 'reaction', 'taken', 'something', 'soul', '.']
 IN:
 "He did not try to navigate after the first bold flight, for the reaction had taken something out of his soul."
 OUT:
 ['He', 'did', 'not', 'try', 'to', 'navigate', 'after', 'the', 'first', 'bold', 'flight', ',', 'for', 'the', 'reaction', 'had', 'taken', 'something', 'out', 'of', 'his', 'soul', '.']
 IN:
['And', 'from', 'their', 'high', 'summits', ',', 'one', 'by', 'one', ',', 'drop', 'everlasting', 'dews', '.']
 OUT:
[('And', 'CC'),
 ('from', 'IN'),
 ('their', 'PRP$'),
 ('high', 'JJ'),
 ('summits', 'NNS'),
 (',', ','),
 ('one', 'CD'),
 ('by', 'IN'),
 ('one', 'CD'),
 (',', ','),
 ('drop', 'NN'),
 ('everlasting', 'VBG'),
 ('dews', 'NNS'),
 ('.', '.')]
 Definitions of Parts of Speech
('their', 'PRP$') PRP$: pronoun, possessive
    her his mine my our ours their thy your
 IN:
["It never once occurred to me that the fumbling might be a mere mistake."]
 OUT:
 ['it', 'never',  'onc',  'occur',  'to',  'me',  'that',  'the', 'fumbl',  'might', 'be', 'a', 'mere',  'mistake.'],
 The stemmed form of leafs is: leaf
The stemmed form of leaves is: leav
 The lemmatized form of leafs is: leaf
The lemmatized form of leaves is: leaf
 
 
Item 859: 
{
 “parameters”: {
 “keyword.original”: “chat”,
 “keyword”: [“chat”]
 }
}
 
 
Item 864: 
go get golang.org/x/tour/gotour
 
 
Item 866: 
{"and": 0.87, "the": 0.73}
 ... These are external links and will open in a new windowA neat feature of many modern laptops is the ability to power them up through the USB port. Unlike the rectangular USB ports of old, the newer type - USB-C - can carry enough power to charge your 
machine.That’s great news: it means ...
 Share this withEmailFacebookMessengerMessengerTwitterPinterestWhatsAppLinkedInCopy this linkThese are external links and will open in a new windowRyanair is tightening the rules on what passengers pay to take luggage onto the plane, in order to "speed up boarding".
 Ryanair is tightening the rules on what passengers pay to take luggage onto the plane, in order to "speed up boarding"
 
 
Item 867: 
crawl-main:
    scrapy crawl ebay_wear
crawl-enrich: crawl-main
    RECENCY_ONLY=1 IN_7=1 scrapy crawl zara
    RECENCY_ONLY=1 IN_30=1 scrapy crawl h_n_m
    RECENCY_ONLY=1 IN_90=1 scrapy crawl zappos
agg-last: crawl-enrich
    python process_results.py
 JOB_ID=run-11 make agg-last
 /out
  /run-10
    ebay_wear.csv
    zara.csv
    h_n_m.csv
    zappos.csv
    merged.csv
    agged.csv
  /run-11
    ebay_wear.csv
    zara.csv
    h_n_m.csv
    zappos.csv
    merged.csv
    agged.csv
 
 
Item 871: 
Fëanor and Fingolfin and Men were so at the top and saw the brothers that the stars of hard they could go had a ground of he shall. In the trees of the treps had some words; and he had said at his fear and sheer on their booled; and the trees at tell her. But a bow her stands say and the boat on the despair of the sea and and shining, and he had nearer they came to the store of a long shoping they would see the black beard, and shall see. The white was to the saunt of the song and shoulders of any stirred. ‘The woods was been a between words and wooded the with a great waters as a wood with take one of sat and saw a good.’ I called the sense of the March of Sauloth in the Song of the Silmarils of the Elvens of the Misty Ring of the Naun he shall not come on a s
 Beren and Lúthien, and they deven at how seemed in. ‘I had been,” he said and some are that still huse here again, and seen hobbits of the song of Minas Tirith and there the shall a long first shield they seemed alone; but the still seeks and burrent, standed on the ships was a shadow about the stars in take the learn of the still of those that here, and she can hold on a ships of the time as that seen, but they shall not hear. He was a blanken already he soon, and that he were the black, and they hoursed to the star of a stream, still as the lands of his would heard and which think went the listens that the same and searth and the leaves of the stofter of the Might of the Elves of the North of the Ring of Gondor. But a golden hard seemers and as any wellow.
 Aragorn and Arwen; from. Bilbo has nucking if the stranges tone that as were brought him on the shape, they seemed and set. Then they was a stone tone to him, and he had sinted a middle of through the stars, that seemed to the stone as they had been stared and best.

‘I was netted in still a little andow that was all the world of his head, and have the less took were to here with the hands of Minas Tarin. But they was a laster of stand or to hobbits with him, and say of this hall of the Merry of the Noldor of the Men the listentar, and they had become of the still, but it were sharped a bear of the deal of this shore of the Stirgount in his, and then the lunged west which he shall be to somp them as he saw the time on the stoused sea, and so there were forde
 Smaug, Bilbo and Gandalf alone in hobbits than they deven a foring, both a should best, and to a stories seemed at our first and star at one back. There had too both and the time with his head and theys, but they heard to him were battle track there. But he walls to the holain.

‘I came up and the house though you set him. If they had sat to think, and how the white, they were things, and his hound had been became to a ground to an is the sons, and he had brought the stone, and all the wise and a shone of the hand of these sent of as had say the tankes and to his sank at a book of the three a steper, but in his same traid and was bore and all them out of his far the last and ferchour at, but the rise of to the high tale. I had some hare or belossed in the station.
 Ents and Entwives of Men through the Saruman. Then went and bethe was the thought; and he was buitted is beated, to see helped to her breached the thought of the Marth that hart the tines.

‘To that shall all the lisses. But the hand of the shirr than any hand was taken that the song of the Morner our time they had started and that set in the top of the horn of the Mountain, and as a bottom of a bragening wonder in the way with his things and strided of the bright. All there were a large shadow of the sound of the worm in her horses to the song of the Shire, and the stars that with the tares of the waters of his tried of hard the strish was business of the water of his top and as all the same and the white horse and seemed;

They were bundled the shadow th
 A new fellowship of the rings with a bank, shorter at all the world.

There was he sang oft, but at least the tarred were they wish as he had said they sat on it and hurry that the shadow they had a galrilings and here. All to the waters was been began the bear and tried in the dark bent which she stowed.

‘Then it was neas of stories that then then, best a some as they sound in that they deep and to the day is them. Bilbo were not a side to a lord, but that his shorls all the way or horn, all the like sang, and then the way, what were stay. The storiss though we said.

‘That say and horry tha the with was the what all him. As I should tell we there some soon at his fellows and his tale as all a long wars. A time was strong, and who were all them to time. He seemed, se
 
 
Item 872: 
logits = [2.0, 1.0, 0.1]
import numpy as np
exps = [np.exp(i) for i in logits]
 sum_of_exps = sum(exps)
 softmax = [j/sum_of_exps for j in exps]
 >>> softmax
[0.6590011388859679, 0.2424329707047139, 0.09856589040931818]
>>> sum(softmax)
1.0
 [1,0,0] #cat
[0,1,0] #dog
[0,0,1] #bird
 
 
Item 874: 
git clone https://github.com/bloomsburyai/ctrlf-tutorial
 
 
Item 880: 
1 = Blatant scam. Avoid at all costs!
 2 = Avoid this as well, too many red flags.
 3 = A terrible project but with some glimmer of hope.
 4 = Not recommended, unless you have spare change.
 5 = We like the concept, don't have complete faith in the project.
 6 = Perhaps good as a short term hold until it hits an exchange.
 7 = A good buy, but some concerns.
 8 = Strongly consider investing in this.
 9 = Potentially great, but we have some reservations.
10 = Sell the wife and kids, this is a home banker!
 
 
Item 881: 
export API_KEY=<YOUR_API_KEY>
 {
 "document":{
 "type":"PLAIN_TEXT",
 "content":"Inception is one of the best movies of all time. I think everyone should watch it."
 },
 "encodingType": "UTF8"
 }
 curl "https://language.googleapis.com/v1/documents:analyzeSentiment?key=${API_KEY}" -s -X POST -H "Content-Type: application/json" --data-binary @request.json
 {
 "document":{
 "type":"PLAIN_TEXT",
 "content":"The food was really bad at this restaurant. However, really enjoyed the drinks."
 },
 "encodingType": "UTF8"
 }
 
 
Item 884: 
fig,ax = plt.subplots()

ax.plot(x, y)
plt.show()
 plt.plot(x, y)
plt.title('SEU TÍTULO LINDO')
plt.show()
 plt.plot(x, y) #Criando o gráfico
plt.title('SEU TÍTULO LINDO') #adicionando o título
 plt.xlabel('NOME DO EIXO X')
plt.ylabel('NOME DO EIXO Y')
 plt.show()
 seu_dataframe.plot()
plt.title('SEU TÍTULO LINDO') #adicionando o título
 plt.xlabel('NOME DO EIXO X')
plt.ylabel('NOME DO EIXO Y')
 plt.show()
 seu_dataframe.plot(x, y, kind='bar')
 seu_dataframe.plot() #criando gráfico
plt.title('SEU TÍTULO LINDO') #adicionando o título
 plt.xlabel('NOME DO EIXO X') #definindo nome do eixo X
plt.ylabel('NOME DO EIXO Y') #definindo nome do eixo Y
 plt.legend(loc='best') #colocando a legenda no melhor lugar
 plt.show() #mostrando gráfico
 plt.plot(x, y, color='green')
plt.scatter(x, y, color='red')
plt.show()
 plt.plot(x, y, 'b--')
plt.scatter(x, y, marker="*", color='red')
plt.show()
 fig, ax = plt.subplots()
ax.bar(x, y)
ax.annotate("Maior valor", 
            xy=(10, 12),
            xycoords='data',
            xytext=(11, 10),
            textcoords='data',
            arrowprops=dict(arrowstyle="->",connectionstyle="arc3"))
plt.show()
 
 
Item 886: 
def load_data(filename, seq_len, normalise_window):
    f = open(filename, 'r').read()
    data = f.split('\n')
    sequence_length = seq_len + 1
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index: index + sequence_length])
    if normalise_window:
        result = normalise_windows(result)
    result = np.array(result)
    return result
 def normalise_windows(window_data):
    normalised_data = []
    for window in window_data:
        normalised_window = [((float(p) / float(window[0])) - 1) for p in window]
        normalised_data.append(normalised_window)
    return normalised_data
 
 
Item 887: 
so if predicted  = 1 and actual= 1
error = 0
so if predicted  = 1 and actual= 0
error = 1
so if predicted  = 0 and actual= 1
error = 1
so if predicted  = 0 and actual= 0
error = 0
 Note: predicted can be 0.5 and so on... also
So every time we get the error between 0 and 1 which is not useful.
 
 
Item 888: 
dbinom(0,12,1/3)
[1] 0.007707347
 dbinom(0,12,1/3)*26306
[1] 202.7495
 dbinom(12,12,1/3)
[1] 1.881676e-06
 dbinom(12,12,1/3)*26306
[1] 0.04949938
 PearsonChiSquare <- data.frame(Face5or6=c(0,1,2,3,4,5,6,7,8,9,10,11,12),
                                  Theoretical=c(203,1217,3345,5576,6273,5018,2927,1254,392,87,13,1,0),
                                  Observed=c(185,1149,3265,5475,6114,5194,3067,1331,403,105,14,4,0))
 #as group of 12 has 0 theoretical and 0 observed counts, drop this observation
PearsonChiSquare <- PearsonChiSquare[!(PearsonChiSquare$Theoretical==0),]
 PearsonChiSquare$probs <- with(PearsonChiSquare, Theoretical/sum(Theoretical))
 chisq.test(x=PearsonChiSquare$Observed, p=PearsonChiSquare$probs)
 Chi-squared test for given probabilities
 data:  PearsonChiSquare$Observed
X-squared = 43.876, df = 11, p-value = 7.641e-06
 PearsonChiSquare$Deviation <- with(PearsonChiSquare,Observed-Theoretical)
 PearsonChiSquare[, c("Face5or6","Deviation")]
    Face5or6 Deviation
1         0       -18
2         1       -68
3         2       -80
4         3      -101
5         4      -159
6         5       176
7         6       140
8         7        77
9         8        11
10        9        18
11       10         1
12       11         3
 library("ggplot2")
ggplot(data.frame(x=c(0,50)), aes(x=x))+
        stat_function(fun=dchisq, args=list(df=11))+
        labs(title="Chi-squared probability distribution for 11 df",
             x="Chi-squared value",
             y="Probability")
 ggplot(data.frame(x=c(0,50)), aes(x=x))+
        stat_function(fun=dchisq, args=list(df=11), aes(colour="11 df"))+
        stat_function(fun=dchisq, args=list(df=12), aes(colour="12 df"))+
        scale_colour_manual(name="", values=c("black","blue"))+
        labs(title="Chi-squared probability distribution for 11 and 12 df",
             x="Chi-squared value",
             y="Probability")
 RejectionArea   <- data.frame(x=seq(0,50,0.1))
RejectionArea$y <- dchisq(RejectionArea$x,11)
 library(ggplot2)
ggplot(RejectionArea) + 
  geom_path(aes(x,y)) +
 geom_ribbon(data=RejectionArea[RejectionArea$x>qchisq(0.025,11,lower.tail = FALSE),],
                 aes(x, ymin=0, ymax=y),fill="red")+
  labs(title="Chi-squared probability distribution for 11 df showing rejection area\nfor p<=0.025",
       x="Chi-squared value",
       y="Probability")
 
 
Item 892: 
#Installing Keras and Tensorflow at the Backend 
 #You need to install Rtools for installing ‘reticulate’ package for using the above packahes in R. 
 #The reticulate package provides an R interface to Python modules, classes, and functions
 #You can install Rtools for your R version here — 
#https://cran.r-project.org/bin/windows/Rtools/
 #installing 'devtools' package for installing Packages from github
 install.packages('devtools')
 #installing keras
 devtools::install_github("rstudio/keras") 

 #Loading the keras package
 library(keras)
 #The R interface to Keras uses TensorFlow as it’s underlying #computation engine.So we need to install Tensorflow engine
 install_tensorflow()
 install_tensorflow(gpu = T)
 #loading keras library
library(keras)
 #loading the keras inbuilt cifar10 dataset
 ?dataset_cifar10 #to see the help file for details of dataset
 cifar<-dataset_cifar10()
 #TRAINING DATA
train_x<-cifar$train$x/255
 #convert a vector class to binary class matrix
#converting the target variable to once hot encoded vectors using #keras inbuilt function 'to_categorical()
 train_y<-to_categorical(cifar$train$y,num_classes = 10)
 #TEST DATA
test_x<-cifar$test$x/255
 test_y<-to_categorical(cifar$test$y,num_classes=10) 
 #checking the dimentions
dim(train_x) 
 cat("No of training samples\t",dim(train_x)[[1]],"\tNo of test samples\t",dim(test_x)[[1]])

 #a linear stack of layers
model<-keras_model_sequential()
 #configuring the Model
 model %>%  
#defining a 2-D convolution layer
  
layer_conv_2d(filter=32,kernel_size=c(3,3),padding="same",                input_shape=c(32,32,3) ) %>%  
layer_activation("relu") %>%  
 #another 2-D convolution layer
  
layer_conv_2d(filter=32 ,kernel_size=c(3,3))  %>%  layer_activation("relu") %>%
 #Defining a Pooling layer which reduces the dimentions of the #features map and reduces the computational complexity of the model
layer_max_pooling_2d(pool_size=c(2,2)) %>%  
 #dropout layer to avoid overfitting
 layer_dropout(0.25) %>%
 layer_conv_2d(filter=32 , kernel_size=c(3,3),padding="same") %>% layer_activation("relu") %>%  layer_conv_2d(filter=32,kernel_size=c(3,3) ) %>%  layer_activation("relu") %>%  
layer_max_pooling_2d(pool_size=c(2,2)) %>%  
layer_dropout(0.25) %>%
 #flatten the input  
layer_flatten() %>%  
 layer_dense(512) %>%  
layer_activation("relu") %>%  
 layer_dropout(0.5) %>%  
 #output layer-10 classes-10 units  
layer_dense(10) %>%  
 #applying softmax nonlinear activation function to the output layer #to calculate cross-entropy  
 layer_activation("softmax") 
 #for computing Probabilities of classes-"logit(log probabilities)
 #Model's Optimizer
 #defining the type of optimizer-ADAM-Adaptive Momentum Estimation
 opt<-optimizer_adam( lr= 0.0001 , decay = 1e-6 )
 #lr-learning rate , decay - learning rate decay over each update
 
model %>%
 compile(loss="categorical_crossentropy",
 optimizer=opt,metrics = "accuracy")
 #Summary of the Model and its Architecture
summary(model)
 #TRAINING PROCESS OF THE MODEL
 data_augmentation <- TRUE  
 if(!data_augmentation) {  
model %>% fit( train_x,train_y ,batch_size=32,
               epochs=80,validation_data = list(test_x, test_y),
               shuffle=TRUE)
}
  else {  
 #Generating images
  
gen_images <- image_data_generator(featurewise_center = TRUE,
      featurewise_std_normalization = TRUE,
      rotation_range = 20,
      width_shift_range = 0.30,
      height_shift_range = 0.30,
      horizontal_flip = TRUE  )
 #Fit image data generator internal statistics to some sample data
gen_images %>% fit_image_data_generator(train_x)
#Generates batches of augmented/normalized data from image data and #labels to visually see the generated images by the Model
 model %>% fit_generator(
     flow_images_from_data(train_x, train_y,gen_images,
     batch_size=32,save_to_dir="F:/PROJECTS/CNNcifarimages/"),
     steps_per_epoch=as.integer(50000/32),epochs = 80,
     validation_data = list(test_x, test_y) )
 }
 #use save_to_dir argument to specify the directory to save the #images generated by the Model and to visually check the Model's #output and ability to classify images.

 
 
Item 893: 
def get_model():
    Input_1 = Input(shape=(256, 512, 512, 1))
    MaxPooling3D_27 = MaxPooling3D(pool_size= (1,3,3))(Input_1)
    Convolution3D_1 = Convolution3D(kernel_dim1= 4,nb_filter= 10,activation= 'relu' ,kernel_dim3= 4,kernel_dim2= 4)(MaxPooling3D_27)
    Convolution3D_7 = Convolution3D(kernel_dim1= 4,nb_filter= 10,activation= 'relu' ,kernel_dim3= 4,kernel_dim2= 4)(Convolution3D_1)
    BatchNormalization_28 = BatchNormalization()(Convolution3D_7)
    MaxPooling3D_12 = MaxPooling3D(pool_size= (2,2,2))(BatchNormalization_28)
    SpatialDropout3D_1 = SpatialDropout3D(p= 0.5)(MaxPooling3D_12)
    Convolution3D_9 = Convolution3D(kernel_dim1= 2,nb_filter= 20,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(SpatialDropout3D_1)
    Convolution3D_11 = Convolution3D(kernel_dim1= 2,nb_filter= 20,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(Convolution3D_9)
    BatchNormalization_9 = BatchNormalization()(Convolution3D_11)
    MaxPooling3D_14 = MaxPooling3D(pool_size= (2,2,2))(BatchNormalization_9)
    SpatialDropout3D_4 = SpatialDropout3D(p= 0.5)(MaxPooling3D_14)
    Convolution3D_12 = Convolution3D(kernel_dim1= 2,nb_filter= 40,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(SpatialDropout3D_4)
    Convolution3D_13 = Convolution3D(kernel_dim1= 2,nb_filter= 40,activation= 'relu' ,kernel_dim3= 2,kernel_dim2= 2)(Convolution3D_12)
    MaxPooling3D_23 = MaxPooling3D(pool_size= (2,2,2))(Convolution3D_13)
    BatchNormalization_23 = BatchNormalization()(MaxPooling3D_23)
    SpatialDropout3D_5 = SpatialDropout3D(p= 0.5)(BatchNormalization_23)
    GlobalMaxPooling3D_1 = GlobalMaxPooling3D()(SpatialDropout3D_5)
    Dense_1 = Dense(activation= 'relu' ,output_dim= 10)(GlobalMaxPooling3D_1)
    Dropout_14 = Dropout(p= 0.3)(Dense_1)
    Dense_6 = Dense(activation= 'relu' ,output_dim= 10)(Dropout_14)
    Dense_2 = Dense(activation= 'softmax' ,output_dim= 2)(Dense_6)

    return Model([Input_1],[Dense_2])
 
 
Item 895: 
Actual sum:  [2000. 2012. 2024. 2036. 2048. 2060. 2072. 2084. 2096. 2108.]

Predicted sum:  [1999.9021 2011.9015 2023.9009 2035.9004 2047.8997 2059.8992 2071.8984
 2083.898  2095.8975 2107.8967]
 Actual product:  [1000000. 1012032. 1024128. 1036288. 1048512. 1060800. 1073152. 1085568.
 1098048. 1110592.]

Predicted product:  [1000000.2  1012032.   1024127.56 1036288.6  1048512.06 1060800.8
 1073151.6  1085567.6  1098047.6  1110592.8 ]
 
 
Item 898: 
<script src=”https://code.jquery.com/jquery-3.2.1.js" </script>.
 
 
Item 900: 
W/System.err: Caused by: com.google.firebase.ml.common.FirebaseMLException: Internal error has occurred when executing Firebase ML tasks
    Caused by: java.lang.IllegalArgumentException: Cannot create interpreter: Didn't find custom op for name 'Normalize'
07-23 18:01:16.018 21726-21726/io.github.the_dagger.mlkit W/System.err: Didn't find custom op for name 'ExtractFeatures'
    Didn't find custom op for name 'Predict'
    Registration failed.
 ./configure
 ./sdkmanager "platforms;android-27"
./sdkmanager "build-tools;27.0.2"
 bazel-genfiles/tensorflow/contrib/lite/java/
 implementation project(':tensorflow-lite')
 
 
Item 902: 
#rank test documents according to entropy of their class label
top_K = 20
X_test_class_logp = nb_clf.predict_log_proba(X_test_vec)
X_test_class_entropy = -np.sum(np.exp(X_test_class_logp) * X_test_class_logp, axis=1)
planning_entropy_idx = np.argsort(X_test_class_entropy)[-top_K:]
 #rank test documents according to mutual information (MI) between class labels and theta
log_pi = nb_clf.class_log_prior_              #log pi(c) global
log_py = nb_clf.predict_log_proba(X_test_vec) #log py(c) per doc
log_pxy = nb_clf.feature_log_prob_            #log pxy(c,j) = log p(x_ij = 1 | y = c)

[C,D] = log_pxy.shape
log_pi_tile = np.tile(log_pi.reshape(-1,1), log_pxy.shape[1])
#log px(j) = log [\sum_c p(x_ij = 1 | y = c) x p(y = c)]
#          = log [\sum_c exp{log_pxy(c,j) + log_pi(c)}]
log_px = logsumexp(log_pxy + log_pi_tile, axis=0)

#MI between x_j and pi
X_test_word_MI = np.zeros(D)
for class_idx in range(C):
    X_test_word_MI = X_test_word_MI + \
    np.exp(log_pi[class_idx]) * np.exp(log_pxy[class_idx,:]) * np.log(np.exp(log_pxy[class_idx,:])/np.exp(log_px)) + \
    np.exp(log_pi[class_idx]) * (1-np.exp(log_pxy[class_idx,:])) * np.log((1-np.exp(log_pxy[class_idx,:]))/(1-np.exp(log_px)))    
#end for

#MI between y_j and theta
logp_theta = logsumexp(log_pxy + log_pi_tile, axis=0)
X_test_docs_MI = np.zeros(X_test_vec.shape[0])
for doc_idx in range(X_test_vec.shape[0]):
    doc_MI = 0
    for class_idx in range(C):
        doc_MI += np.sum(np.exp(log_pxy[class_idx,:] +    
                  log_py[doc_idx, class_idx]) * \
                  (log_pxy[class_idx,:] - logp_theta), axis=-1)
    #end for
    X_test_docs_MI[doc_idx] = doc_MI
#end for

planning_MI_idx = np.argsort(X_test_docs_MI)[-top_K:]
 
 
Item 903: 
like mission impossible but hate tom cruise get that straight update day in a row like magic and shit
 
 
Item 904: 
learning_rate = [max_feature_value * 0.1, max_feature_value * 0.01, max_feature_value * 0.001,]
 for lrate in learning_rate:
     Optimization code(lrate)
 
 
Item 905: 
docker run -p 8080:8080 -e “MB_KEY=$MB_KEY” machinebox/facebox
 
 
Item 907: 
yarn add brain.js
 const brain = require('brain.js')
 ...
iterations: 530, training error: 0.006119754857500322
iterations: 540, training error: 0.005870729613810573
iterations: 550, training error: 0.005638886768098001
iterations: 560, training error: 0.005422631733361282
iterations: 570, training error: 0.0052205261001664685
iterations: 580, training error: 0.005031314834044752
result { white: 0.059708621352910995, black: 0.9404082298278809 }
 const result = brain.likely({ r: 0, g: 1, b: 0.65 }, network);
console.log(result);
 
 
Item 908: 
A simple linear equation is y = wx + b
 y = y-axis, x = x-axis, w = slope, b = y-intercept
 y = prediction, b= bias, w = weight of feature, x = feature
 
 
Item 909: 
claim ID, claim total amount, transaction amount, timestamp
 
 
Item 910: 
MATCH (a:SHOP {name:"Walmart}) RETURN a.opening_time
 
 
Item 913: 
precision    recall  f1-score   support

          0       0.66      0.69      0.67       191
          1       0.69      0.67      0.68       202

avg / total       0.68      0.68      0.68       393
 
 
Item 914: 
variants = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
payouts = [0.023, 0.03, 0.029, 0.001, 0.05, 0.06, 0.0234, 0.035, 0.01, 0.11]
 en0 = Environment(machines, payouts, n_trials=10000)
rs = RandomSampler(env=en0)
en0.run(agent=rs)
 - randomly choose k for n trials
 - On each trial estimate the payout rate for each variant
 - after n learning trials:
 - select 1-e% of the time k with the the highest payout rate and;
 - e% of the time sample from the variants randomly
 - Set a uniform prior distribution between 0 and 1 for each variant k’s payout rate
 - Draw a parameter theta from the each k's posterior distribution
 - Select the variant k that is associated with the highest parameter theta
 - Observe the reward and update the distribution parameters
 
 
Item 915: 
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 
                0 < memory_for_TensorFlow < 1)
 trt_graph = trt.create_inference_graph(
    input_graph_def = frozen_graph_def,
    outputs = output_node_name,
    max_batch_size=batch_size,
    max_workspace_size_bytes=workspace_size,
    precision_mode=precision,
    minimum_segment_size=3)
 trt_graph = trt.create_inference_graph(
                getNetwork(network_file_name), 
                outputs,
                max_batch_size=batch_size, 
                max_workspace_size_bytes=workspace_size, 
                precision_mode=”FP16")
 trt_graph = trt.create_inference_graph(
                getNetwork(network_file_name), 
                outputs,
                max_batch_size=batch_size,
                max_workspace_size_bytes=workspace_size, 
                precision_mode=”INT8")
 trt_graph=trt.calib_graph_to_infer_graph(calibGraph)
 
 
Item 916: 
python -m pysc2.bin.agent \
--map Simple64 \
--agent smart_agent.SmartAgent \
--agent_race T \
--max_agent_steps 0 \
--norender
 
 
Item 917: 
<!-- Original image without overlay -->
 <img src='https://s3.amazonaws.com/mturk-solutions/tutorials/images/monkey.jpg'/>
 <!-- Dynamically render a new version with a bounding box -->
 <img src='https://jmwym8qv12.execute-api.us-east-1.amazonaws.com/Public?img=https%3A%2F%2Fs3.amazonaws.com%2Fmturk-solutions%2Ftutorials%2Fimages%2Fmonkey.jpg&left=375&top=300&width=225&height=200'/>
 <!-- Original image without overlay -->
 <img src='https://s3.amazonaws.com/mturk-solutions/tutorials/images/machines.jpg'/>
 <!-- Dynamically render a new version with two bounding boxes -->
 <img src='https://jmwym8qv12.execute-api.us-east-1.amazonaws.com/Public?img=https%3A%2F%2Fs3.amazonaws.com%2Fmturk-solutions%2Ftutorials%2Fimages%2Fmachines.jpg&left=140,700&top=210,210&width=300,310&height=530,550'/>
 
 
Item 918: 
kernel = tf.Variable(tf.truncated_normal([k, k, chIn, chOut], stddev=0.1))
 conv = tf.nn.conv2d(inputTensor, kernel, padding=’SAME’, strides=(1, 1, 1, 1))
 relu = tf.nn.relu(conv)
 pool = tf.nn.max_pool(relu, (1, px, py, 1), (1, sx, sy, 1), ‘VALID’)
 cells = [tf.contrib.rnn.LSTMCell(num_units=256, state_is_tuple=True) for _ in range(2)]
 stacked = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)
 ((fw, bw),_) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=inputTensor, dtype=inputTensor.dtype)
 gtTexts = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]), tf.placeholder(tf.int32, [None]), tf.placeholder(tf.int64, [2]))
 seqLen = tf.placeholder(tf.int32, [None])
 loss = tf.nn.ctc_loss(labels=gtTexts, inputs=inputTensor, sequence_length=seqLen, ctc_merge_repeated=True)
 decoder = tf.nn.ctc_greedy_decoder(inputs=inputTensor, sequence_length=seqLen)
 optimizer = tf.train.RMSPropOptimizer(0.001).minimize(loss)
 
 
Item 919: 
git clone https://github.com/dcshapiro/smallCompanyType.git
cd smallCompanyType/
pip3 install h5py
pip3 install keras --upgrade
pip3 install tensorflow --upgrade
python3 setup.py install
 cd test
python3 test_text.py
python3 test_pandas.py
 
 
Item 923: 
MATCH (you {name:"You"})
MATCH (expert)-[:WORKED_WITH]->(db:Database {name:"Neo4j"})
MATCH path = shortestPath( (you)-[:FRIEND*..5]-(expert) )
RETURN db,expert,path
 
 
Item 926: 
python tensormouse.py
 
 
Item 927: 
docker pull machinebox/tagbox:latest
docker pull machinebox/facebox:latest
docker pull machinebox/nudebox:latest
docker pull machinebox/videobox:latest
 version: '3'
services:
  facebox1:
    image: machinebox/facebox
    environment:
      - MB_KEY=${MB_KEY}
    ports:
      - "8081:8080"
  tagbox1:
    image: machinebox/tagbox
    environment:
      - MB_KEY=${MB_KEY}
    ports:
      - "8082:8080"
  nudebox1:
    image: machinebox/nudebox
    environment:
      - MB_KEY=${MB_KEY}
    ports:
      - "8083:8080"
  videobox:
    image: machinebox/videobox
    environment:
      - MB_KEY=${MB_KEY}
      - MB_VIDEOBOX_FACEBOX_ADDR=http://facebox1:8080
      - MB_VIDEOBOX_TAGBOX_ADDR=http://tagbox1:8080
      - MB_VIDEOBOX_NUDEBOX_ADDR=http://nudebox1:8080
    ports:
      - "8080:8080"
 docker-compose up
 
 
Item 929: 
[(0,
  [('game', 0.019290676),
   ('team', 0.014655967),
   ('player', 0.013197457),
   ('sport', 0.0096272184),
   ('play', 0.0086391047)]),
 (1,
  [('child', 0.071610503),
   ('school', 0.039327879),
   ('parent', 0.023915363),
   ('family', 0.020240301),
   ('student', 0.01847663)]),
 (2,
  [('life', 0.011472762),
   ('people', 0.011282165),
   ('woman', 0.0072893538),
   ('family', 0.0068212799),
   ('home', 0.0065922942)]),
 (3,
  [('art', 0.0068352227),
   ('book', 0.0065797614),
   ('film', 0.0050195237),
   ('street', 0.0049045617),
   ('life', 0.0043060104)]),
 (4,
  [('state', 0.024362968),
   ('president', 0.016713507),
   ('governor', 0.013491742),
   ('republican', 0.012855075),
   ('budget', 0.011843665)]),
 (5,
  [('court', 0.027797233),
   ('state', 0.021660788),
   ('case', 0.020019565),
   ('judge', 0.016594727),
   ('lawyer', 0.015601465)]),
 (6,
  [('woman', 0.033056237),
   ('gun', 0.028255416),
   ('law', 0.018436665),
   ('abortion', 0.014594568),
   ('violence', 0.014348775)]),
 (7,
  [('study', 0.01611951),
   ('disease', 0.012828876),
   ('brain', 0.012020445),
   ('health', 0.0082923695),
   ('percent', 0.0081615159)]),
 (8,
  [('patient', 0.038924955),
   ('mental', 0.020712202),
   ('treatment', 0.020659719),
   ('drug', 0.017774586),
   ('doctor', 0.01549102)]),
 (9,
  [('city', 0.025483603),
   ('state', 0.018783275),
   ('hospital', 0.016500311),
   ('people', 0.013982687),
   ('service', 0.012257091)]),
 (10,
  [('police', 0.031944588),
   ('officer', 0.017374802),
   ('man', 0.012399676),
   ('shot', 0.0080928812),
   ('death', 0.0073555997)]),
 (11,
  [('people', 0.011733172),
   ('life', 0.0056124884),
   ('question', 0.0054705907),
   ('work', 0.0054668467),
   ('social', 0.0050076959)]),
 (12,
  [('war', 0.015990684),
   ('veteran', 0.013699779),
   ('military', 0.011781215),
   ('government', 0.0091844629),
   ('country', 0.0084220674)]),
 (13,
  [('health', 0.024999907),
   ('care', 0.019101448),
   ('company', 0.015718631),
   ('state', 0.011817242),
   ('percent', 0.011760511)])]
 
 
Item 937: 
y = mX+b { m->slope , b->Y-intercept }
 y1=m*x1+b {m,b->random values lets say 0.5,1 
          x1->lets say -3 (first value from our data-set)
 y1=(0.5 * -3) + 1
y1=-0.5 
by applying all x values for m and b values we get our first line.
 Above picture has its own random variables ( I hope you understand the concept)
 X        y
1000      40
2000      70
500       25
............
 x1   x2   y
1000  2    50
2000  4    90
500   1    35
............
 
 
Item 940: 
NB: For simplicity's sake a risk free rate of 0% was assumed for this post
 
 
Item 955: 
{
  'messageVersion':'1.0',
  'invocationSource':'FulfillmentCodeHook',
  'userId':'zbf4r3orv6z2df6ryhcay9ulu97pd4uj',
  'sessionAttributes':{},
  'requestAttributes':None,
  'bot':{
    'name':'CryptoBot',
    'alias':'$LATEST',
    'version':'$LATEST'
  },
  'outputDialogMode':'Text',
  'currentIntent':{
    'name':'FindBitcoinPrice',
    'slots':{
      'Date':'2019-02-01'
    },
    'slotDetails':{
      'Date':{
        'resolutions':[],
        'originalValue':'first feb'
      }
    },
    'confirmationStatus':'None'
  },
  'inputTranscript':"what's the price of bitcoin on first feb?"
}
 
 
Item 959: 
error_formula: = (target_outcome - model_prediction)
 insert_values: = (0 — 0.049429)
 answer: = (0.49429)
 - The slope of the MSE (y-axis) with respect to the weight (x-axis)
 - The partial derivative of the MSE with respect to weight 1
 - The partial change of the MSE with respect to weight 1
 - The MSE gradient with respect to weight 1
 - Rise over run (a change in starting and ending point on y-axis divided by a change in starting and ending point on x-axis)
 This is also called:
 * The ‘gradient’
 * The 'slope' (MSE is on the y-axis and the weight is on the x-axis)
 It is a parameter that we control and it controls how fast our model learns.
 Intuitively, you may be thinking…”well, I want to learn fast so I want a high learning rate”… as we’ll see below though, if we make this rate too high, our error ("the loss") will either bounce around or it won't be minimized. A good learning rate will minimize our error at decent pace.  In practice, this is usually a trial & error process and common rates that are typically tested in factors of ten (0.10, 0.01, 0.001, etc.).
 new_weight = current_weight — (learning_rate x change_in_weight)
 1. change_in_weight = 1.0 (also called the 'gradient' or 'slope')
 2. learning_rate = 0.1
 3. current_weight = 0.25
 new_weight = 0.25 - (0.1 x 1.0)
new_weight = 0.25 - 0.1
new_weight = 0.15
 1. change_in_weight = 0.00294 (the 'gradient')
2. learning_rate = 0.5
3. current_weight = (0.4)
 new_weight = current_weight — (learning_rate x change_in_weight)
 new_weight = (0.4) - (0.5 x 0.00294)
new_weight = (0.4) - 0.00147
new_weight = (0.40147)
 
 
Item 960: 
Song lyrics                        | Shakespeare translation                       
 I do not like your little games.   | i do not like your little dry .
Do not like your tilted stage.     | do not like thy mind .                          
The role you made me play.         | the offence you made me .     
Of the fool, no, I do not like you.| no , no , i am not like you .                   I don't like your perfect crime.   | i do so much as thou art .                      
How you laugh when you lie.        | how thou dost lie .                 
You said the gun was mine.         | you say the seas was mine .                     
Isn't cool, no, I don't like you.  | no cool , no , i protest you .
Oh, look what you made me do       | o , look you , look you , what                              >                                    you make me do ?
Look what you made me do           | what thou hast made me do ?  
 
 
Item 961: 
   X
x1 x2 x3
1  1  0 --> just like our previous ml data and we can do what ever
1  1  1     we wanna do.
 
 
Item 966: 
>>> df.dtypes
 user_id                object
order_id               object
order_date             object
number_of_products      int64
total_amount          float64
day_of_week            object
dtype: object
 >>> df.memory_usage(deep=True) / 1024 ** 2
 Index                  0.000076
user_id               61.988831
order_id              61.035156
order_date            38.146973
number_of_products     7.629395
total_amount           7.629395
day_of_week           61.171283
dtype: float64
 def memory_usage(df):
    return(round(df.memory_usage(deep=True).sum() / 1024 ** 2, 2))
 >>> print('Memory used:', memory_usage(df), 'Mb')
 Memory used: 237.6 Mb
 >>> df.describe()
 >>> df.number_of_products = df.number_of_products.astype('uint8')
>>> df.total_amount = df.total_amount.apply(pd.to_numeric, downcast='float')
>>> print('Memory used:', memory_usage(df_copy), 'Mb')
 Memory used: 230.93 Mb
 >>> type(df.total_amount[0])
 numpy.float64
 >>> df.total_amount = df.total_amount.astype('float16')
>>> print('Memory used:', memory_usage(df), 'Mb')
 Memory used: 225.21 Mb
 >>> df.day_of_week = df.day_of_week.astype('category')
>>> df.user_id = df.user_id.astype('uint32')
>>> df.order_id = df.order_id.astype('uint32')
 >>> print('Memory used:', memory_usage(df_copy), 'Mb')
 Memory used: 49.59 Mb
 >>> df_copy.memory_usage(deep=True) / 1024 ** 2
 Index                  0.000076
user_id                3.814697
order_id               3.814697
order_date            38.146973
number_of_products     0.953674
total_amount           1.907349
day_of_week            0.954408
dtype: float64
 column_dtypes = {'day_of_week': 'category',
                 'user_id': 'uint32',
                 'order_id': 'uint32',
                 'total_amount': 'float16',
                 'number_of_products': 'uint8'}
 df = pd.read_csv('sample_dataframe.csv', ,dtype = column_dtypes)
 
 
Item 968: 
John's favourite programming languages are Haskell and x86 Assembley. Let A represent the event that he forces a class to learn Haskell and B represent the event that he forces a class to learn x86 Assembley.
 On a randomly selected day, John is taken over by Satan himself, so the probability of P(A) is 0.6 and the probability of P(B) is 0.4 and the conditional probability that he teaches Haskell, given that he has taught x86 Assembley that day is P(A|B) = 0.7.
 Based on the information, what is P(B|A), the conditional probability that John teaches x86 Assembley given that he taught Haskell, rounded to the nearest hundredth?
 P(A and B) = P(A|B) * P(B) = 0.7 * 0.5 = 0.35
 Solving it
P(B|A) * P(A)
P(A) = 0.5
So
0.6 * P(B|A)
Now we don't know what P(B|A) is, but we want to find out. We know that P(B|A) must be a part of P(A and B) because P(A and B) is the probability that both of these events happen so...
 P(A and B) = 0.35
 0.35 = P(B|A) * 0.5
With simple algebraic manipulation
0.35/0.5 = P(B|A)
P(B|A) = 0.7
 Suppose a new drug is found on the streets and the police want to identify whether someone is a user or not.
 The drug is 99% sensitive, that is that the proportion of people who are correctly identified as taking the drug.
 The drug is 99% specific, that is that the proportion of people who are correctly identified as not taking the drug.
 Note: there is a 1% false positive rate for both users and non users.
 Suppose that 0.5% of people at John Moores (A rival university) takes the drug. What is the probability that a randomly selected John Moores student with a positive test is a user?
 Now we have P(Sunny|Yes) = 3/9 = 0.33.
P(Sunny) = 5/14 = 0.36. 
P(yes) = 9/14 = 0.64
 Now using Bayes Therom we can work out
P(Yes|Sunny) = 0.33 * 0.674 / 0.36 = 0.60 which has high probability.
 Thus, given that it is sunny, there is a 60% chance they will play.
 Viagra for sale, get sexy body soon!
U want sum viagra bby? u sexy s00n
VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA
SEXY SOON VIAGRA PLEASE YASSSSS 
VIAGRA BUY PLEASE
I really like today
 Hello, how are you?
I hope you have the work ready for January the 8th.
Do you like cats or dogs?
Viagra
viagra
 ['Viagra for sale, get sexy body soon!', 'U want sum viagra bby? u sexy s00n', 'VIAGRA VIAGRA VIAGRA VIAGRA VIAGRA', 'SEXY SOON VIAGRA PLEASE YASSSSS ', 'VIAGRA BUY PLEASE', 'I really like today']
 ['Hello, how are you?', 'I hope you have the work ready for January the 8th.', 'Do you like cats or dogs?', 'Viagra', 'viagra']
 {'body': 1.0, '': 1.0, 'want': 1.0, 'get': 1.0, 'I': 0.5, 'January': 0.0, 's00n': 1.0, 'SOON': 1.0, 'you?': 0.0, 'dogs?': 0.0, 'sale,': 1.0, 'U': 1.0, 'are': 0.0, 'Viagra': 0.5, 'sexy': 1.0, 'ready': 0.0, 'SEXY': 1.0, 'the': 0.0, '8th.': 0.0, 'really': 1.0, 'Do': 0.0, 'Hello,': 0.0, 'BUY': 1.0, 'bby?': 1.0, 'like': 0.5, 'for': 0.5, 'sum': 1.0, 'work': 0.0, 'PLEASE': 1.0, 'VIAGRA': 1.0, 'YASSSSS': 1.0, 'how': 0.0, 'viagra': 0.5, 'cats': 0.0, 'u': 1.0, 'hope': 0.0, 'have': 0.0, 'you': 0.0, 'or': 0.0, 'today': 1.0, 'soon!': 1.0}
 W1, W2, ..., Wn 
 ('Hello', 'Goodbye', 'Email', 'Cheese', 'United Kingdom')
 ('Hello', 'Cheese')
(1, 0, 0, 1, 0)
 wget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data
 6.5, 3.2, 5.1, 2.0, Iris-virginica
6.4, 2.7, 5.3, 1.9, Iris-virginica
6.8, 3.0, 5.5, 2.1, Iris-virginica
5.7, 2.5, 5.0, 2.0, Iris-virginica
5.8, 2.8, 5.1, 2.4, Iris-virginica
6.4, 3.2, 5.3, 2.3, Iris-virginica
 5.0,3.4,1.5,0.2,Iris-setosa
 5.0,3.4,1.5,0.2
 Lambda var1, var2: calculation
 map ( function, list)
 [4, 4, 4]
 [1, 2, 3]
 [1, 2, 3]
x = 1, y = 2
1 + 2 = 3
x = 3, y = 3
x + y = 6
 [3.4641016151377544, 0.0]
 trainSet = [[2, 2, 2, 'a'], [4, 4, 4, 'b']]
 testInstance = [4, 4, 4]
 k = 1
 [([2, 2, 2, ‘a’], 3.4641016151377544), ([4, 4, 4, ‘b’], 0.0)]
 [[4, 4, 4, 'b']]
 
 
Item 970: 
colSums(is.na(train_users_2))
 colSums(is.na(train_users_2))
 train_users_2$gender <- factor(train_users_2$gender)
 
 
Item 971: 
camera.addFrameProcessor { frame -> 
    faceDetector.detectFaces(frame) 
}
 
 
Item 973: 
alias python=python3
 tensorflow-for-poets-2 > tf_files > flower_photos
 
 
Item 981: 
Name
 Aashirvaad Shudh Chakki Whole Wheat Atta
Fortune Soya Health Refined Soybean Oil (Pouch)
Best Value Refined Sugar
Onion
Potato 
Lemon
Tata Salt
Best Value Arhar Dal/Toor Dal
Lady Finger
Coriander
 Name                                                   Reordered %
 Nestle a+ Nourish Toned Milk (Tetra Pak)                       46%
 Mother Dairy Toned Milk (Tetra Pak                             41%
 Aashirvaad Shudh Chakki Whole Wheat Atta                       41%
 Mother Dairy Toned Milk (Polypack)                             40%
 Mother Dairy Full Cream Milk (Polypack)                        39%
 Nestle a+ Slim Skimmed Milk (Tetra Pak)                        38%
 
 
Item 985: 
Output:
[-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6]
[41 30 21 14  9  6  5  6  9 14 21 30 41]
 Output:
2*x
 Output:
iterations = 5000 
x =  0.00026968555624761565
 
 
Item 986: 
let body = {
  "requests": [
    {
      "image": {
        "source": {
          "imageUri": "https://i.imgur.com/Nlot5mR.jpg" //image URL
        }
      },
      "features": [
        {
          "type": "TEXT_DETECTION",
          "maxResults": 1
        }
      ]
    }
  ]
}
 axios.post('https://vision.googleapis.com/v1/images:annotate?key=YOUR_API_KEY_HERE', body)
  .then((response) => console.log(response));
 {
  "responses": [
    {
      "textAnnotations": [
        {
          "locale": "en",
          "description": "I HAVE NO IDEA\nWHAT TM DOING\n",
          "boundingPoly": {
          "vertices": [
            {
              "x": 11,
              "y": 2
            },
            {
              "x": 257,
              "y": 2
            },
            {
              "x": 257,
              "y": 102
            },
            {
              "x": 11,
              "y": 102
            }
          ]
        }
      },...
 
 
Item 987: 
error LNK2038: mismatch detected for 'RuntimeLibrary': value 'MD_DynamicRelease' doesn't match value 'MT_StaticRelease' in file.obj
 
 
Item 988: 
dqn_solver.remember(state, action, reward, state_next, terminal)
 dqn_solver.experience_replay()
 
 
Item 989: 
library(AER)
fit <- ivreg(Y ~ X, Z, data = df)
summary(fit, vcov = sandwich, df = Inf, diagnostics = TRUE)
 library(glmnet)
lasso.fit <- cv.glmnet(M.excl, df.excl$X, alpha=1)
coef <- predict(lasso.fit, type = “nonzero”)
M.excl.names <- colnames(M.excl)
Z <- M.excl.names[unlist(coef)]
 Z.list <- paste(“~ “, paste(Z, collapse = “ + “))
fit.lasso <- ivreg(Y ~ X, Z.list, data = df.excl)
summary(fit.lasso, vcov = sandwich, df = Inf, diagnostics = TRUE)
 firststage <- paste(“X ~ “, paste(Z, collapse = “ + “))
fit.firststage <- lm(firststage, data = df.excl)
summary(fit.firststage)
 
 
Item 990: 
Copyright was the succession of independence in the slop of Syrian influence that was a famous German movement based on a more popular servicious, non-doctrinal and sexual power post.
 
 
Item 992: 
Men are men, and men should clean the kitchen
 Männer sind Männer, und Frauen sollten die Küche sauber
 
 
Item 993: 
f(u, v) = [ (a + b*cos(v))*cos(u), (a + b*cos(v))*sin(u),
c*sin(v) ]
 
 
Item 995: 
           name handedness height weight    avg   HR
0  Brandon Hyde          R     75    210  0.000    0
1   Carey Selph          R     69    175  0.277    0
2  Philip Nastu          L     74    180  0.040    0
3    Kent Hrbek          L     76    200  0.282  293
4   Bill Risley          R     74    215  0.000    0
               name handedness height weight    avg  HR
0     Brandon Hyde          R     75    210  0.000   0
1      Carey Selph          R     69    175  0.277   0
4      Bill Risley          R     74    215  0.000   0
6  Steve Gajkowski          R     74    200  0.000   0
7        Rick Schu          R     72    170  0.246  41
                name handedness height weight    avg   HR
2      Philip Nastu          L     74    180  0.040    0
3        Kent Hrbek          L     76    200  0.282  293
9      Tom Browning          L     73    190  0.153    2
13        Tom Brown          L     70    168  0.265   64
15  Floyd Bannister          L     73    190  0.175    0
 Ttest_indResult(statistic=-9.935702226242094, pvalue=3.810274225888738e-23)
 There is a significant difference
 False
 ('\n========== Epoch', 0, '==========')
Cost:  16.004954
('\n========== Epoch', 10, '==========')
Cost:  7.52057727374
('\n========== Epoch', 20, '==========')
Cost:  3.71424722592
('\n========== Epoch', 30, '==========')
Cost:  2.0064263714
('\n========== Epoch', 40, '==========')
Cost:  1.23996949709
('\n========== Epoch', 50, '==========')
Cost:  0.895797093343
('\n========== Epoch', 60, '==========')
Cost:  0.741057191151
('\n========== Epoch', 70, '==========')
Cost:  0.671295578372
('\n========== Epoch', 80, '==========')
Cost:  0.639655519089
('\n========== Epoch', 90, '==========')
Cost:  0.625117716849
 
 
Item 997: 
export API_KEY=<YOUR_API_KEY>
 {
 "document":{
 "type":"PLAIN_TEXT",
 "content":"Joanne Rowling, who writes under the pen names J. K. Rowling and Robert Galbraith, is a British novelist and screenwriter who wrote the Harry Potter fantasy series."
 },
 "encodingType":"UTF8"
 }
 {
 "entities": [
 {
 "name": "Robert Galbraith",
 "type": "PERSON",
 "metadata": {
 "mid": "/m/042xh",
 "wikipedia_url": "https://en.wikipedia.org/wiki/J._K._Rowling"
 },
 "salience": 0.7980405,
 "mentions": [
 {
 "text": {
 "content": "Joanne Rowling",
 "beginOffset": 0
 },
 "type": "PROPER"
 },
 {
 "text": {
 "content": "Rowling",
 "beginOffset": 53
 },
 "type": "PROPER"
 },
 {
 "text": {
 "content": "novelist",
 "beginOffset": 96
 },
 "type": "COMMON"
 },
 {
 "text": {
 "content": "Robert Galbraith",
 "beginOffset": 65
 },
 "type": "PROPER"
 }
 ]
 }
 ]
 }
 {
 "entities": [
 {
 "name": "Robert Galbraith",
 "type": "PERSON",
 "metadata": {
 "mid": "/m/042xh",
 "wikipedia_url": "https://en.wikipedia.org/wiki/J._K._Rowling"
 },
 "salience": 0.7980405,
 "mentions": [
 {
 "text": {
 "content": "Joanne Rowling",
 "beginOffset": 0
 },
 "type": "PROPER"
 },
 {
 "text": {
 "content": "Rowling",
 "beginOffset": 53
 },
 "type": "PROPER"
 },
 {
 "text": {
 "content": "novelist",
 "beginOffset": 96
 },
 "type": "COMMON"
 },
 {
 "text": {
 "content": "Robert Galbraith",
 "beginOffset": 65
 },
 "type": "PROPER"
 }
 ]
 }
 ]
 }
 
 
Item 998: 
"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :."
 "this isnt the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robins new love of the thriller but this isnt a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until williams character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all its worth a watch though its definitely not friday saturday night fare it rates a   from the fiend"
 
 
Item 999: 
gen = ClusterGenerator(structure=’anisotropy’, randomize=45)
clf = ToMaTo(*gen.generate())
_ = clf.estimate_clusters(graph=True)
 gen = ClusterGenerator(structure='moons', randomize=45)
clf = ToMaTo(*gen.generate())
_ = clf.estimate_density(graph=False)
clf.estimate_clusters(graph=False)
prd = clf.fit_predict(num_clusters=2, graph=True)
 gen = ClusterGenerator(structure='anisotropy', randomize=45)
clf = ToMaTo(*gen.generate())
_ = clf.estimate_density(graph=False)
clf.estimate_clusters(graph=False)
prd = clf.fit_predict(num_clusters=3, graph=True)
 
 
Item 1003: 
dask version: 0.14.1
numba version: 0.34.0
numpy version: 1.13.1
 
 
Item 1008: 
~/.ipython/profile_nbserver/ipython_notebook_config.py 
 - develop
  - <date>-<name>-<use case>
    + analysis1.ipynb
    + analysis1.py
    + somefig.png
- deliver
  - <use case>
    + final_analysis.ipynb
    + final_analysis.py
    + fig1.png
 
 
Item 1011: 
compute centrality of station, in [station, tunnel], using degree;
 compute path from "V90192", to "V1191984", in [station, tunnel];
 
 
Item 1013: 
keyword_article = []
for k in keywords:
 k = re.sub(“\r\n”,””,k)
 if k in articlekeyword_article.append(k)
 keyword_overlap = []
for g in keyword_article:
 for h in keyword_article:
  if g != h:
   if h in g:
    keyword_overlap.append(h)
 wiki_terms = list(set(keyword_article)-set(keyword_overlap))
 term_no = []
term_sum = 0
wordcount = {}
tfidf = {} 
 for i in xrange(len(wiki_terms)):  
 term = articles.count(wiki_terms[i])  
 term_no.append(term)
 for i in term_no:  
 term_sum = term_sum + i
 for i in xrange(len(wiki_terms)): 
 tf = Decimal(term_no[i])/Decimal(term_sum)
 wordcount[wiki_terms[i]]=tf
 for k in wiki_terms:
 x2.execute(“select key_idf from key_cn where key_term = %s”,(k))
 idf = x2.fetchone()
 if idf:
  tfidf_value = float(wordcount[k])* idf[0]
 if tfidf_value > 0.1:
  tfidf[k] = tfidf_value
 #if the keywords appear in header, it is important. 
 if k in articles_header:
  tfidf[k] = 1
 
 
Item 1015: 
detection_threshold = 0.1
 [{objectX: confidence_scoreX},{objectY: confidence_scoreY}…]
 {dl_timestamp: YYYY-MM-DDTHH:MM:SS.msmsms, object_type, objectX, confidence: confidence_scoreX}
 
 
Item 1018: 
for epoch in range(number_of_epochs):
     for j in range(0, length_of_input_array, batch_size):
          batch_train_X = train_X[j:j+batch_size]
          batch_train_y = train_y[j:j+batch_size]
          ...
          m = m - learning_rate * m_gradient
          b = b - learning_rate * b_gradient
 for epoch in range(number_of_epochs):
     for j in range(0, length_of_input_array, batch_size):
          batch_train_X = train_X[j:j+batch_size]
          batch_train_y = train_y[j:j+batch_size]
           if epoch == 0:
               v_m = 0
               v_b = 0
           m_gradient = -(2/N) * sum(X[j:j+batch_size] * (y[j:j+batch_size] - y_current))
          b_gradient = -(2/N) * sum(y[j:j+batch_size] - y_current)
           v_m = mu * v_m + learning_rate * m_gradient
          v_b = mu * v_b + learning_rate * b_gradient
           m_current = m_current - v_m
          b_current = b_current - v_b
 for epoch in range(number_of_epochs):
     for j in range(0, length_of_input_array, batch_size):
          batch_train_X = train_X[j:j+batch_size]
          batch_train_y = train_y[j:j+batch_size]
           if epoch == 0:
               v_m = 0
               v_b = 0
           y_nesterov_m = (m_current - mu * v_m) * X[j:j+batch_size] + b_current
          y_nesterov_b = (b_current - mu * v_b) * X[j:j+batch_size] + b_current
           m_gradient = -(2/N) * sum(X[j:j+batch_size] * (y[j:j+batch_size] - y_nesterov_m))
          b_gradient = -(2/N) * sum(y[j:j+batch_size] - y_nesterov_b)
           v_m = mu * v_m + learning_rate * m_gradient
          v_b = mu * v_b + learning_rate * b_gradient
           m_current = m_current - v_m
          b_current = b_current - v_b
 
 
Item 1019: 
python tensorflow/python/tools/import_pb_to_tensorboard.py --model_dir tmp/tensorflow_inception_graph.pb --log_dir tmp/
 tensorboard --logdir=tmp/
 bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
 —-inputs=”input_1" —-in_graph=tmp/tensorflow_inception_graph.pb \
 —-outputs=”output_node0" —-out_graph=tmp/quantized_graph.pb \
 —-transforms=’add_default_attributes strip_unused_nodes(type=float, shape=”1,299,299,3") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights strip_unused_nodes sort_by_execution_order’
 const int wanted_input_width = 299;
 const int wanted_input_height = 299;
 const int wanted_input_channels = 3;
 const float input_mean = 0.0f;
 const float input_std = 255.0f;
 const std::string input_layer_name = "input_1";
 const std::string output_layer_name = "output_node0";
 
 
Item 1021: 
> Number of mislabeled points out of a total 357 points: 68, performance 80.95%
 > Survival prob = 39.50%, Not survival prob = 60.50%
 mean_fare_survived = 54.75
std_fare_survived = 66.91
mean_fare_not_survived = 24.61
std_fare_not_survived = 36.29
 Number of mislabeled points out of a total 357 points: 128, performance 64.15%
Std Fare not_survived 36.29
Std Fare survived: 66.91
Mean Fare not_survived 24.61
Mean Fare survived: 54.75
 
 
Item 1024: 
{
  "timestamp": 1502135820943,
  "method": "get",
  "query": {
    "query": "Lawn & Garden Buying Guides"
  },
  "path": "/search",
  "statusCode": 200,
  "source": {
    "remoteAddress": "22.73.58.187",
    "userAgent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36"
  },
  "route": "/search",
  "headers": {
    "host": "localhost:8002",
    "connection": "keep-alive",
    "cache-control": "no-cache",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36",
    "accept": "*/*",
    "accept-encoding": "gzip, deflate, br",
    "accept-language": "en-US,en;q=0.8,es;q=0.6"
  },
  "requestPayload": null,
  "responsePayload": "SEARCH"
}
 {" ": 39, "(": 77, ",": 7, "0": 8, "4": 26, "8": 28, "<": 64, "D": 71, "H": 59, "L": 61, ...
 0 = Normal request log entry
1 = Request log indicates attempted injection attack
 
 
Item 1026: 
CREATE INDEX ON :User(name);
CREATE INDEX ON :Tweet(text);
CREATE INDEX ON :Hashtag(name);
 UNWIND [
 'http://www.trumptwitterarchive.com/data/realdonaldtrump/2018.json',
'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2017.json',
'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2016.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2015.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2014.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2013.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2012.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2011.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2010.json',
  'http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2009.json'
] AS url
CALL apoc.load.json(url) YIELD value as t
MERGE (s:Source { name: t.source })
CREATE (tweet:Tweet {
    id_str: t.id_str,
    text: t.text,
    created_at: t.created_at,
    retweets: t.retweet_count,
    favorites: t.favorite_count,
    retweet: t.is_retweet,
    in_reply: coalesce(t.in_reply_to_user_id_str, '')
})
CREATE (tweet)-[:from]->(s)
RETURN count(t);
 /* Hashtag Analysis */
MATCH (t:Tweet) 
WHERE t.text =~ ".*#.*" 
WITH 
  t, 
  apoc.text.regexGroups(t.text, "(#\\w+)")[0] as hashtags 
UNWIND hashtags as hashtag
MERGE (h:Hashtag { name: toUpper(hashtag) })
MERGE (h)<-[:hashtag { used: hashtag }]-(t)
RETURN count(h);
 /* User Mention Analysis */
MATCH (t:Tweet) 
WHERE t.text =~ ".*@.*" 
WITH 
  t, 
  apoc.text.regexGroups(t.text, "(@\\w+)")[0] as mentions
UNWIND mentions as mention
MERGE (u:User { name: mention })
MERGE (u)<-[:mention]-(t)
RETURN count(u);
 /* Detect language and update each tweet with that information */
MATCH (t:Tweet)
CALL ga.nlp.detectLanguage(t.text)
YIELD result
SET t.language = result
RETURN count(t);
 /* Annotate all text that's detected as English, as the underlying library may not support things it detects as non-English */
MATCH (t:Tweet { language: "en" })
CALL ga.nlp.annotate({text: t.text, id: id(t)})
YIELD result
MERGE (t)-[:HAS_ANNOTATED_TEXT]->(result)
RETURN count(result);
 MATCH (n:Tag)
CALL ga.nlp.enrich.concept({tag: n, depth:2, admittedRelationships:["IsA","PartOf"]})
YIELD result
RETURN count(result);
 MATCH (:NER_Person {value: 'clintons'})-[]-(s:Sentence)-[]-(:AnnotatedText)-[]-(tweet:Tweet) 
RETURN distinct tweet.text;
 MATCH (t:Tweet)-[]-(a:AnnotatedText) 
CALL ga.nlp.sentiment(a) YIELD result 
RETURN result;
 MATCH (:NER_Person {value: 'clintons'})-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet) 
RETURN distinct tweet.text;
 MATCH (tag:NER_Person)-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet) 
WHERE 
   NOT tag:NER_Organization AND 
   NOT tag:NER_O
RETURN distinct tag.value, count(tweet) as negativeTweets ORDER BY negativeTweets DESC;
 MATCH (t:NER_Organization)-[]-(s:Sentence)
WHERE 
  'Negative' in labels(s) OR 
  'Positive' in labels(s) 
  AND length(labels(t)) = 2 
RETURN distinct t.value as orgName, 
       s.text as sentence, 
       labels(s) as sentiment;
 
 
Item 1030: 
read messages -> accumulate to file -> upload -> commit offsets
 #before
partitioner.granularity.hour=true
 partitioner.granularity.minute=false
 partitioner.granularity.date.prefix=dt=
 partitioner.granularity.hour.prefix=hr=
 partitioner.granularity.date.format=yyyy-MM-dd
 partitioner.granularity.hour.format=HH
 #now
PARTITION_FORMAT='dt='YYYY-MM-dd'/hr='HH
 TOPIC_SCHEMA_MAPPING=audit-log:com.gojek.AuditLog:edit_time|booking-log:com.gojek.BookingLog:event_timestamp
 
 
Item 1031: 
for i in range(3000000):
 params-=gradient(params)
 if objective(params)<0.01:
 break
 
 
Item 1036: 
sample_test_data = ['Country is looking to encourage agriculture schemes']
sample_test_dtm = vec.transform(sample_test_data)
 iDF("is") = log(20/20) = 0 [Since 'is' occurs in all the documents ]
iDF("Everest") = log(20/5) = 0.6 [Since corpus is talking about 'Mountains']
iDF("stratovolcano") = log(10/1) = 1 [Since stratovolcano occurs in one doc]
 TfiDF = TF * iDF
 Therefore,
  TfiDF("is") = 1000 * 0 = 0 
  TfiDF("Everest") = 50 * 0.6 = 30
  TfiDF("stratovolcano") = 2 * 1 = 2
 def prepare_model(self):
    model = svm.SVC(kernel='linear')
    model.fit(train_vectors, train_labels)
 
 
Item 1039: 
data = pd.DataFrame(
    {
        'important_feature_poisson': np.random.poisson(3, 1000),
        'important_feature_dummy': np.random.binomial(1,0.5,1000),
        'random_feature_normal': np.random.normal(0,100,1000),
        'random_feature_dummy': np.random.binomial(1,0.5,1000)
    }
)

data['Y'] = np.random.normal(10,3,1000) * data['important_feature_poisson'] + np.random.normal(10,3,1000) * data['important_feature_dummy']
 X = data.drop(['Y'], axis=1)
Y = data['Y']
reg = RandomForestRegressor(random_state=1)
reg.fit(X, Y)
 features = X.columns.values
importances = reg.feature_importances_
indices = np.argsort(importances)

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='#8f63f4', align='center')
plt.yticks(range(len(indices)), features[indices])
plt.xlabel('Relative Importance')
plt.show()
 X = X.as_matrix()

# Below code from http://blog.datadive.net/selecting-good-features-part-iii-random-forests/
for train_idx, test_idx in ShuffleSplit(len(X), 100, .3):
    X_train, X_test = X[train_idx], X[test_idx]
    Y_train, Y_test = Y[train_idx], Y[test_idx]
    r = rf.fit(X_train, Y_train)
    acc = r2_score(Y_test, rf.predict(X_test))
    for i in range(X.shape[1]):
        X_t = X_test.copy()
        np.random.shuffle(X_t[:, i])
        shuff_acc = r2_score(Y_test, rf.predict(X_t))
        scores[features[i]].append((acc-shuff_acc)/acc)

mda_features = [f for f in scores.keys()]
mda_importance = [(np.mean(score)) for score in scores.values()]
mda_indices = np.argsort(mda_importance)

plt.title('Feature Importances')
plt.barh(range(len(mda_indices)), [mda_importance[i] for i in mda_indices], color='#8f63f4', align='center')
plt.yticks(range(len(mda_indices)), [mda_features[i] for i in mda_indices])
plt.xlabel('Mean decrease accuracy')
plt.show()
 
 
Item 1043: 
library(XML)
library(RCurl)
library(rvest)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggthemes)
library(reshape)
library(treemap)
 ggplot(aes(x=Year, y=Val), data = df_long_total) + geom_line(size = 2.5, alpha = 0.7, color = "mediumseagreen", group=1) + 
  geom_point(size = 0.5) + 
  ggtitle('Total Number of Accidents and Fatalities in the US 1994 - 2015') +
  ylab('count') +
  xlab('Year') +
  theme_economist_white()
 ggplot(aes(x=Year, y=Val), data = df_long_travel) + geom_line(size = 2.5, alpha = 0.7, color = "mediumseagreen", group=1) + 
  geom_point(size = 0.5) + 
  ggtitle('Total Vehicle Miles Traveled 1994 - 2015') +
  ylab('Billion Miles') +
  xlab('Year') +
  theme_economist_white()
 state <- state[c('State', 2015, 2014, 'Percent.Change')]
newdata <- state[order(-state$`2015`),]
newdata
 ggplot(aes(x = year, y=count, fill=killed), data=kill_full) +
  geom_bar(stat = 'identity', position = position_dodge()) + 
  xlab('Year') +
  ylab('Killed') +
  ggtitle('Number of Persons Killed in Traffic Accidents by Gender 1994 - 2015') + theme_economist_white()
 age_full$age <- ordered(age_full$age, levels = c('< 5', '5 -- 9', '10 -- 15', '16 -- 20', '21 -- 24', '25 -- 34', '35 -- 44', '45 -- 54', '55 -- 64', '65 -- 74', '> 74'))
ggplot(aes(x = age, y=count), data =age_full) + geom_bar(stat = 'identity') +
  xlab('Age') +
  ylab('Number of Killed') +
  ggtitle('Fatalities Distribution by Age Group 1994 - 2015') + theme_economist_white()
 ggplot(age_full, aes(x = year, y = count, colour = age)) + 
  geom_line() +
  geom_point() +
  facet_wrap(~age) + xlab('Year') +
  ggtitle('Traffic Fatalities by Age 1994 - 2015') + 
  theme(legend.position="none")
 treemap(kill_by_hour_group, index=c("hours","variable"), vSize="sum_hour", type="index", fontsize.labels=c(15,12), title='Fatalities by time of the day', fontcolor.labels=c("white","orange"), fontface.labels=c(2,1), bg.labels=c("transparent"),  align.labels=list(
  c("center", "center"), c("right", "bottom")), overlap.labels=0.5, inflate.labels=F,
)
 ggplot(aes(x = variable, y = sum_hour, fill = hours), data = kill_by_hour_group) +
  geom_bar(stat = 'identity', position = position_dodge()) +
  xlab('Hours') +
  ylab('Total Fatalities') +
  ggtitle('Fatalities Distribution by Time of the Day and Day of the week 1994-2015') + theme_economist_white()
 ggplot(aes(x = year, y = count, fill = hour), data = pair_all) +
  geom_bar(stat = 'identity', position = position_dodge()) +
  xlab('Year') +
  ylab('Number of Fatalities') +
  ggtitle('Fatal Crashes caused by Alcohol-Impaired Driving, by Time of Day 1994-2015') + theme_economist_white()
 ggplot(aes(x = year, y = mean, color = bac), data = al_all_by_bac) +
  geom_jitter(alpha = 0.05) +
  geom_smooth(method = 'loess') +
  xlab('Year') +
  ylab('Percentage of Killed') +
  ggtitle('Fatalities and Blood Alcohol Concentration of Drivers 1994-2015') + theme_economist_white()
 
 
Item 1044: 
/* A typical React class has state */
class HeroCharacter extends Component {
  constructor(props) {
    this.state = { health: 100; }
  }
 
 
Item 1046: 
> node index.js ../images/banana.jpg center up 2
Processing image: ../images/banana.jpg
Path will be placed center-up
Image saved at: ../images/banana-pathed.jpg
 node index.js ../images/bird.jpg left center 3
 
 
Item 1048: 
The person with phone number +86 921 547 9004 has been identified as a lead. We (company "Telecom") would like to know which of our customers have been in contact with this person since September 14th. This will help us in converting this lead into a customer.
 Get me the customers of company “Telecom” who called the target person with phone number +86 921 547 9004 from September 14th onwards.
 match
  $customer isa person has phone-number $phone-number;
  $company isa company has name "Telecom";
  (customer: $customer, provider: $company) isa contract;
  $target isa person has phone-number "+86 921 547 9004";
  (caller: $customer, callee: $target) isa call has started-at
  $started-at;
  $min-date == 2018-09-14T17:18:49; $started-at > $min-date;
get $phone-number;
 [ '+62 107 530 7500', '+370 351 224 5176', '+54 398 559 0423', 
  '+7 690 597 4443',  '+263 498 495 0617', '+63 815 962 6097', 
  '+81 308 988 7153', '+81 746 154 2598']
 We (company "Telecom") have received a number of harassment reports, which we suspect is caused by one individual. The only thing we know about the harasser is that he/she is aged roughly over 50 and lives in London. The reports have been made by young adults all aged under 20. We wonder if there is a pattern and so would like to speak to anyone who has received a call from a suspect, since the he/she potentially started harassing.
 Get me the phone number of people who have received a call from a customer aged over 50 after the suspect made a call to a customer aged under 20.
 match
  $suspect isa person has city "London", has age > 50;
  $company isa company has name "Telecom";
  (customer: $suspect, provider: $company) isa contract;
  $pattern-callee isa person has age < 20;
  (caller: $suspect, callee: $pattern-callee) isa call 
    has started at $pattern-call-date;
  $target isa person has phone-number $phone-number, 
    has is-customer false;
  (caller: $suspect, callee: $target) isa call 
    has started-at $target-call-date;
  $target-call-date > $pattern-call-date;
get $phone-number;
 [ '+30 419 575 7546',  '+86 892 682 0628', '+1 254 875 4647', 
  '+351 272 414 6570', '+33 614 339 0298', '+86 922 760 0418', 
  '+86 825 153 5518',  '+48 894 777 5173', '+351 515 605 7915', 
  '+63 808 497 1769',  '+27 117 258 4149', '+86 202 257 8619' ]
 The customer with phone number +7 171 898 0853 and +370 351 224 5176 have been identified as friends. We'd like to know who their common contacts are in order to offer them a group promotion.
 Get me the phone number of people who have received calls from both customer with phone number +7 171 898 0853 and customer with phone number +370 351 224 5176.
 match
  $common-contact isa person has phone-number $phone-number;
  $customer-a isa person has phone-number "+7 171 898 0853";
  $customer-b isa person has phone-number "+370 351 224 5176";
  (caller: $customer-a, callee: $common-contact) isa call;
  (caller: $customer-b, callee: $common-contact) isa call;
get $phone-number;
 ['+86 892 682 0628', '+54 398 559 0423']
 The person with phone number +48 894 777 5173 has been identified as a lead. We (company "Telecom") would like to know who his circle of  (customer) contacts are, so that we can encourage them in converting this lead to a customer.
 Get me the phone phone number of all customers who have called each other as well the person with phone number +48 894 777 5173.
 match
  $target isa person has phone-number "+48 894 777 5173";
  $company isa company has name "Telecom";
  $customer-a isa person has phone-number $phone-number-a;
  $customer-b isa person has phone-number $phone-number-b;
  (customer: $customer-a, provider: $company) isa contract;
  (customer: $customer-b, provider: $company) isa contract;
  (caller: $customer-a, callee: $customer-b) isa call;
  (caller: $customer-a, callee: $target) isa call;
  (caller: $customer-b, callee: $target) isa call;
get $phone-number-a, $phone-number-b;
 [ '+62 107 530 7500', '+261 860 539 4754', '+81 308 988 7153' ]
 In order to better understand our customers' behaviour, we (company "Telecom") like to know how the average phone call duration among those aged under 20 compares to those aged over 40.
 Get me the average call duration among customers who have a contract with company "Telecom" and are aged under 20.
 match 
  $customer isa person has age < 20; 
  $company isa company has name "Telecom";
  (customer: $customer, provider: $company) isa contract;
  (caller: $customer, callee: $anyone) isa call has duration
  $duration; 
aggregate mean $duration;
 1348 seconds
 Get me the average call duration among customers who have a contract with company "Telecom" and are aged over 40.
 match 
  $customer isa person has age > 40; 
  $company isa company has name "Telecom";
  (customer: $customer, provider: $company) isa contract;
  (caller: $customer, callee: $anyone) isa call has duration
  $duration; 
  aggregate mean $duration;
 1587 seconds
 
 
Item 1049: 
with payment_enrichment as (
  
  select
  
    order_id
    , order_date
    , user_id
    , total_charges
    , common_id
    , pup_id
    , pickup_date
    , date_trunc('month', order_date) as order_month
    , row_number() over (partition by user_id order by order_date) as payment_number
  
  from kdboller.relay_foods_v1  
  )
  
  select
    pe.*
    , case when payment_number = 1 then 'Initial' else 'Repeat' end as payment_type
    
  from payment_enrichment as pe
 
 
Item 1050: 
x = [2, 4, 6]
y = [1, 3, 5]
plt.plot(x, y)
plt.show()
 year = [1960, 1970, 1980, 1990, 2000, 2010]
pop_pakistan = [44.91, 58.09, 78.07, 107.7, 138.5, 170.6]
pop_india = [449.48, 553.57, 696.783, 870.133, 1000.
4, 1309.1]
 plt.plot(year, pop_pakistan, color='g')
plt.plot(year, pop_india, color='orange')
plt.xlabel('Countries')
plt.ylabel('Population in million')
plt.title('Pakistan India Population till 2010')
plt.show()
 
 
Item 1052: 
filter(year(timestamp) == year(today()) - 1
 filter(year(timestamp) == 2017)
 
 
Item 1054: 
val inputDescriptor = IndexedSeq(DataDesc(“data”, Shape(1,3,224,224), DType.Float32, “NCHW”))
val predictor = Predictor(“/path/to/model”, inputDescriptor)
 val resultNDArray = predictor.predictWithNDArray(inputNDArray)
 val result = predictor.predict(input)
 val classifier = Classifier(“/path/to/model”, inputDescriptor)
 val result = classifier.classify(input, topK)
val resultNDArray = classifier.classifyWithNDArray(inputNDArray, topK)
 val img = ImageClassfier.loadImageFromFile("/path/to/image")
val reshapedImage = ImageClassifier.reshapeImage(img, 224, 224)
val imgInNDArray = ImageClassifier.bufferedImageToPixels(reshapedImage, Shape(224,224))
 val classifier = new ImageClassifier("/path/to/model", inputDescriptor)
classifier.classifyWithNDArray(imgInNDArray, topK)
 val objectDector = new ObjectDetector("/path/to/model", inputDescriptor)
 val result = objectDector.imageObjectDetect(inputImage, topK)
val result_nd = objectDector.objectDetectWithNDArray(NDArrayInput, topK)
 
 
Item 1057: 
helm install seldon-core --name seldon-core --repo https://storage.googleapis.com/seldon-charts
 kubectl apply -f my_ml_deployment.yaml
 
 
Item 1059: 
--.--......-1----.-..--...-1---..-------..1---------..-1---------.----.--------.----.---.--.-.------------.
 
 
Item 1060: 
Antonio Castelli, Data Scientist at Booking.com (antonio.castelli@booking.com)
 Andrea Schioppa, Data Scientist at Booking.com (andrea.schioppa@booking.com)
 Marco Velliscig, Data Scientist at Booking.com (marco.velliscig@booking.com)
 
 
Item 1063: 
#standardsql
CREATE OR REPLACE MODEL flights.arrdelay
 OPTIONS
  (model_type='linear_reg', input_label_cols=['arr_delay']) AS
 SELECT
  arr_delay,
  carrier,
  origin,
  dest,
  dep_delay,
  taxi_out,
  distance
FROM
  `cloud-training-demos.flights.tzcorr`
WHERE
  arr_delay IS NOT NULL
 #standardsql
SELECT * FROM ML.PREDICT(MODEL flights.arrdelay,
(
SELECT
  carrier,
  origin,
  dest,
  dep_delay,
  taxi_out,
  distance,
  arr_delay AS actual_arr_delay
FROM
  `cloud-training-demos.flights.tzcorr`
WHERE
  arr_delay IS NOT NULL
LIMIT 10))
 #standardsql
CREATE OR REPLACE MODEL flights.ontime
OPTIONS
  (model_type='logistic_reg', input_label_cols=['on_time']) AS
 SELECT
  IF(arr_delay < 15, 1, 0) AS on_time,
  carrier,
  origin,
  dest,
  dep_delay,
  taxi_out,
  distance
FROM
  `cloud-training-demos.flights.tzcorr`
WHERE
  arr_delay IS NOT NULL
 #standardsql
SELECT * FROM ML.EVALUATE(MODEL flights.ontime,
(
SELECT
  IF(arr_delay < 15, 1, 0) AS on_time,
  carrier,
  origin,
  dest,
  dep_delay,
  taxi_out,
  distance
FROM
  `cloud-training-demos.flights.tzcorr`
WHERE
  arr_delay IS NOT NULL
))
 
 
Item 1067: 
p3 = []
index = int(random.random()*N)
beta = 0.0
mw = max(w)
for i in range(N):
  beta += random.random()*2*mw
   while w[index] < beta:
     beta = beta - w[index]
     index = index + 1

  p3.append(p[index]) 
 if(is_initialized) {
	return;
} 

//Number of particles
num_particles = 100;

//SD
double std_x = std[0];
double std_y = std[1];
double std_theta = std[2];

//Normal distributions
normal_distribution<double> dist_x(x, std_x);
normal_distribution<double> dist_y(y, std_y);
normal_distribution<double> dist_theta(theta, std_theta);

//Generate particles with normal distribution with mean on GPS values.
for(int i = 0; i < num_particles; i++) {
	Particle p;
	p.id = i;
	p.x = dist_x(gen);
	p.y = dist_y(gen);
	p.theta = dist_theta(gen);
	p.weight = 1.0;
	particles.push_back(p);
}
is_initialized = true;
 //Normal distributions for sensor noise
normal_distribution<double> dist_x(0, std_pos[0]);
normal_distribution<double> dist_y(0, std_pos[1]);
normal_distribution<double> dist_theta(0, std_pos[2]);

for(int i = 0; i < num_particles; i++) {
	if(fabs(yaw_rate) < 0.00001) {  
		particles[i].x += velocity * delta_t * cos(particles[i].theta);
		particles[i].y += velocity * delta_t * sin(particles[i].theta);
	} 
	else{
		particles[i].x += velocity / yaw_rate * (sin(particles[i].theta + yaw_rate*delta_t) - sin(particles[i].theta));
		particles[i].y += velocity / yaw_rate * (cos(particles[i].theta) - cos(particles[i].theta + yaw_rate*delta_t));
		particles[i].theta += yaw_rate * delta_t;
	}

	//Noise
	particles[i].x += dist_x(gen);
	particles[i].y += dist_y(gen);
	particles[i].theta += dist_theta(gen);
}
 //Each particle for loop
for(int i = 0; i < num_particles; i++) {
	double paricle_x = particles[i].x;
	double paricle_y = particles[i].y;
	double paricle_theta = particles[i].theta;

	//Create a vector to hold the map landmark locations predicted to be within sensor range of the particle
	vector<LandmarkObs> predictions;

	//Each map landmark for loop
	for(unsigned int j = 0; j < map_landmarks.landmark_list.size(); j++) {

		//Get id and x,y coordinates
		float lm_x = map_landmarks.landmark_list[j].x_f;
		float lm_y = map_landmarks.landmark_list[j].y_f;
		int lm_id = map_landmarks.landmark_list[j].id_i;

		//Only consider landmarks within sensor range of the particle (rather than using the "dist" method considering a circular region around the particle, this considers a rectangular region but is computationally faster)
		if(fabs(lm_x - paric
le_x) <= sensor_range && fabs(lm_y - paricle_y) <= sensor_range) {
			predictions.push_back(LandmarkObs{ lm_id, lm_x, lm_y });
		}
	}

	//Create and populate a copy of the list of observations transformed from vehicle coordinates to map coordinates
	vector<LandmarkObs> trans_os;
	for(unsigned int j = 0; j < observations.size(); j++) {
		double t_x = cos(paricle_theta)*observations[j].x - sin(paricle_theta)*observations[j].y + paricle_x;
		double t_y = sin(paricle_theta)*observations[j].x + cos(paricle_theta)*observations[j].y + paricle_y;
		trans_os.push_back(LandmarkObs{ observations[j].id, t_x, t_y });
	}

	//Data association for the predictions and transformed observations on current particle
	dataAssociation(predictions, trans_os);
	particles[i].weight = 1.0;
	for(unsigned int j = 0; j < trans_os.size(); j++) {
		double o_x, o_y, pr_x, pr_y;
		o_x = trans_os[j].x;
		o_y = trans_os[j].y;
		int asso_prediction = trans_os[j].id;

		//x,y coordinates of the prediction associated with the current observation
		for(unsigned int k = 0; k < predictions.size(); k++) {
			if(predictions[k].id == asso_prediction) {
				pr_x = predictions[k].x;
				pr_y = predictions[k].y;
			}
		}

		//Weight for this observation with multivariate Gaussian
		double s_x = std_landmark[0];
		double s_y = std_landmark[1];
		double obs_w = ( 1/(2*M_PI*s_x*s_y)) * exp( -( pow(pr_x-o_x,2)/(2*pow(s_x, 2)) + (pow(pr_y-o_y,2)/(2*pow(s_y, 2))) ) );

		//Product of this obersvation weight with total observations weight
		particles[i].weight *= obs_w;
	}
}
 //Get weights and max weight.
vector<double> weights;
double maxWeight = numeric_limits<double>::min();
for(int i = 0; i < num_particles; i++) {
	weights.push_back(particles[i].weight);
	if(particles[i].weight > maxWeight) {
		maxWeight = particles[i].weight;
	}
}

uniform_real_distribution<double> distDouble(0.0, maxWeight);
uniform_int_distribution<int> distInt(0, num_particles - 1);
int index = distInt(gen);
double beta = 0.0;
vector<Particle> resampledParticles;
for(int i = 0; i < num_particles; i++) {
	beta += distDouble(gen) * 2.0;
	while(beta > weights[index]) {
		beta -= weights[index];
		index = (index + 1) % num_particles;
	}
	resampledParticles.push_back(particles[index]);
}

particles = resampledParticles;
 
 
Item 1068: 
array([[0.57273 , 0.677385],
       [1.87446 , 2.06253 ],
       [3.33843 , 5.47434 ],
       [7.88282 , 3.52778 ],
       [9.77052 , 9.16828 ]])
 git clone https://github.com/pjreddie/darknet
cd darknet
make
wget https://pjreddie.com/media/files/yolov3.weights
./darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg
 
 
Item 1070: 
def agg_count(df, group_field):
    grouped = df.groupby(group_field, as_index=False).size()
    grouped.sort(ascending = False)
     grouped = pd.DataFrame(grouped).reset_index()
    grouped.columns = [group_field, 'Count']
    return grouped
 vehicles_2016 = vehicles[vehicles['Year']==2016]
category_counts = agg_count(vehicles_2016, 'Vehicle Category')
 ax = sns.barplot(data=category_counts, x='Count', y='Vehicle Category')
 ax.set(xlabel='\n Number of Vehicles Manufactured')
sns.plt.title('Vehicles Manufactured by Category (2016) \n')
 vehicles_1985 = vehicles[vehicles['Year']==1985]
category_counts = agg_count(vehicles, 'Vehicle Category')
 ax = sns.barplot(data=category_counts, x='Count', y='Vehicle Category')
 ax.set(xlabel='\n Number of Vehicles Manufactured')
sns.plt.title('Vehicles Manufactured by Category (1985) \n')
 class_counts = agg_count(vehicles_2016, 'Vehicle Class')
 ax = sns.barplot(data=class_counts, x='Count', y='Vehicle Class')
ax.set(xlabel='\n Number of Vehicles Manufactured')
sns.plt.title('Vehicles Manufactured by Class (2016) \n')
 make_counts = agg_count(vehicles_2016, 'Make')
 ax = sns.barplot(data=make_counts, x='Count', y='Make')
ax.set(xlabel='\n Number of Vehicles Manufactured')
sns.plt.title('Vehicles Manufactured by Make (2016) \n')
 very_efficient = vehicles[vehicles['Fuel Efficiency']=='Very High Efficiency']
make_counts = agg_count(very_efficient, 'Make')
 ax = sns.barplot(data=make_counts, x='Count', y='Make')
ax.set(xlabel='\n Number of Vehicles Manufactured')
sns.plt.title('Very Fuel Efficient Vehicles by Make \n')
 def agg_avg(df, group_field, calc_field):
    grouped = df.groupby(group_field, as_index=False)[calc_field].mean()
    grouped = grouped.sort(calc_field, ascending = False)
    grouped.columns = [group_field, 'Avg ' + str(calc_field)]
    return grouped
 category_avg_mpg = agg_avg(vehicles_2016, 'Vehicle Category', 'Combined MPG')
 ax = sns.barplot(data=category_avg_mpg, x='Avg Combined MPG', y='Vehicle Category')
ax.set(xlabel='\n Average Combined MPG')
sns.plt.title('Average Combined MPG by Category (2016) \n')
 def pivot_count(df, rows, columns, calc_field):
    df_pivot = df.pivot_table(values=calc_field, 
                              index=rows, 
                              columns=columns, 
                              aggfunc=np.size
                             ).dropna(axis=0, how='all')
    return df_pivot
 effic_size_pivot = pivot_count(vehicles_2016,'Fuel Efficiency',
                               'Engine Size','Combined MPG')
 sns.heatmap(effic_size_pivot, annot=True, fmt='g')
ax.set(xlabel='\n Engine Size')
sns.plt.title('Fuel Efficiency vs. Engine Size (2016) \n')
 effic_size_pivot = pivot_count(vehicles_1985,'Fuel Efficiency',
                               'Engine Size','Combined MPG')
 fig, ax = plt.subplots(figsize=(15,8))
sns.heatmap(effic_size_pivot, annot=True, fmt='g')
ax.set(xlabel='\n Engine Size')
sns.plt.title('Fuel Efficiency vs. Engine Size (1985) \n')
 effic_size_category = pivot_count(vehicles_2016,
                                  ['Engine Size','Fuel Efficiency'],
                                  'Vehicle Category','Combined MPG')
 fig, ax = plt.subplots(figsize=(20,10))
sns.heatmap(effic_size_category, annot=True, fmt='g')
ax.set(xlabel='\n Vehicle Category')
sns.plt.title('Fuel Efficiency + Engine Size vs. Vehicle Category (2016) \n')
 effic_size_pivot = pivot_count(vehicles_2016, 'Make',
                               'Vehicle Category','Combined MPG')
 fig, ax = plt.subplots(figsize=(20,20))
sns.heatmap(effic_size_pivot, annot=True, fmt='g')
ax.set(xlabel='\n Vehicle Category')
sns.plt.title('Make vs. Vehicle Category (2016) \n')
 def multi_line(df, x, y):
    ax = df.groupby([x, y]).size().unstack(y).plot(figsize=(15,8), cmap="Set2")
 multi_line(vehicles, 'Year', 'Vehicle Category')
ax.set(xlabel='\n Year')
sns.plt.title('Vehicle Categories Over Time \n')
 bmw = vehicles[vehicles['Make'] == 'BMW']
 multi_line(bmw, 'Year', 'Vehicle Category')
ax.set(xlabel='\n Year')
sns.plt.title('BMW Vehicle Categories Over Time \n')
 toyota = vehicles[vehicles['Make'] == 'Toyota']
 multi_line(toyota, 'Year', 'Vehicle Category')
ax.set(xlabel='\n Year')
sns.plt.title('Toyota Vehicle Categories Over Time \n')
 select_columns = ['Engine Displacement', 'Cylinders', 'Fuel Barrels/Year', 'City MPG','Highway MPG','Combined MPG', 'CO2 Emission Grams/Mile', 'Fuel Cost/Year', 'Cluster Name']
 sns.pairplot(vehicles[select_columns], hue='Cluster Name', size=3)
 sns.lmplot('Engine Displacement', 'Combined MPG', data=vehicles, 
           hue='Cluster Name', size=8, fit_reg=False)
 
 
Item 1072: 
react-native init BusinessCards
 yarn add react-native-camera react-native-text-detector
 
 
Item 1077: 
all_pisa_2015.head()
 #add column with country name
name_code_dict = create_name_code_dict()
code_name_dict = reverse_dict(name_code_dict)
add_country_name(all_pisa_2015, code_name_dict)
 all_pisa_2015.head()
 gov_edu_expenses.head()
 basic_edu_exp.head()
 basic_edu_exp.head()
 basic_student_pop.head()
 basic_student_pop.head()
 edu_data_joined.head()
 edu_data_per_student.head()
 
 
Item 1083: 
dependencies {
    implementation "ai.fritz:core:1.2.1"
    implementation "ai.fritz:vision-style-model:1.2.1"
}
 FritzVisionImage fritzImage = FritzVisionImage.fromBitmap(bitmap)
 
 
Item 1087: 
For a given task and class, the precision/recall curve is
computed from a method’s ranked output. Recall is defined
as the proportion of all positive examples ranked above a
given rank. Precision is the proportion of all examples above
that rank which are from the positive class. The AP summarises
the shape of the precision/recall curve, and is de-
fined as the mean precision at a set of eleven equally spaced
recall levels [0,0.1,...,1]:
 The precision at each recall level r is interpolated by taking
the maximum precision measured for a method for which
the corresponding recall exceeds r.
 
 
Item 1090: 
{
  "dataset_reader": {
    "type": "20newsgroups"
  },
  "train_data_path": "train",
  "test_data_path": "test",
  "evaluate_on_test": true,
  "model": {
    "type": "20newsgroups_classifier",
    "model_text_field_embedder": {
      "tokens": {
        "type": "embedding",
        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz",
        "embedding_dim": 100,
        "trainable": false
      }
    },
    "internal_text_encoder": {
      "type": "lstm",
      "bidirectional": true,
      "input_size": 100,
      "hidden_size": 100,
      "num_layers": 1,
      "dropout": 0.2
    },
    "classifier_feedforward": {
      "input_dim": 200,
      "num_layers": 2,
      "hidden_dims": [200, 100],
      "activations": ["relu", "linear"],
      "dropout": [0.2, 0.0]
    }
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [["text", "num_tokens"]],
    "batch_size": 64
  },
  "trainer": {
    "num_epochs": 40,
    "patience": 3,
    "cuda_device": 0,
    "grad_clipping": 5.0,
    "validation_metric": "+accuracy",
    "optimizer": {
      "type": "adagrad"
    }
  }
}
 @DatasetReader.register("20newsgroups")
class NewsgroupsDatasetReader(DatasetReader):
     @overrides
    def text_to_instance(self, newsgroups_post: str, label: str = None) -> Instance:  
        tokenized_text = self._tokenizer.tokenize(newsgroups_post)
        post_field = TextField(tokenized_text, self._token_indexers)
        fields = {'post': post_field}
        if label is not None:
            fields['label'] = LabelField(int(label),skip_indexing=True)
        return Instance(fields)
 
 
Item 1092: 
input + programming = output
 input + output = programming
 
 
Item 1095: 
User: Tell me when my wife’s flight lands.
 Assistant: Oh the flight from Bangalore? It lands at 9 PM. She should be out by 9:30
 User: Cool. When should I leave?
 Assistant: It’s better you leave at 8:30. Should I book you a cab?
 User: Umm no. I’ll drive today.

 … after a few hours

 User: Hey can you book me the cab?
 Assistant: Sure! I’ll book it right away.
 Priming
 In psychology, priming is a technique whereby exposure to one stimulus influences a response to a subsequent stimulus, without conscious guidance or intention. For example, you are more likely to answer ‘Mongolia’ when asked to name a place that starts with the letter ‘M’ if you’ve just seen a documentary about Mongolia.
 Letting the user know what to expect is also a form of priming. It informs users on how to prepare themselves. Priming however can be subtle, if the VUI responds to the query “Can you play me ‘Fix you’?” as “Playing ‘Fix you’ by ‘Coldplay.’” Next time the user can simply say “Play ‘Fix you’ by ‘Coldplay’”
 
 
Item 1096: 
Sequence 1:    G   K   N
Sequence 2:    T   R   N
Sequence 3:    S   H   E
Sum of pairs: -1 + 1 + 6 = 6
 chmod 777 clustal-omega-1.2.3-macosx
 clustalo -i /Users/vijinimallawaarachchi/Documents/Python/Acanthaster_planci_Gnomon.fsa -o aligned.fasta — auto -v
 ./clustal-omega-1.2.3-macosx -i /Users/vijinimallawaarachchi/Documents/Python/Acanthaster_planci_Gnomon.fsa -o aligned.fasta --auto -v
 
 
Item 1105: 
(20 time-step simulation)
RMSE Kalman Filter: 0.2917
RMSE Measurements:  0.4441
 (100,000 time-step simulation)
RMSE Kalman Filter: 0.3162
RMSE Measurements:  0.4241
 
 
Item 1106: 
test_image = image.load_img(‘./newimages/puppy3.jpg’, target_size = (64, 64))
 ‘./newimages/puppy3.jpg’ to ‘./newimages/bunny1.jpg’
 
 
Item 1110: 
Black-grass
Charlock
Cleavers
Common Chickweed
Common wheat
Fat Hen
Loose Silky-bent
Maize
Scentless Mayweed
Shepherds Purse
Small-flowered Cranesbill
Sugar beet
 
 
Item 1112: 
E.g. for Age:
 x = 55
 min (x) = 35
 max (x) =77
 z = (55- 35) / (77- 35) = 0.47
 E.g. for Age:
 x = 55
 mean of all values= 51.8
 standard deviation of all values=16.02
 z = (55- 51.8) / 16.02 = 0.19
 E.g. for Age:
 x = 55
 c = 10
 z = 55/10 = 5.5
 
 
Item 1113: 
MSA, Year, Occupancy, Occupancy_Growth_1yr, Occupancy_Growth_3yr, Occupancy_Growth_5yr, F2, F2_Growth_1yr,..,FN_Growth_5yr, RevPAF_Growth_1yr_ahead, RevPAF_Growth_3yr_ahead, RevPAF_Growth_5yr_ahead
 Generate a range of hyperparameters for each regression model
For each regression model:
    For each cross_validation method(rolling window,rolling origin):
        For each associated hyperparameter:
            Cross validate
            Keep track of average Spearman over cross validation
Record the model, hyperparameters, and cross validation method that achieves the highest average Spearman correlation over all steps of cross validation
 
 
Item 1114: 
m <- glmer(late ~ treatment + (1 | user), data=df, family=binomial)
 
 
Item 1115: 
[{‘col’: ‘finished_race_order’, 
  ‘mapping’: [("First", 1), 
              (‘Second’, 2), 
              (‘Third’, 3)]
}]
 
 
Item 1119: 
for comment_tracker in subreddit.stream.comments():
     if re.search("FloBot", comment_tracker.body, re.IGNORECASE):
         comment = comment_tracker.parent()
 print("Subreddit: ", comment.subreddit)
 print("Author: ", comment.author)
 print("Text: '", comment.body,"'")
 print("Score: ", comment.score)
 print("Sentiment Analysis Subjectivity: ", np.round(TextBlob(comment.body).sentiment.subjectivity,4))
 print("Sentiment Analysis Polarity: ", np.round(TextBlob(comment.body).sentiment.polarity,4))
 
 
Item 1121: 
brew install git-lfs
 git lfs install
 git clone https://github.com/udacity/self-driving-car-sim.git
 git clone https://github.com/naokishibuya/car-behavioral-cloning.git
 cd car-behavioral-cloning
conda env create -f environments.xml
 source activate car-behavioral-cloning
 python drive.py model.h5
 
 
Item 1123: 

if (experience < = 10)
{ salary = experience * 1.5 * 100000} 
else if(experience >10)
{ salary = experience * 2 * 100000}
 Salary = Experience * Magic_Number_1 + JobLevel * Magic_Number_2 + Skill * Magic_Number_3 + Magic_Number_4
 
 
Item 1124: 
tfidf_matrix = tf.fit_transform(ds['Book Title'])
 
 
Item 1127: 
Localization losses: 
* WeightedL2LocalizationLoss 
* WeightedSmoothL1LocalizationLoss 
* WeightedIOULocalizationLoss
 Classification losses: 
* WeightedSigmoidClassificationLoss 
* WeightedSoftmaxClassificationLoss 
* BootstrappedSigmoidClassificationLoss
 
 
Item 1129: 
epochs = 100
kernel_size = 3 #kernel_size of 1 worked surprisingly well
pool_size = 2
dropout_rate = 0.15
f_act = 'relu'
 first_model = Sequential()
first_model.add(Conv1D(512, (kernel_size), input_shape=(X_trainS1.shape[1],X_trainS1.shape[2]), activation=f_act, padding='same'))
first_model.add(BatchNormalization())
first_model.add(MaxPooling1D(pool_size=(pool_size)))
first_model.add(Dropout(dropout_rate))
first_model.add(Conv1D(64, (kernel_size), activation=f_act, padding='same'))
first_model.add(BatchNormalization())
first_model.add(MaxPooling1D(pool_size=(pool_size)))
first_model.add(Dropout(dropout_rate))
first_model.add(Conv1D(32, (kernel_size), activation=f_act, padding='same'))
first_model.add(BatchNormalization())
first_model.add(MaxPooling1D(pool_size=(pool_size)))
first_model.add(LSTM(128, return_sequences=True))
first_model.add(LSTM(128, return_sequences=True))
first_model.add(LSTM(128))
first_model.add(Dropout(dropout_rate))
 second_model = Sequential()
second_model.add(Conv1D(512, (kernel_size), input_shape=(X_trainS2.shape[1],X_trainS2.shape[2]), activation=f_act, padding='same'))
second_model.add(BatchNormalization())
second_model.add(MaxPooling1D(pool_size=(pool_size)))
second_model.add(Dropout(dropout_rate))
second_model.add(Conv1D(64, (kernel_size), activation=f_act, padding='same'))
second_model.add(BatchNormalization())
second_model.add(MaxPooling1D(pool_size=(pool_size)))
second_model.add(Dropout(dropout_rate))
second_model.add(Conv1D(32, (kernel_size), activation=f_act, padding='same'))
second_model.add(BatchNormalization())
second_model.add(MaxPooling1D(pool_size=(pool_size)))
second_model.add(LSTM(128, return_sequences=True))
second_model.add(LSTM(128, return_sequences=True))
second_model.add(LSTM(128))
second_model.add(Dropout(dropout_rate))
 third_model = Sequential()
third_model.add(Conv1D(512, (kernel_size), input_shape=(X_trainS3.shape[1],X_trainS3.shape[2]), activation=f_act, padding='same'))
third_model.add(BatchNormalization())
third_model.add(MaxPooling1D(pool_size=(pool_size)))
third_model.add(Dropout(dropout_rate))
third_model.add(Conv1D(64, (kernel_size), activation=f_act, padding='same'))
third_model.add(BatchNormalization())
third_model.add(MaxPooling1D(pool_size=(pool_size)))
third_model.add(Dropout(dropout_rate))
third_model.add(Conv1D(32, (kernel_size), activation=f_act, padding='same'))
third_model.add(BatchNormalization())
third_model.add(MaxPooling1D(pool_size=(pool_size)))
third_model.add(LSTM(128, return_sequences=True))
third_model.add(LSTM(128, return_sequences=True))
third_model.add(LSTM(128))
third_model.add(Dropout(dropout_rate))
 model = Sequential()
model.add(Merge([first_model, second_model, third_model], mode='concat'))
model.add(Dropout(0.4))
model.add(Dense(n_classes))
model.add(BatchNormalization())
model.add(Activation('softmax'))
 model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])
 history = model.fit([X_trainS1, X_trainS2, X_trainS3], Y_train,
                    batch_size=batch_size,
                    validation_data=([X_valS1, X_valS2, X_valS3], Y_val),epochs=epochs)
 
 
Item 1130: 
transforms_basic    = [RandomRotate(10), RandomLighting(0.05, 0.05)] transforms_side_on  = transforms_basic + [RandomFlip()] transforms_top_down = transforms_basic + [RandomDihedral()]
 aug_tfms = transform_basic = [RandomRotate(10), RandomLighting(0.05, 0.05)]
 aug_tfms = transforms_side_on = transforms_basic + [RandomFlip()] 
 aug_tfms = transforms_top_down = transforms_basic + [RandomDihedral()]
 aug_tfms = RandomRotate(90, p=0.75, mode=cv2.BORDER_REFLECT, tfm_y=TfmType.NO)]
 aug_tfms = [RandomLighting(b=0.5, c=0, tfm_y=TfmType.NO)]
 aug_tfms = [RandomLighting(b=0, c=10, tfm_y=TfmType.NO)]
 aug_tfms = [RandomZoom(zoom_max=1)]
 aug_tfms = [RandomStretch(max_stretch=2)]
 aug_tfms = [RandomDihedral()]
 aug_tfms = [GoogleNetResize(targ_sz=10)]
 aug_tfms = [Cutout(n_holes=10, length=20, tfm_y=TfmType.NO)]
 aug_tfms = [AddPadding(pad=20, mode=cv2.BORDER_WRAP)]
 aug_tfms = [AddPadding(pad=20, mode=cv2.BORDER_REPLICATE)]
 aug_tfms = [AddPadding(pad=20, mode=cv2.BORDER_CONSTANT)]
 aug_tfms = [RandomRotateZoom(deg=45, zoom=2, stretch=1)]
 
 
Item 1131: 
dfs(vertex v)
    {
    visit(v);
    for each neighbor w of v
        if w is unvisited
        {
        dfs(w);
        add edge vw to tree T
        }
    }
 
 
Item 1134: 
> tensorboard --logdir={model_dir}
 
 
Item 1138: 
curl -H 'Client-ID: uo6dggojyb8d6soh92zknwmi5ej1q2' \
-X GET 'https://api.twitch.tv/helix/streams/metadata'
 {"data":
   [
      {
         "user_id":"23161357",
         "game_id":"488552",
         "overwatch":{
            "broadcaster":{
               "hero":{
                  "role":"Offense",
                  "name":"Soldier 76",
                  "ability":"Heavy Pulse Rifle"
               }
            }
         },
         "hearthstone":null
      },
      {
         "user_id":"1564968",
         "game_id":"138585",
         "overwatch":null,
         "hearthstone":{
            "broadcaster":{
               "hero":{
                  "type":"Classic hero",
                  "class":"Shaman",
                  "name":"Thrall"
               }
            },
            "opponent":{
               "hero":{
                  "type":"Classic hero",
                  "class":"Warrior",
                  "name":"Garrosh Hellscream"
               }
            }
         }
      },
      {
         "user_id":"5848726",
         "game_id":null,
         "overwatch":null,
         "hearthstone":null
      },
         . . .
   ],
   "pagination":{"cursor": "eyJiIjpudWxsLCJhIjp7Ik9mZnNldCI6MjB9fQ=="}
}
 
 
Item 1140: 
{
    "intent": "order-items",
    "items": [{
        "item-name": "pizza",
        "toppings": ["anchovies"]
    }]
}
 {
    "intent": "order-items",
    "items": [{
        "item-name": "pizza"
    },{ 
        "item-name": "fries"
    }]
}
 
 
Item 1143: 
Update [17/11/17]: The full implementation of Supervised Linear Regression can be found here.
 X (input) = Assignment Results
Y (output) = Final Exam Mark
f = function which describes the relationship between X and Y
e (epsilon) = Random error term (positive or negative) with a mean zero (there are move assumptions for our residuals, however we won't be covering them)
 Regression
 Y = f(X) + e, where X = (X1, x2...Xn)
 Training: Machine learns (fits) f from labelled training set
 Test: Machine predicts Y from unlabeled test set
 Note: f(x) can be derived through matrices to perform least square linear regression. However this beyond the scope of this tutorial, if you'd like to learn how to derive regression lines here is a good link . Also, X can be a tensor with any number of dimensions. A 1D tensor is a vector (1 row, many columns), 2D tensor is a matrix (many rows, many columns), and higher dimensional tensor.
 Bias-Variance Tradeoff
 Bias: is the amount of error introduced by approximating real-world phenomena with a simplified model.
 Variance: is how much your model’s test error changes based on variation in the training data. It reflects the model’s sensitivity to the idiosyncrasies of the data set it was trained on
 As a model increases in complexity and it becomes flexible, its bias decreases (it does a good job of explaining the training data), but variance increases (it doesn’t generalise as well).
 For those whom are interested in the full implementation of Supervised Learning: Linear Regression. It can be found here 
 
 
Item 1147: 
                     Steps to build and deploy
 Step 1: > bazel run deploy:deploy-to-minikube
 Step 2: > echo “Done.”
 
 
Item 1149: 
Add noise to image
noise = np.random.random((IMG_SIZE, IMG_SIZE), np.uint8)
 Matrix addition.. add noise to the image
image += noise

 
 
Item 1151: 
>install.packages('prophet')
 >library(prophet)
 >df <- example_wp_peyton_manning
 > head(df) 
# A tibble: 6 x 2 ds y <date> <int> 1 2007-12-10 14629 2 2007-12-11 5012 3 2007-12-12 3582 4 2007-12-13 3205 5 2007-12-14 2680 6 2007-12-15 2401
 >plot(df$y)
 >df1 <- df
 >df1$y <- log(df$y)
 >plot(df1$y)
 >m <- prophet(df1)
 > future <- make_future_dataframe(m , periods = 365)
 > tail(future) 
ds 3265 2017-01-14 3266 2017-01-15 3267 2017-01-16 3268 2017-01-17 3269 2017-01-18 3270 2017-01-19
 > head(future) 
ds 1 2007-12-10 2 2007-12-11 3 2007-12-12 4 2007-12-13 5 2007-12-14 6 2007-12-15
 > pred <- predict(m, future) |=============================================|100% ~0 s remaining
 > tail(pred$yhat) 
[1] 7.823991 8.205770 8.530798 8.318204 8.150827 8.162741
 > plot(m, pred)
 
 
Item 1152: 
Note: The code to collect data and perform insights on your own music is linked below.
 If you would like to try this for yourselves and have a look at your own Spotify tastes and try to do this yourselves, I've attached the link to the code I've used to collect this data here
 
 
Item 1159: 
[
a1 -> area of the house
a2 -> distance of the house from the nearest store
a3 -> previous selling price of the house.
...
an
]
 
 
Item 1162: 
tensorflowjs_converter --quantization_bytes 1 --input_format=tf_frozen_model --output_node_names=logits/BiasAdd --saved_model_tags=serve ./model/input_graph.pb ./web_model
 {
  "name":"conv0/filter",
  "shape":[3,3,3,32],
  "dtype":"float32",
  "quantization":{
    "dtype":"uint8",
    "scale":0.004699238725737029,
    "min":-0.7471789573921876
  }
}
 const manifest = await (await fetch('/models/my-model-weights_manifest.json')).json()
 const tensorMap = tf.io.loadWeights(manifest, '/models')
 {
  "conv0/filter": tf.Tensor4D,
  ...
}
 
 
Item 1166: 
%%bash
rm -rf dialog_converter
git clone https://github.com/b0noI/dialog_converter.git
cd dialog_converter
git checkout b9cc7b7d82a959c80e5048b18e956841233c7688
python3 ./converter.py
ls
 %%bash
rm -rf subword-nmt
git clone https://github.com/b0noI/subword-nmt.git
cd subword-nmt
git checkout dbe97c8f95f14d06b2e46b8053e2e2f9b9bf804e
 %%bash
# Create unique words (vocabulary) from training data
subword-nmt/learn_joint_bpe_and_vocab.py --input dialog_converter/train.a dialog_converter/train.b -s 50000 -o code.bpe --write-vocabulary vocab.train.bpe.a vocab.train.bpe.b
 %%bash
# Remove the tab from vocabulary 
sed -i '/\t/d' ./vocab.train.bpe.a
sed -i '/\t/d' ./vocab.train.bpe.b
 %%bash
# Remove the frequency from vocabulary
cat vocab.train.bpe.a | cut -f1 --delimiter=' ' > revocab.train.bpe.a
cat vocab.train.bpe.b | cut -f1 --delimiter=' ' > revocab.train.bpe.b
 %%bash
subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.a --vocabulary-threshold 5 < dialog_converter/train.a > train.bpe.a
subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.b --vocabulary-threshold 5 < dialog_converter/train.b > train.bpe.b
subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.a --vocabulary-threshold 5 < dialog_converter/test.a > test.bpe.a
subword-nmt/apply_bpe.py -c code.bpe --vocabulary vocab.train.bpe.b --vocabulary-threshold 5 < dialog_converter/test.b > test.bpe.b
 %%bash
rm -rf /content/nmt_model
rm -rf nmt
git clone https://github.com/tensorflow/nmt/
 %%bash
mkdir -p /content/nmt_model
cp dialog_converter/train.a /content/nmt_model
cp dialog_converter/train.b /content/nmt_model
cp dialog_converter/test.a /content/nmt_model
cp dialog_converter/test.b /content/nmt_model
cp revocab.train.bpe.a /content/nmt_model
cp revocab.train.bpe.b /content/nmt_model
cp train.bpe.a /content/nmt_model
cp test.bpe.a /content/nmt_model
cp train.bpe.b /content/nmt_model
cp test.bpe.b /content/nmt_model
 !cd nmt && python3 -m nmt.nmt \
    --src=a --tgt=b \
    --vocab_prefix=/content/nmt_model/revocab.train.bpe \
    --train_prefix=/content/nmt_model/train.bpe \
    --dev_prefix=/content/nmt_model/test.bpe \
    --test_prefix=/content/nmt_model/test.bpe \
    --out_dir=/content/nmt_model \
    --num_train_steps=45000000 \
    --steps_per_stats=100000 \
    --num_layers=2 \
    --num_units=128 \
    --batch_size=16 \
    --num_gpus=1 \
    --dropout=0.2 \
    --learning_rate=0.2 \
    --metrics=bleu
 %%bash
pwd
cd nmt
touch /content/output
chat () {
    echo $1 > /content/input
    $HOME/subword-nmt/apply_bpe.py -c $HOME/code.bpe --vocabulary $HOME/vocab.train.bpe.a --vocabulary-threshold 5 < /content/input > /content/input.bpe
    cd $HOME/nmt
    python -m nmt.nmt  --out_dir=/content/nmt_model --inference_input_file=/content/input.bpe --inference_output_file=/content/output > /dev/null 2>&1
    cat /content/output
}
chat "hi"
 
 
Item 1167: 
%timeit df.state_bottle_retail.apply(lambda x: x.replace('$',''))
 %timeit df.state_bottle_retail.apply(lambda x: x.strip('$'))
 %timeit [x.strip('$') for x in df.state_bottle_retail]
 %timeit [x[1:] for x in df.state_bottle_retail]
 df.state_bottle_retail = [x.strip('$') for x in df.state_bottle_retail]
 np.fromstring(df.state_bottle_retail.values.astype('|S7').tobytes().replace(b'$',b''), dtype='|S6')
 
 
Item 1171: 
File "/Users/orion.delwaterman/.pyenv/versions/3.6.1/Python.framework/Versions/3.6/lib/python3.6/ssl.py", line 625, in read
    v = self._sslobj.read(len, buffer)
 //https://github.com/python/cpython/blob/master/Modules/_ssl.c#L2361-L2364
static PyObject *_ssl__SSLSocket_read_impl(PySSLSocket *self, int len, int group_right_1, Py_buffer *buffer)
 
 
Item 1180: 
implementation 'com.google.firebase:firebase-ml-vision:15.0.0'
 <meta-data
    android:name="com.google.firebase.ml.vision.DEPENDENCIES"
    android:value="text" />
 android:value="text,barcode,face,label"
 FirebaseVisionTextDetector detector = FirebaseVision.getInstance()
    .getVisionTextDetector();
 FirebaseVisionBarcodeDetectorOptions options =
    new FirebaseVisionBarcodeDetectorOptions.Builder()
        .setBarcodeFormats(FirebaseVisionBarcode.FORMAT_QR_CODE,
                           FirebaseVisionBarcode.FORMAT_AZTEC)
        .build();
 FirebaseVisionBarcodeDetector detector = FirebaseVision.getInstance()
    .getVisionBarcodeDetector(options);
 FirebaseVisionFaceDetector detector = FirebaseVision.getInstance()
    .getVisionFaceDetector();
 implementation 'com.google.firebase:firebase-ml-vision-image-label-model:15.0.0'
 FirebaseVisionLabelDetector detector = FirebaseVision.getInstance()
    .getVisionLabelDetector();
 
 
Item 1182: 
Chicago, IL
chicago il
Chicago Illinois
...
 
 
Item 1186: 
s3_input_trains3_inpu  = sagemaker.s3_input(s3_data='s3://{}/{}'.format(bucket, path_train), content_type='csv',distribution='ShardedByS3Key')
 container = get_image_uri(boto3.Session().region_name, 'xgboost')
                          sess = sagemaker.Session()
                          role = get_execution_role()
xgb = sagemaker.estimator.Estimator(container,
                                    role,
                                    train_instance_count=4,
                                    train_instance_type=
                                    'ml.m4.xlarge',
                                    output_path=output_path,
                                    sagemaker_session=sess)
 xgb.set_hyperparameters(max_depth=9,
                        eta=0.2, gamma=4,
                        min_child_weight=300,
                        subsample=0.8,
                        silent=0,
                        objective='reg:linear',
                        early_stopping_rounds=10,
                        num_round=10000)
 xgb.fit({'train': s3_input_trains3_inpu, 
         'validation': s3_input_validation})
 endpoint_name = 'taxi-fare-prediction'
content_type = 'text/csv'
 
runtime = boto3.Session().client('sagemaker-runtime')
response = runtime.invoke_endpoint(EndpointName=endpoint_name,\
                                    ContentType='text/csv',\
                                    Body=data)
 results = list(ast.literal_eval(response['Body'].read().decode()))
print(results)
 
 
Item 1188: 
{ 
    “sessionAttributes”: { “key1”: “value1”, “key2”: “value2” … },
    “dialogAction”: { “type”: “ElicitIntent, ElicitSlot, 
                               ConfirmIntent, Delegate, or Close"
    } 
}
 {
    "sessionAttributes": {
        "key1": "value1",
        "key2": "value2"
        ...
    },
    "dialogAction": {
        "type": "ElicitIntent, ElicitSlot, ConfirmIntent, Delegate, 
                   or Close"
    }
}
 
 
Item 1189: 
func manager(_ manager: PhotoPickerManager, didPickImage image: UIImage) { 
        self.photoImageView.image = image
        KairosAPI.sharedInstance.detect(image) { [unowned self] result in
            switch result {
            case .success(let analyzis):
                self.setUIForResponse(.analyzed(analyzis))
            case .error(let error):
                self.setUIForResponse(.error(error))
            }
        }
        manager.dismissPhotoPicker(animated: true, completion: nil)
    }
 
 
Item 1195: 
x = input, w = weight 
 input + weight = E
 (x0 * w0) + (x1 * w1) + (x2 * w2)
 (2.0 * 0.1) + (3.0 * 0.5) + (4.0 * 0.9) = 5.3
 b = bias, Y = b + E
 E + bias = Y
 5.3 + (-2.0) =  3.3
 A = activation function 
 A = sigmoid(Y)
 
 
Item 1198: 
using TensorFlow;
 #if UNITY_ANDROID
TensorFlowSharp.Android.NativeBinding.Init();
#endif
 
 
Item 1199: 
let faceDetector = vision.faceDetector(options: options)
 let visionImage = VisionImage(image: image)
faceDetector.detect(in: visionImage) { (faces, error) in
    guard error == nil, let faces = faces, !faces.isEmpty else {
        return
    }
 for face in faces {
        if face.hasSmilingProbability {
            let smileProb = face.smilingProbability
            if (smileProb > 0.5) {
                Log.info("Smiling!", error: nil)
            }
        }
    }
}
 
 
Item 1206: 
| Term   | Term Count | 
|--------|------------| 
| this   |     1      | 
| is     |     1      | 
| a      |     2      | 
| sample |     1      |
 | Term    | Term Count | 
|---------|------------| 
| this    |     1      | 
| is      |     1      | 
| another |     2      | 
| example |     3      |
 TF(t) = (Número de vezes que o termo t aparece no documento) / (Número total de termos presentes no documento)
 TF('this',   Documento 1) = 1/5 = 0.2
TF('example',Documento 1) = 0/5 = 0
 TF('this',   Documento 2) = 1/7 = 0.14
TF('example',Documento 2) = 3/7 = 0.43
 IDF(t) = log_e(Número total de documentos / Número de documentos com o termo t presente)
 IDF('this',   Documentos) = log(2/2) = 0
 IDF('example',Documentos) = log(2/1) = 0.30
 TF-IDF('this',   Documento 1) = 0.2 x 0  = 0
TF-IDF('this',   Documento 2) = 0.14 x 0 = 0
 TF-IDF('example',Documento 2) = 0.43 x 0.30 = 0.13
 c1: Human machine interface for ABC computer applications 
c2: A survey of user opinion of computer system response time 
c3: System and human system engineering testing of EPS 
 m1: The generation of random, binary, ordered trees 
m2: The intersection graph of paths in trees 
m3: Graph minors: A survey
 | termo     | c1 | c2 | c3 | m1 | m2 | m3 | 
|-----------|----|----|----|----|----|----|
| human     | 1  | 0  | 1  | 0  | 0  | 0  |
| interface | 1  | 0  | 0  | 0  | 0  | 0  | 
| computer  | 1  | 1  | 0  | 0  | 0  | 0  | 
| user      | 0  | 1  | 0  | 0  | 0  | 0  | 
| system    | 0  | 1  | 2  | 0  | 0  | 0  | 
| survey    | 0  | 1  | 0  | 0  | 0  | 1  | 
| trees     | 0  | 0  | 0  | 1  | 1  | 0  | 
| graph     | 0  | 0  | 0  | 0  | 1  | 1  | 
| minors    | 0  | 0  | 0  | 0  | 0  | 1  | 
 human     = (-1.031, 0.440)
interface = (-0.318, 0.109)
computer  = (-0.922, -0.123)
user      = (-0.604, -0.232)
system    = (-2.031, -0.232) 
survey    = (-0.759, -0.988)
trees     = (-0.035, -0.637)
graph     = (-0.184, -1.231) 
minors    = (-0.152, -0.758)
 c1        = (-0.850, 0.214)
c2        = (-1.614, -0.458)
c3        = (-1.905, 0.658)
m1        = (-0.013, -0.321)
m2        = (-0.083, -0.942)
m3        = (-0.409, -1.501)
 y topic:
[('object', 0.29383227033104375),
 ('software', -0.22197520420133632),
 ('algorithm', 0.20537550622495102),
 ('robot', 0.18498675015157251),
 ('model', -0.17565360130127983),
 ('project', -0.164945961528315),
 ('busines', -0.15603883815175643),
 ('management', -0.15160458583774569),
 ('process', -0.13630070297362168),
 ('visual', 0.12762128292042879)]
 
 
Item 1211: 
1 + 1 = 11        1 + 1 = 10        1 + 1 = 3      
 2 + 2 = 22        2 + 2 = 20        2 + 2 = 5
 3 + 3 = ?         3 + 3 = ??        3 + 3 = ???
 Addition   Above examples(logic)
2,3 5       1,1 11      1,1 10    
3,4,7       2,2 22      2,2 20
4,5,9       3,3 33      3,3 30
.....       ......      ......
 “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at the tasks improves with the experiences”
 
 
Item 1214: 
define
  
  contract sub relationship,
    relates provider,
    relates customer;
 call sub relationship,
    relates caller,
    relates callee;
 company sub entity,
    plays provider;
 person sub entity,
    plays customer,
    plays caller,
    plays callee;
 define
  
  contract sub relationship,
    relates provider,
    relates customer;
 call sub relationship,
    relates caller,
    relates callee,
    has started-at,
    has duration;
 company sub entity,
    plays provider,
    has name;
 person sub entity,
    plays customer,
    plays caller,
    plays callee,
    has first-name,
    has last-name,
    has phone-number,
    has city,
    has age,
    has is-customer;
 define
  
  contract sub relationship,
    relates provider,
    relates customer;
 call sub relationship,
    relates caller,
    relates callee,
    has started-at,
    has duration;
 company sub entity,
    plays provider,
    has name;
 person sub entity,
    plays customer,
    plays caller,
    plays callee,
    has first-name,
    has last-name,
    has phone-number,
    has city,
    has age,
    has is-customer;
  
  name sub attribute datatype string;
  started-at sub attribute datatype date;
  duration sub attribute datatype long;
  first-name sub attribute datatype string;
  last-name sub attribute datatype string;
  phone-number sub attribute datatype string;
  city sub attribute datatype string;
  age sub attribute datatype long;
  is-customer sub attribute datatype boolean;
 ./grakn server start
 ./graql console --keyspace phone_calls --file path/to/the/schema.gql
 ./graql console --keyspace phone_calls
 match $x sub thing; get;
 {$x label thing;}
{$x label entity;}
{$x label relationship;}
{$x label attribute;}
{$x label company sub entity;}
{$x label person sub entity;}
{$x label call sub relationship;}
{$x label "@has-attribute" sub relationship;}
{$x label contract sub relationship;}
{$x label name sub attribute;}
{$x label started-at sub attribute;}
{$x label age sub attribute;}
{$x label city sub attribute;}
{$x label last-name sub attribute;}
{$x label first-name sub attribute;}
{$x label duration sub attribute;}
{$x label phone-number sub attribute;}
{$x label is-customer sub attribute;}
{$x label "@has-started-at" sub "@has-attribute";}
{$x label "@has-first-name" sub "@has-attribute";}
{$x label "@has-city" sub "@has-attribute";}
{$x label "@has-name" sub "@has-attribute";}
{$x label "@has-duration" sub "@has-attribute";}
{$x label "@has-age" sub "@has-attribute";}
{$x label "@has-last-name" sub "@has-attribute";}
{$x label "@has-phone-number" sub "@has-attribute";}
{$x label "@has-is-customer" sub "@has-attribute";}
 
 
Item 1215: 
git clone https://github.com/bloomsburyai/cape-slack-tutorial.git
 pip3 install -r requirements.txt
 ./get-id.py your_slack_key you_bot_name
 
 
Item 1218: 
cb.Teach(ctx, "model1", classificationbox.Example{
    Class: class,
    Inputs: []classificationbox.Feature{
        classificationbox.FeatureText("line", line),
    },
})
 POST /classificationbox/models/1/predict
{
    "limit": 1
    "inputs": [{
        "key": "line",
        "type": "text",
        "value": "Log line goes here"
    }]
}
 {
    "success": true,
    "classes": [{
            "id": "noise",
            "score": 0.97
    }]
}
 cat logfile.txt | logclass 
 sometool | logclass | notifytool
 
 
Item 1219: 
Final policy:

  [[2 1 1 2 2 2 2]
   [2 2 1 1 2 1 2]
   [1 2 1 1 2 2 2]
   [2 1 1 2 1 1 2]
   [1 1 2 2 2 1 2]
   [1 1 1 1 1 2 2]
   [1 1 1 1 1 1 3]]

  action['up'] = 0
  action['right'] = 1
  action['down'] = 2
  action['left'] = 3
 
 
Item 1221: 
IN:
[‘He’, ‘did’, ‘not’, ‘try’, ‘to’, ‘navigate’, ‘after’, ‘the’, ‘first’, ‘bold’, ‘flight’, ‘,’, ‘for’, ‘the’, ‘reaction’, ‘had’, ‘taken’, ‘something’, ‘out’, ‘of’, ‘his’, ‘soul’, ‘.’]
OUT:
[‘try’, ‘navigate’, ‘first’, ‘bold’, ‘flight’, ‘,’, ‘reaction’, ‘taken’, ‘something’, ‘soul’, ‘.’]
 
 
Item 1223: 
const Collection = require('dstools').Collection;
 data = Collection().loadCSV('/path/to/file/data/winemag-data-130k-v2.csv')
 data.head().show()
 data.head().table().save('my-table-html.html');
 data
.terms({field:'description'}) //extract terms in field description
.dropStopwords('term')//remove stopwords
.sortDesc('count') //sort by count or terms
.head(5)//choose top 5 terms
.show();//show it
 data.terms({field:'description'}).dropStopwords('term')
.sortDesc('count').head(50)
.wordCloud('term','count')//arguments are label and measure
.show();
 data
.terms({field:'description',groupBy:'variety',calc:'tfidf,idf'}) //calculate tfidf and idf
.dropStopwords('term')
.filterEqual('variety','Cabernet Sauvignon')
.sortDesc('tfidf') //sort by tfidf
.head(30)//top 30 terms
.show()
 data
.terms({field:'description',groupBy:'variety',calc:'tfidf,idf'})
.dropStopwords('term')
.filterEqual('variety','Cabernet Sauvignon')
.filter((term)=>term.idf>2)
.sortDesc('tfidf')
.head(50)
.wordCloud('term','tfidf',{title:'Word Cloud for Cabernet Sauvignon'})
.show()
 data
.column('description')//get a vector of wine's description field
.toLowerCase()//turn the descriptions into lower case
.merge()//merge all descriptions into one string
.save('wine-descriptions.txt')//save them into a file
 word2vec = require( 'word2vec' );
word2vec.word2vec('wine-descriptions.txt','wine-model.txt');
 
//load the model from file
word2vec.loadModel('wine-model.txt', function( err, model ) {
['blackberry','chocolate','tropical','mineral','green']//terms
.forEach((base)=>console.log(base + ': ' + 
//most similar function returns terms most similar to base
model.mostSimilar(base,10)
.map((term)=>term.word).join()));//show terms in list
});
 
 
Item 1224: 
Pearson's product-moment correlation

data:  hpi$gdp and hpi$life_expectancy
t = 9.3042, df = 138, p-value = 2.766e-16
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.5072215 0.7133067
sample estimates:
      cor 
0.6208781
 Pearson's product-moment correlation

data:  hpi$gdp and hpi$hpi_index
t = 1.3507, df = 138, p-value = 0.179
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.05267424  0.27492060
sample estimates:
      cor 
0.1142272
 
 
Item 1225: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1226: 
install.packages(c("foreign", "survey", "knitr"))
  library(foreign)
 library(survey)
 library(knitr)
 Apr17 <- read.spss("Apr17 public.sav", #file path to dataset 
                    to.data.frame = TRUE) #sets object to data frame
 ## re-encoding from CP1252
 table(Apr17$party)
  ## 
 ##                Republican                  Democrat 
 ##                       375                       466 
 ##               Independent      No preference (VOL.) 
 ##                       616                        28 
 ##        Other party (VOL.) Don't know/Refused (VOL.) 
 ##                         9                         7
 Apr17_design = svydesign(
         ids = ~0, #formula indicating there are no clusters  
         data = Apr17,      #this is the dataset 
         weights = ~weight) #this is the 'weight' variable
                            #from the Apr17 dataset
 svymean(~q1,                    #variable to estimate
         design =  Apr17_design #survey design object
                                #created with svydesign()
         )
  ##                                 mean     SE
 ## q1Approve                   0.394008 0.0144
 ## q1Disapprove                0.542368 0.0147
 ## q1Don't know/Refused (VOL.) 0.063624 0.0078
 q1_by_sex = svyby(~q1,            #variable to estimate
                   ~sex,          #subgroup variable
                   design = Apr17_design,
                   FUN = svymean, #function to use on each subgroup
                   keep.names = FALSE #does not include row.names 
                                       #for subgroup variable
      ) 
 
 knitr::kable(q1_by_sex, digits = 2)
 
 
Item 1230: 
if let img = CIImage(image: image){
     self.scanImage(image: img)
 }
 
 
Item 1232: 
Number of Observations : 768
Number of Features : 8
Input Neurons : 8
Output Neurons : 2 (Diabetic and Non-diabetic)
Test Data size : 20%
Train Data size : 80%
 
 
Item 1239: 
Original sentence : What can I do to improve my running skills?
 Augmented data generated :
* What can I do to advance my running skills?
* What can I do to better my running skills?
* What can I do to correct my running know-how?
* What can I do to correct my running proficiency?
 “I have a flying disk”
 [“I”, “have”, “a”, “flying”, “disk”]
 H(have)=[\#ha, hav, ave, ve\#]
 
 
Item 1241: 
python -m pysc2.bin.agent \
--map Simple64 \
--agent simple_agent.SimpleAgent \
--agent_race T
 
 
Item 1243: 
Product A: 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 4 4 5
 Product B: 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 4 4 5 
                             ^
                          Median
 Product C: 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4
 Product D: 2 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 5 5 5 
                             ^
                          Median
 Injuries: 1 1 1 1 1 1 1 1 2 2 2 3 3 5 8 16 40
                          ^
                        Median
Mean: 5.2
 
 
Item 1248: 
Fi0tYwluPWxvA5JoeXmoBGt2a2VyPXRlc3Q7cGFzc3dvcmC9VBNzNDU=.landoop.com
 “domain=landoop;user=spiros;password=123456” 
 
 
Item 1250: 
year     group    value
1        A        7
1        A        8.5
1        A        10
1        A        11.5
1        A        13
1        B        32
1        B        34
2        A        5.5
2        A        7
2        A        8.5
2        B        27
2        B        28.5
2        B        29.5
2        B        33
 
 
Item 1252: 
mutate(Month = str_c(Month, "-01"), Month = ymd(Month))
 mutate(Month = ymd(str_c(Month, "-01")))
 mutate(date_month = floor_date(Date, "month"))
 
 
Item 1265: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1266: 
graph TD
MB[Motorbike] -->|Needs| W(Wheels)
MB[Motorbike] -->|Needs| M(Motor)
 M(Motor) -->|Needs| E(Energy)
  E(Energy) -->|Is| F(Fuel)
  E(Energy) -->|Is| B(Battery)
 M(Motor) -->|Powers| W
 MB[Motorbike] -->|Needs| D(Driver)
  D(Driver) -->|Is| H(Human)
  D(Driver) -->|Is| C(Computer)
 
 
Item 1268: 
"90% of the data ever created was created in the last two years."
  This research employed data from the Seshat Databank (seshatdatabank.info) under Creative Commons Attribution Non-Commercial (CC By-NC SA) licensing.Turchin P. et al. 2015. “Seshat: The Global History Databank.” Cliodynamics 6(1): 77–107Mullins, D., Hoyer, D. et al. 2018. “A Systematic Assessment of ‘Axial Age’ Proposals Using Global Comparative Historical Evidence.” American Sociological Review 83(3): 596–626
 
 
Item 1271: 
a = array([[ 12., -22., -20., -19.,  -3.],
       [-23.,  21., -17., -11.,  -1.],
       [ -4.,  -5.,  16.,  -9., -14.],
       [-10.,  -6., -18.,  15.,  -8.],
       [-25.,  -2., -13.,  -7.,  24.]])
 sum_matrix = a.sum(axis=1)
 array([-52., -31., -16., -27., -23.])
 arr = np.arange(12).reshape(4,3)
 array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 9, 10, 11]])
 df = pd.DataFrame(data=np.arange(12).reshape(4,3),index=['row1','row2','row3','row4'],columns=['col1','col2','col3'])
 three_d_array = np.arange(24).reshape(4,3,2)
 array([[[ 0,  1],
        [ 2,  3],
        [ 4,  5]],

       [[ 6,  7],
        [ 8,  9],
        [10, 11]],

       [[12, 13],
        [14, 15],
        [16, 17]],

       [[18, 19],
        [20, 21],
        [22, 23]]])
 three_d_array[0]+three_d_array[1]+three_d_array[2]+three_d_array[3]
 array([[36, 40],
       [44, 48],
       [52, 56]])
 array([[ 6,  9],
       [24, 27],
       [42, 45],
       [60, 63]])
 array([[ 1,  5,  9],
       [13, 17, 21],
       [25, 29, 33],
       [37, 41, 45]])
 a = array([[ 12., -22., -20., -19.,  -3.],
       [-23.,  21., -17., -11.,  -1.],
       [ -4.,  -5.,  16.,  -9., -14.],
       [-10.,  -6., -18.,  15.,  -8.],
       [-25.,  -2., -13.,  -7.,  24.]])
 
 
Item 1272: 
form.addScaleItem()
.setTitle(dataToLabel)                    
.setBounds(1, 10)        
.setRequired(true);
 form.addCheckboxItem()
.setTitle(dataToLabel)
.setChoices([
           item.createChoice('Is a cat')
           ])
 form.addMultipleChoiceItem()
.setTitle(dataToLabel)
.setChoices([
         item.createChoice('Cats'),
         item.createChoice('Dogs'),
         item.createChoice('Fish')   
])
 
 
Item 1276: 
['Villa', 'de', 'm', 'chambres', 'salles', 'de', 'bain', 'Beaucoup', 'de', 'charme', 'Au', 'calme', 'absolu', 'proche', 'du', 'centre', 'ville', 'et', 'de', 'toutes', 'commodites', 'cour', 'interieure', 'terrasse', 'de', 'm', 'Belle', 'exposition', 'Sud', 'vue', 'panoramique', 'sur', 'mer']
 [('propriete', 0.7699100971221924),
 ('demeure',   0.7666690349578857),
 ('longere',   0.7325247526168823),
 ('batisse',   0.714929461479187),
 ('villa',     0.7136778831481934)]
 [('quercynoise', 0.8929086923599243),
 ('vigneronne',  0.8886078596115112),
 ('charentaise', 0.8866457939147949),
 ('agricoles',   0.8861927390098572),
 ('matha',       0.8771463632583618)]
 [('loggia',  0.7683628797531128),
 ('balcon',  0.765031099319458),
 ('cour',    0.7153904438018799),
 ('veranda', 0.6810243725776672),
 ('donnant', 0.6466798186302185)]
 [('chauffee',   0.8212169408798218),
 ('pool',       0.7625634670257568),
 ('interieure', 0.7517303824424744),
 ('couverte',   0.7392628192901611),
 ('jacuzzi',    0.7313171625137329)]
 [('clair',       0.7369235157966614),
 ('agreable',    0.6812050342559814),
 ('entretenu',   0.6783103942871094),
 ('absolu',      0.6680794358253479),
 ('ensoleillee', 0.6667557954788208)]
 [('degagee',    0.9124640226364136),
 ('imprenable', 0.8999924659729004),
 ('dominante',  0.8524136543273926),
 ('mer',        0.8523256778717041),
 ('splendide',  0.7981457710266113)]
 'piscine'
 'colmar'
 
 
Item 1277: 
Regression Formula:
Predicted = -1065201.67 + Grade*209786.29
 R-squared = 0.46
 price = a * grade + b * grade^2 + c
 Regression Formula:
Predicted = 1639674.31 + Grade*-473161.41 + Grade2*42070.46
 R-squared = 0.52
 598824.2200000001 USD
 
 
Item 1279: 
docker run -p 7474:7474 -p 7687:7687 ryguyrg/neo4j-paradise-papers
 FROM neo4j:3.3.0
MAINTAINER Ryan Boyd, <ryan@neo4j.com>
 RUN apk update
RUN apk add --quiet openssl sed wget unzip
RUN cd /var/lib/neo4j/plugins/; wget "https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/3.3.0.1/apoc-3.3.0.1-all.jar"
RUN cd /var/lib/neo4j/plugins/; wget "https://github.com/neo4j-contrib/neo4j-graph-algorithms/releases/download/3.3.0.0/graph-algorithms-algo-3.3.0.0.jar"
 COPY download_db.sh /download_db.sh
COPY configure.cql /configure.cql
 ENV EXTENSION_SCRIPT /download_db.sh
ENV NEO4J_AUTH=none
 
 
Item 1281: 
add filename
 git init
 git status 
 touch .gitignore
 #to stage specific modified files
git add filename
 #to stage all modified files
git add .
 git commit -m 'describe change(s) made here' 
 git log
 git show commit-id
 git reset --hard commit-id
 git branch
 git branch new-branch-name
 git checkout another-branch
 git checkout -b new-branch-name
 git checkout master
 git pull
 git merge feature-branch
 git branch -d feature-branch
 git clone filepath
 git clone repository
 git checkout -b branch
 #ensure local version of master is up-to-date
git pull --rebase origin master
 #push to remote branch
git push origin branch
 git pull --rebase origin master
 
 
Item 1282: 
train_carts = (order_products_train_df.groupby('user_id',as_index=False)
                                      .agg({'product_id':(lambda x: set(x))})
                                      .rename(columns={'product_id':'latest_cart'}))
 df_X = df_X.merge(train_carts, on='user_id')
df_X['in_cart'] = (df_X.apply(lambda row: row['product_id'] in row['latest_cart'], axis=1).astype(int))
 lr_balanced = LogisticRegression(class_weight='balanced', C=1000000)
 
 
Item 1284: 
logistic_regression = LogisticRegression()
logistic_regression.fit(X_train, y_train)
y_pred = logistic_regression.predict(X_test)
logistic_regression.score(X_train, y_train)
 knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
knn.score(X_train, y_train)
 random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(X_train, y_train)
y_pred = random_forest.predict(X_test)
random_forest.score(X_train, y_train)
 
 
Item 1286: 
gnuplot> f(x,y) = (-5 - 2*x + y > 0)? 1 : 1/0
gnuplot> unset colorbox
gnuplot> set isosample 300, 300
gnuplot> set sample 300
gnuplot> set pm3d map
gnuplot> splot [-5:5] [-5:5] f(x,y)
 gnuplot> f(x,y) = (x > 3? (y > 1? 1 : 1/0) : (y > 3? 1/0: 1))
gnuplot> unset colorbox
gnuplot> set isosample 300, 300
gnuplot> set sample 300
gnuplot> set pm3d map
gnuplot> splot [0:5] [0:5] f(x,y)
 output = input > 0 ? input : 0
 linear_output = w0 + w1*x + w2*y
 relu_output = linear_output > 0 ? linear_output : 0
 output_without_relu
   = v0 + v1*(w0 + w1*x + w2*y) + v2*(u0 + u1*x + u2*y)
   = k0 + k1*x + k2*y
 where,
  k0 = v0 + v1*w0 + v2*u0
  k1 = v1*w0
  k2 = v2*u0
 gnuplot> f(x,y) = y > 0 ? ((-5 - 2*x + y > 0)? 1 : 1/0) : 1/0
gnuplot> unset colorbox
gnuplot> set isosample 300, 300
gnuplot> set sample 300
gnuplot> set pm3d map
gnuplot> splot [-5:5] [-5:5] f(x,y)
 gnuplot> f(x,y) = (y > 0 ? ((-4 -2*x + y > 0)?1:1/0) : 1/0) + (y>0?((4 + 2*x + y) > 0 ? 1: 1/0 ): 1/0) + (y>0?((4 - x - 2*y)>0? 1: 1/0):1/0) + (y>0?((9 + 2*x - y)>0 ? 1: 1/0): 1/0)
gnuplot> unset colorbox
gnuplot> set isosample 300, 300
gnuplot> set sample 300
gnuplot> set pm3d map
gnuplot> splot [-5:5] [-5:5] f(x,y)
 
 
Item 1288: 
## Error: stat_count() must not be used with a y aesthetic.
 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
 ## Make a box and whisker plot for each column of ``x`` or each vector in sequence ``x``.
 
 
Item 1298: 
dbms.security.procedures.whitelist=algo.*,apoc.*,regression.*
 MATCH (hood:Neighborhood {neighborhood_id:'78704'})
CREATE (list:Listing 
      {listing_id:'12345', bedrooms:4, bathrooms:2, accommodates:5}) 
MERGE (list)-[:IN_NEIGHBORHOOD]->(hood)
 MATCH (list:Listing)-[:IN_NEIGHBORHOOD]->(:Neighborhood {neighborhood_id:'78704'}) 
WHERE exists(list.bedrooms) AND exists(list.bathrooms)
AND exists(list.price) 
WITH regression.linear.split(collect(id(list)), 0.75) AS trainingIDs
MATCH (list:Listing) WHERE id(list) in trainingIDs 
SET list:Train
 MATCH (list:Listing)-[n:IN_NEIGHBORHOOD]->(hood:Neighborhood {neighborhood_id:'78704'})
WHERE exists(list.bedrooms) AND exists(list.bathrooms)
AND exists(list.price) AND NOT list:Train SET list:Test
 create('model-name', 'model-type' = 'Simple',
       constantTerm? = true, noOfIndependentVars = 1)
 MATCH (list:Listing:Train) WHERE NOT list:Seen 
CALL regression.linear.add('rental prices',
     [list.bedrooms+list.bathrooms], list.price) 
SET list:Seen
 CALL regression.linear.info('rental prices')
 MATCH (list:Listing:Test) WHERE NOT list:Seen 
CALL regression.linear.add('rental prices', 
               [list.bedrooms + list.bathrooms], list.price, 'test') 
SET list:Seen
 CALL regression.linear.test('rental prices')
 RETURN regression.linear.predict('rental prices', [6])
 MATCH (list:Listing)-[:IN_NEIGHBORHOOD]->
      (:Neighborhood {neighborhood_id:'78704'})
WHERE exists(list.bedrooms)and exists(list.bathrooms) 
  AND NOT exists(list.price)
 WITH list, list.bedrooms + list.bathrooms as rooms
SET list.predicted_price = 
  regression.linear.predict('rental prices', [rooms])
RETURN DISTINCT rooms, list.predicted_price AS predictedPrice 
ORDER BY rooms
 CALL regression.linear.create('popular rental prices', 'Simple')
 CALL regression.linear.copy('rental prices', 
                            'popular rental prices')
 MATCH (list:Listing:Train) 
WHERE NOT (:Review)-[:REVIEWS]->(list) 
CALL regression.linear.remove('popular rental prices',  
       [list.bedrooms+list.bathrooms], list.price) 
REMOVE list:Train, list:Seen
 MATCH (list:Listing:Test) WHERE NOT (:Review)-[:REVIEWS]->(list) REMOVE list:Test
 MATCH (list:Listing:Test) 
CALL regression.linear.add('popular rental prices', 
              [list.bedrooms + list.bathrooms], list.price, 'test') 
RETURN count(list)
 CALL regression.linear.test('popular rental prices')
 MATCH (list:Listing)-[:IN_NEIGHBORHOOD]->
      (:Neighborhood {neighborhood_id:’78704'})
WHERE exists(list.bedrooms)and exists(list.bathrooms) 
      AND NOT exists(list.price)
 WITH list, list.bedrooms + list.bathrooms AS rooms
SET list.predicted_price = 
      regression.linear.predict('popular rental prices', [rooms])
 RETURN DISTINCT rooms, list.predicted_price AS predictedPrice 
ORDER BY rooms
 MERGE (m:ModelNode {model: 'popular rental prices'})
SET m.data = regression.linear.data('popular rental prices')
 CALL regression.linear.delete('popular rental prices')
 MATCH (m:ModelNode {model: 'popular rental prices'})
CALL regression.linear.load(m.model, m.data, 'Simple')
YIELD model, framework, hasConstant, numVars, state, nTrain, nTest, trainInfo, testInfo
RETURN *
 
 
Item 1300: 
['tooth', u'retina', u'X_ray', u'light']
['John', u'George', u'Frank_Sinatra', u'Wonderful', u'perfect']
['pillow', u'plastic_bag', u'pickup_truck', u'car']
 
 
Item 1303: 
docker-machine create --digitalocean-size "s-2vcpu-4gb" --driver digitalocean --digitalocean-access-token PERSONAL_ACCESS_TOKEN facebox-prod-1
 docker-machine ls
 facebox-prod-1 * digitalocean Running tcp://152.63.254.5:2376 v18.05.0-ce
 eval $(docker-machine env facebox-prod-1)
 docker run -d -p 80:8080 -e "MB_WORKERS=2" -e "MB_KEY=$MB_KEY" machinebox/facebox
 docker-machine ip facebox-prod-1
152.65.253.2
 
 
Item 1309: 
parms = {‘max_depth’:12, #maximum depth of a tree
 ‘objective’:’reg:linear’,
 ‘eta’ :0.3,
 ‘subsample’:0.8,#SGD will use this percentage of data
 ‘lambda ‘ :4, #L2 regularization term,>1 more conservative 
 ‘colsample_bytree ‘:0.9,
 ‘colsample_bylevel’:1,
 ‘min_child_weight’: 10,
 ‘nthread’ :3} #number of cpu core to use
 
 
Item 1314: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1324: 
install.packages('sentimentr')
#or
library(devtools)
install_github('trinker/sentimentr')
 sentiment_by('I am not very good', by = NULL)
 element_id sentence_id word_count   sentiment
1:          1           1          5 -0.06708204
 sentiment('I am not very good. He is very good')
 element_id sentence_id word_count   sentiment
1:          1           1          5 -0.06708204
2:          1           2          4  0.67500000
 'My life has become terrible since I met you and lost money' %>% extract_sentiment_terms()
   element_id sentence_id      negative positive
1:          1           1 terrible,lost    money
 'My life has become terrible since I met you and lost money. But I still have got a little hope left in me' %>% 
  sentiment_by(by = NULL) %>%
  highlight()
 
 
Item 1326: 
with tf.name_scope("variables_scope"):
    x = tf.placeholder(tf.float32, shape=[None, 784], name="x_placeholder")
    y_ = tf.placeholder(tf.float32, shape=[None, 10], name="y_placeholder")
 with tf.name_scope("weights_scope"):
    W = tf.Variable(tf.zeros([784, 10]), name="weights_variable")
    tf.summary.histogram("weight_histogram", W)
 
 
Item 1327: 
#PSEUDOCODE
i = randomnumber()
fetch_item = url + ‘SHOE_’ + i
new_item = httprequest(fetch_item)
 batch.append(new_item)
 
 
Item 1330: 
Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) 227.779544   0.481849 472.720  < 2e-16 ***
days         -0.023887   0.006326  -3.776 0.000264 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
 Residual standard error: 3.861 on 105 degrees of freedom
Multiple R-squared:  0.1196, Adjusted R-squared:  0.1112 
F-statistic: 14.26 on 1 and 105 DF,  p-value: 0.0002642
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  131.9   3.133e-01  420.843   <2e-16 ***
days         0.0095  3.808e-03   2.388   0.0185 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
 Residual standard error: 2.679 on 121 degrees of freedom
Multiple R-squared:  0.04502, Adjusted R-squared:  0.03713 
F-statistic: 5.704 on 1 and 121 DF,  p-value: 0.01847
 
 
Item 1334: 
              + good - bad      - good + bad
scheme                plan              scam
intelligence     knowledge               spy
naive           idealistic            stupid
possibility      potential      consequences
dream                 wish         nightmare
notion             concept        stereotype
bold            courageous             risky    
                 + fun - boring    - fun + boring
presentation          showcase        powerpoint
woman                     girl         housewife
acquaintance            friend          coworker
music                      pop        orchestral
test                 challenge              exam
premiere              festival             opera
              + elegant - clumsy    - elegant + clumsy
persuade               convince            pressuring
walked                 strolled              stumbled
threw                    tossed                hurled
placed                  adorned            mishandled
stylish                   sleek            uninspired
 
 
Item 1335: 
wget -O pokemon.min.json https://gist.githubusercontent.com/yi-jiayu/ea4f0a6c5b9734dd82920cf739b1f707/raw/79dabce87503050d4d64d0332e2076e4d5485171/pokemon.min.json
 Invoke-WebRequest -OutFile pokemon.min.json https://gist.githubusercontent.com/yi-jiayu/ea4f0a6c5b9734dd82920cf739b1f707/raw/79dabce87503050d4d64d0332e2076e4d5485171/pokemon.min.json
 [
  {
    "abilities": [
      "Overgrow"
    ],
    "detailPageURL": "/us/pokedex/bulbasaur",
    "weight": 15.2,
    "weakness": [
      "Fire",
      "Flying",
      "Ice",
      "Psychic"
    ],
    "number": "001",
    "height": 28.0,
    "collectibles_slug": "bulbasaur",
    "featured": "true",
    "slug": "bulbasaur",
    "name": "Bulbasaur",
    "ThumbnailAltText": "Bulbasaur",
    "ThumbnailImage": "https://assets.pokemon.com/assets/cms2/img/pokedex/detail/001.png",
    "id": 1,
    "type": [
      "grass",
      "poison"
    ]
  },
...
 *bold text*
_italic text_
[inline URL](http://www.example.com/)
[inline mention of a user](tg://user?id=123456789)
`inline fixed-width code`
```block_language
pre-formatted fixed-width code block
```
 *Phanpy (#231)*
Type: Ground
Abilities: Pickup
Height: 1' 8"
Weight: 73.9 lbs
[Image](https://assets.pokemon.com/…truncated…/231.png)
 {
    "update_id":646911422,
    "inline_query":{
        "id":"432549315453598698",
        "from":{
            "id":10000xxxx,
            "is_bot":false,
            "first_name":"Jiayu",
            "username":"jiayu",
            "language_code":"en-US"
        },
        "query":"eon",
        "offset":""
    }
}
 [
  {
    "type": "article",
    "id": "phanpy",
    "title": "Phanpy (#231)",
    "description": "Ground",
    "thumb_url": "https://…TRUNCATED…/231.png",
    "input_message_content": {
      "message_text": "*Phanpy (#231)*\nType: Ground\n…TRUNCATED…",
      "parse_mode": "markdown"
    }
  }
]
 {
  "inline_query_id": "432549315453598698",
  "results": "[{\"type\":\"article\",\"id\":\"phanpy\",\"title\":\"Phanpy (#231)\",\"description\":\"Ground\",\"thumb_url\":\"https://…TRUNCATED…/231.png\",\"input_message_content\":{\"message_text\":\"*Phanpy (#231)*\\nType: Ground\\n…TRUNCATED…\",\"parse_mode\":\"markdown\"}}]"
}
 rotom-pokedex-bot/
|- index.js
|- pokemon.min.json
 {
  "name": "rotom-pokedex-bot",
  "version": "0.1.0",
  "main": "index.js"
}
 rotom-pokedex-bot/
|- index.js
|- pokemon.min.json
|- package.json
 PS> Compress-Archive -Path * -DestinationPath rotom-pokedex-bot.zip
 gcloud --project YOUR-PROJECT-NAME beta functions deploy YOUR-FUNCTION-NAME --entry-point handler --trigger-http
 
 
Item 1341: 
def _equal_var_ttest_denom(v1, n1, v2, n2):    
    df = n1 + n2 - 2.0    
    svar = ((n1 - 1) * v1 + (n2 - 1) * v2) / df    
    denom = np.sqrt(svar * (1.0 / n1 + 1.0 / n2))    
    return df, denom
 
 
Item 1351: 
POST /suggestionbox/models
{
  "id": "model1",
  "name": "Story recommendations",
  "choices": [
    {
      "id": "article1",
      "features": [
        {
          "key": "title",
          "type": "text",
          "value": "Machine Box releases new box"
        }
      ] 
   },
   {
     "id": "article2",
     "features": [
       {
         "key": "title",
         "type": "text",
         "value": "Elon Musk actually an AI"
       }
     ] 
   },
   ...
  ]
}
 POST /suggestionbox/models/model1/predict
{
  "inputs": [
    {
      "key": "user_age",
      "type": "number",
      "value": "25"
    },
    {
      "key": "user_interests",
      "type": "list",
      "value": "ml,golang,music,cats"
    }
  ]
} 
 {
  "choices": [
    {
      "id": "article3",
      "score": 0.60,
      "reward_id": "5a71d78495864f25c02089e154bc2e30"
    },
    {
      "id": "article1",
      "score": 0.30,
      "reward_id": "5a71d7845587d4b715aa3a0e95842dc8"
    },
    {
      "id": "article2",
      "score": 0.10,
      "reward_id": "d789586445587d89e154cbc2e34b715a"
    }
  ]
}
 POST /suggestionbox/models/model1/rewards
{
  "reward_id": "5a71d78495864f25c02089e154bc2e30",
  "value": 1
}
 POST /suggestionbox/models
{
  "id": "model2",
  "name": "Sushi pics",
  "choices": [
    {
      "id": "california-roll",
      "features": [
        {
          "key": "pic",
          "type": "image_url",
          "value": "https://url.com/to/pic1.jpg"
        }
      ] 
   },
   {
     "id": "sashimi",
     "features": [
       {
         "key": "pic",
         "type": "image_url",
         "value": "https://url.com/to/pic2.jpg"
       }
     ] 
   },
   ...
  ]
}
 POST /suggestionbox/models
{
  "id": "model3",
  "name": "Music genres",
  "choices": [
    {
      "id": "pop"
    },
    {
      "id": "rock"
    },
    {
      "id": "metal"
    },
    {
      "id": "indie"
    },
    ...
  ]
}
 docker run -p 8080:8080 -e "MB_KEY=$MB_KEY" machinebox/suggestionbox 
 
 
Item 1352: 
| gentimes start=”01/01/2018" increment=1h
| eval _time=starttime, loc=0, scale=20
| normal loc=loc scale=scale 
| streamstats count as cnt
| eval gen_normal = gen_normal + cnt
| table _time, gen_normal
| rename gen_normal as “Non-stationary time series (trend)”
 | inputlookup daylogins.csv
| timechart span=1d fixedrange=f max(daylogins) as daylogins
| predict “daylogins” as prediction algorithm=”LLT” future_timespan=365 period=365 lower”95"=lower”95" upper”95"=upper”95"
| eval daylogins = case(daylogins < ‘lower95(prediction)’, ‘lower95(prediction)’, daylogins > ‘upper95(prediction)’, ‘upper95(prediction)’, 1=1, daylogins)
| eval season = daylogins / prediction
| table _time, season
| where _time<strptime(“2018–01–01”, “%F”)
 | inputlookup daylogins.csv 
| timechart span=1d fixedrange=f max(daylogins) as daylogins 
| predict "daylogins" as prediction algorithm="LLT" future_timespan=365 lower"95"=lower"95" upper"95"=upper"95" 
| eval daylogins = case(daylogins < 'lower95(prediction)', 'lower95(prediction)', daylogins > 'upper95(prediction)', 'upper95(prediction)', 1=1, daylogins) 
| eval season = daylogins / prediction 
| predict "season" as season_prediction algorithm="LLP" future_timespan=365 period=365 lower"0"=lower"0" upper"0"=upper"0"
| eval prediction = prediction * season_prediction
| table _time, daylogins, prediction
| eval prediction = if(isnull(daylogins), prediction, null)
| where _time<strptime("2019-01-01", "%F")
 
 
Item 1355: 
twitter_token<-create_token(app = "joysentiment1", api_key, api_secret, accessToken, accessTokenSecret, set_renv = TRUE)
 em_tweets<-get_timeline("ElonMusk",n=5000)
 ggplot(data = em_tweets,
aes(x=em_tweets$month,group=factor(year(created_at)), color=factor(year(created_at)))) +
geom_line(stat="count") +
geom_point(stat="count") +
labs(x="Month", colour="Year",y="Number of tweets") +
theme_classic()
 ggplot(data = em_tweets, aes(x = em_tweets$month)) +
geom_bar(aes(fill = ..count..)) +
xlab("Month") + ylab("Number of tweets") +
theme_classic()
 ggplot(data=primaryscores,aes(x=sentiment,y=Score))+
geom_bar(aes(fill=sentiment),stat = "identity")+
xlab("Sentiments")+ylab("Scores")+
ggtitle("Total sentiment based on scores")+
theme(legend.position="none")+
theme_minimal()
 ggplot(data=monthly_sentiment,aes(x=month,y=value,group=sentiment))+
geom_line(aes(color=factor(sentiment)))+
geom_point(aes(color=factor(sentiment)))+
labs(x="Month", colour="Sentiment",y="Value")+
theme_minimal()
 wordcloud(em_text_corpus,min.freq=1,max.words=50,scale=c(2.5,1),colors=brewer.pal(8,"Accent"),random.color=T,random.order=F)
 elon_raw<-searchTwitter("elon",lang="en",n=1000)
 wordcloud(elon_text_corpus,min.freq=1,max.words=50,scale=c(2.5,1),colors=brewer.pal(8,"Accent"),random.color=T,random.order=F)
 library(wordcloud)
library(httr)
library(rtweet)
library(twitteR)
library(plyr)
library(ggplot2)
library(devtools)
library(tm)
library(dplyr)
library(stringr)
library(tidytext)
library(lubridate)
 
 
Item 1356: 
pred = x1 * (Content-Based Engine) + x2 * (Collab-Filtering Engine)
 
 
Item 1360: 
Nope. I know a little thai and some spanish. haha go figure.Im seriously the whitest korean chick you'd ever meet :]
Idk I think it swings both ways pretty strongly. The human race is never satisfied and I believe that both sexes have 
the tendencey to cheat. At all ages and for all reasons.
Timmy. He held me down and tickled me
im not the one who called you a whore either haha. i play soccer and run track and did dance for eight years and did 
gymnastics
What would a chair look like if your knees bent the other way?
formspringg haha noo..my boyfriend. Were always out staying busy.
Would you rather be rich or famous?
crude?
Haha youu need a phone !(:
you should ask her if shes still 
r Has anybody ever told you that!
 
 
Item 1361: 
Classify new instance
If model_Output == .995 || model_Output == .005
  save new instance to train data and label accordingly
 
 
Item 1368: 
00000001 00000000 00000001
 00000101
 00000001 00000010 00000011 00000000
 00111001
 Initial: 7, 7, 7, 13, 13, 13, 2345, 42, 42, 7, 13, 6543, 7
 =>
 Index:    1, 1, 1, 2, 2, 2, 0, 3, 3, 1, 2, 0, 1
Frequent: 7, 13, 42
Other:    2345, 6543
 Initial: 23, 45, null, null, null, null, null, 3
 =>
 Index:  1, 1, 0, 0, 0, 0, 0, 1
Values: 23, 45, 3
 
 
Item 1369: 
2015 terror deaths EU: 171.0 total, or 0.23 per million residents
 2015 terror deaths US: 44.0 total, or 0.14 per million residents
 suicide     22060
homicide    13018
accident      489
other         284
 2015 gun homicides US: 13018 total, or 40.29 per million residents
 2015 gun homicides US (white, over 30, college degree): 392 total, or 1.21 per million residents
 
 
Item 1373: 
df %>% 
    separate(coordinates, into = c("lat", "long"), sep = "\\s*\\,\\s*", convert = TRUE)
 if_else(show  == "No Reservations" & season  == 2 & ep  == 4,
"Quebec", title)
 df %>%
    separate(coordinates, into = c("lat", "long"), sep = "\\s*\\,\\s*", convert = TRUE) %>%
    mutate(title = if_else(show  == "No Reservations" & season  == 2 & ep  == 4,
"Quebec", title))
 df %>%
    separate(coordinates, into = c("lat", "long"), sep = "\\s*\\,\\s*", convert = TRUE) %>%
    mutate(title = if_else(show  == "No Reservations" & season  == 2 & ep  == 4,
"Quebec", title), 
           country = recode(country, California = "United States", Massachusetts = "United States", Washington = "United States", Oregon = "United States", Kansas = "United States", Missouri = "United States", Sicily = "Italy"))
 df %>%
    separate(coordinates, into = c("lat", "long"), sep = "\\s*\\,\\s*", convert = TRUE) %>%
    mutate(title = if_else(show  == "No Reservations" & season  == 2 & ep  == 4,
"Quebec", title), 
           country = recode(country, California = "United States", Massachusetts = "United States", Washington = "United States", Oregon = "United States", Kansas = "United States", Missouri = "United States", Sicily = "Italy"),
           country_countrycode = countrycode(country, origin="country.name", destination="continent"))
 countrycode(country, origin="country.name", destination="continent")
 df %>%
    separate(coordinates, into = c("lat", "long"), sep = "\\s*\\,\\s*", convert = TRUE) %>%
    mutate(title = if_else(show  == "No Reservations" & season  == 2 & ep  == 4,
"Quebec", title), 
           country = recode(country, California = "United States", Massachusetts = "United States", Washington = "United States", Oregon = "United States", Kansas = "United States", Missouri = "United States", Sicily = "Italy"),
           country_countrycode = countrycode(country, origin="country.name", destination="continent")) %>%
    unite(show_season_ep_title, show, season, ep, title, sep = "_", remove = FALSE)
 
 
Item 1377: 
for layer in model.layers:
    print(layer.name, layer.trainable)
    print('Layer Configuration:')
    print(layer.get_config(), end='\n{}\n'.format('----'*10))
 bidirectional_2 True
Layer Configuration:
{'name': 'bidirectional_2', 'trainable': True, 'layer': {'class_name': 'LSTM', 'config': {'name': 'lstm_2', 'trainable': True, 'return_sequences': False, 'go_backwards': False, 'stateful': False, 'unroll': False, 'implementation': 0, 'units': 64, 'activation': 'tanh', 'recurrent_activation': 'hard_sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'scale': 1.0, 'mode': 'fan_avg', 'distribution': 'uniform', 'seed': None}}, 'recurrent_initializer': {'class_name': 'Orthogonal', 'config': {'gain': 1.0, 'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'unit_forget_bias': True, 'kernel_regularizer': None, 'recurrent_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'recurrent_constraint': None, 'bias_constraint': None, 'dropout': 0.0, 'recurrent_dropout': 0.0}}, 'merge_mode': 'concat'}
 trained_model.layers[i].get_weights()
 
 
Item 1381: 
quality = number_of_amenities * number_of_reviews * review_scores_rating
 
 
Item 1383: 
install.packages("name_of_the_package")
 install.packages("tidyverse")
 package ‘tidyverse’ successfully unpacked...
 library(name_of_the_package)
 library(tidyverse)
 > library(tidyverse)
-- Attaching packages --------------------------------------- tidyverse 1.2.1 --
v ggplot2 3.0.0     v purrr   0.2.5
v tibble  1.4.2     v dplyr   0.7.6
v tidyr   0.8.1     v stringr 1.3.1
v readr   1.1.1     v forcats 0.2.0
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
Warning messages:
1: package ‘tidyverse’ was built under R version 3.4.4 
2: package ‘ggplot2’ was built under R version 3.4.4 
3: package ‘tidyr’ was built under R version 3.4.4 
4: package ‘purrr’ was built under R version 3.4.4 
5: package ‘dplyr’ was built under R version 3.4.4 
6: package ‘stringr’ was built under R version 3.4.4
 draft_data <- read_rds("2018_nhl_draft_data.rds")
 > draft_data
# A tibble: 217 x 16
   draft_league draft_year pick_number round draft_team name 
   <chr>        <chr>            <dbl> <dbl> <chr>      <chr>
 1 NHL Entry D~ 2018                 1     1 Buffalo S~ Rasm~
 2 NHL Entry D~ 2018                 2     1 Carolina ~ Andr~
 3 NHL Entry D~ 2018                 3     1 Montréal ~ Jesp~
 4 NHL Entry D~ 2018                 4     1 Ottawa Se~ Brad~
 5 NHL Entry D~ 2018                 5     1 Arizona C~ Barr~
 6 NHL Entry D~ 2018                 6     1 Detroit R~ Fili~
 7 NHL Entry D~ 2018                 7     1 Vancouver~ Quin~
 8 NHL Entry D~ 2018                 8     1 Chicago B~ Adam~
 9 NHL Entry D~ 2018                 9     1 New York ~ Vita~
10 NHL Entry D~ 2018                10     1 Edmonton ~ Evan~
# ... with 207 more rows, and 10 more variables:
#   position <chr>, shot_handedness <chr>, birth_place <chr>,
#   birth_country <chr>, birthday <chr>, height <dbl>,
#   weight <dbl>, age <dbl>, player_url <chr>,
#   player_statistics <list>
 goals <- c(16, 27, 25)
 assists <- c(37, 20, 19)
 > points <- goals + assists
> points
[1] 53 47 44
 > class(c(1, 2, 3))
[1] "numeric"
> class(c(TRUE, FALSE))
[1] "logical"
> class(c("casey mittelstadt is not that amazing", "TRUE"))
[1] "character"
 > goals <- c(16, 27, 25)
> goals
[1] 16 27 25
> names <- c("Mats Zuccarello", "Mika Zibanejad", "Kevin Hayes")
> names
[1] "Mats Zuccarello" "Mika Zibanejad" 
[3] "Kevin Hayes"    
> goals + names
Error in goals + names : non-numeric argument to binary operator
 > draft_data
# A tibble: 217 x 16
   draft_league draft_year pick_number round draft_team name 
   <chr>        <chr>            <dbl> <dbl> <chr>      <chr>
 1 NHL Entry D~ 2018                 1     1 Buffalo S~ Rasm~
 2 NHL Entry D~ 2018                 2     1 Carolina ~ Andr~
 3 NHL Entry D~ 2018                 3     1 Montréal ~ Jesp~
 4 NHL Entry D~ 2018                 4     1 Ottawa Se~ Brad~
 5 NHL Entry D~ 2018                 5     1 Arizona C~ Barr~
 6 NHL Entry D~ 2018                 6     1 Detroit R~ Fili~
 7 NHL Entry D~ 2018                 7     1 Vancouver~ Quin~
 8 NHL Entry D~ 2018                 8     1 Chicago B~ Adam~
 9 NHL Entry D~ 2018                 9     1 New York ~ Vita~
10 NHL Entry D~ 2018                10     1 Edmonton ~ Evan~
# ... with 207 more rows, and 10 more variables:
#   position <chr>, shot_handedness <chr>, birth_place <chr>,
#   birth_country <chr>, birthday <chr>, height <dbl>,
#   weight <dbl>, age <dbl>, player_url <chr>,
#   player_statistics <list>
 ?class
 > class(goals)
[1] "numeric"
> goals %>% class()
[1] "numeric"
 > made_up_vector <- c(1, 2, 3, NA)
> made_up_vector
[1]  1  2  3 NA
> mean(made_up_vector)
[1] NA
> mean(made_up_vector, na.rm = TRUE)
[1] 2
 > mean(made_up_vector, na.rm = TRUE)
[1] 2
> class(mean(made_up_vector, na.rm = TRUE))
[1] "numeric"
 made_up_vector %>%
  mean(na.rm = TRUE) %>%
  class()
 [1] "numeric"
 mydata %>%
  ggplot(aes(...)) +
  geom_...()
 draft_data %>%
  ggplot(aes(x = height, y = weight)) +
  geom_point()
 draft_data %>%
  ggplot(aes(x = height, y = weight)) +
  geom_point() +
  labs(x = "Height", 
       y = "Weight", 
       title = "Height & Weight for 2018 Drafted Players",
       subtitle = "Data from EliteProspects",
       caption = "Evan Oppenheimer")
 draft_data %>%
  ggplot(aes(x = height, y = weight)) +
  geom_point() +
  labs(x = "Height", 
       y = "Weight", 
       title = "Height & Weight for 2018 Drafted Players",
       subtitle = "Data from EliteProspects",
       caption = "Evan Oppenheimer") +
  theme_bw()
 draft_data %>%
  ggplot(aes(x = height, y = weight)) +
  geom_point() +
  labs(x = "Height", 
       y = "Weight", 
       title = "Height & Weight for 2018 Drafted Players",
       subtitle = "Data from EliteProspects",
       caption = "Evan Oppenheimer") +
  theme_bw() +
  geom_smooth()
 
 
Item 1385: 
docker run -p 8080:8080 -e "MB_KEY=$MB_KEY" machinebox/classificationbox
 
 
Item 1386: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1389: 
fn <- function(a) {a*a}
 fn(10)
 fn -> function(a, b) {
  c = a * b
  c = c + b
  print(x)
}
 fn(10, 20)
 primeNumber = function(n) {
   if(n>=2) {
     s = seq(2,n) 
    p = c() #Initialising the vector which stores prime numbers
     for(i in seq(2,n)) {
      if(any(s == i) {
        p = c(p, i)
        s = c(s[(s%%i) != 0], i)
      }
    }
  
    return(p)
  } else {
    stop("Input greater than 2")
  }
}
 
 
Item 1390: 
L | L | R | R | NA | NA | L | NA | NA | …. | L | R |
# The Cardinality of this Sequence could be something like 1 million decisions results in complete victory in Snake.
 
 
Item 1393: 
Superman's X-Ray vision
 The idea of Superman is that of an augmented human. X-Ray vision is one of his powers. This is nothing but a simplistic way of saying that his retina is sensitive to a much wider spectrum of electromagnetic waves (Sorry to spoil the romance of the idea). If Augmented reality devices could enable us to view a wider spectrum of electromagnetic radiation, it would bring us one step closer to becoming Super People.
 Why are accommodation and vergence important concepts for AR?
 A number of AR glasses users often complain about headaches and eye strain. This is caused due to the eyes being focused on the flat panel within inches of the eye. Even though the 3D objects appear far, the illusion of depth is only simulated. Addition to this, there is a mismatch in the sensory cues provided by the eye to the brain. The eye has to accommodate and converge for a large amount of time which is the main cause of this discomfort. Reducing the effects of accommodation and vergence while simulating real depth is an interesting problem to be solved.
 
 
Item 1396: 
polyaxon notebook start -f polyaxonfile.yml
 polyaxon notebook start -f polyaxonfile.yml -u
 ---
version: 1
 kind: notebook
 build:
  image: python:3
  steps:
    - pip3 install jupyter
 ---
version: 1
 kind: notebook
 environment:
  resources:
    cpu:
      requests: 2
      limits: 4
    gpu:
      requests: 1
      limits: 1
    memory:
      requests: 512
      limits: 2048

build:
  image: tensorflow/tensorflow:1.4.1-py3
  steps:
    - pip3 install scikit-learn
    - pip3 install jupyter
 polyaxon notebook stop
 polyaxon notebook start -f new_polyaxonfile.yml
 polyaxon tensorborad start
 polyaxon tensorboard stop
 polyaxon tensorboard start -f new_polyaxonfile.yml
 
 
Item 1398: 
category: food, description: mcdonalds, price: $4.95, timestamp: 04092018 12:10:00 PM
 
 
Item 1399: 
"The apparent noise one sees in a sensitivity map may be due to essentially meaningless local variations in partial derivatives. After all, given typical training techniques there is no reason to expect derivatives to vary smoothly."
 
 
Item 1400: 
A webhook is an API concept that's growing in popularity. A webhook (also called a web callback or HTTP push API) is a way for an app to provide other applications with real-time information. A webhook delivers data to other applications as it happens, meaning you get data immediately.
 
 
Item 1401: 
1. Load the training and test data 
2. Choose the value of K 
3. For each point in test data:
       - find the Euclidean distance to all training data points
       - store the Euclidean distances in a list and sort it 
       - choose the first k points 
       - assign a class to the test point based on the majority of      classes present in the chosen points
4. End 
 
 
Item 1404: 
def decide(Y, X_dict, previous_node):
    #Calc info gain for each X
    max_IG = 0
    var_to_split = None
 #Calculate information gain to find out which variable to split on
    for x in X_dict.keys():
        IG = InfoGain(Y, X_dict[x])
        if IG > max_IG:
            max_IG = IG
            var_to_split = x
 #See if all variables have been used and none are left.
    if var_to_split == None:
        Y_options = list(set(Y))
        tot = float(len(Y))
        count = [0 for _ in range(len(Y_options))]
 for op in range(len(Y_options)):
            for i in range(len(Y)):
                if Y[i] == op:
                    count[op] += 1
        #Format Node label
        Prob = ""
        for op in range(len(Y_options) - 1):
            Prob += "P("
            Prob += str(Y_options[op]) + ")-> "
            P = float(count[op]) / tot
            Prob += "{0:.2f}".format(P)
        #Make a new node
        nodename = node(Prob, color = "orange")
        edge(previous_node, nodename)
    else:
        print("Splitting on {0}".format(var_to_split))
        X_options = list(set(X_dict[var_to_split]))
        #Make decision variable node
        Var_nodename = node(var_to_split, color = "red")
        edge(previous_node, Var_nodename)
        #Init new data for each new branch of the tree
        for X_option in X_options:
            X_nodename = node(str(X_option))
            edge(Var_nodename, X_nodename)
            New_X_dict = {}
            #get remaining variables
            for key in X_dict.keys():
                if key != var_to_split:
                    New_X_dict[key] = []
            New_Y = []
            #Populate
            for i in range(len(Y)):
                if X_dict[var_to_split][i] == X_option:
                    New_Y.append(Y[i])
                    for key in New_X_dict.keys():
                        New_X_dict[key].append(X_dict[key][i])
 #Check if this is a terminal node:
            if len(set(New_Y)) == 1:
                nodename = node(str(New_Y[0]), color = "green")
                edge(X_nodename, nodename)
            else:
                #No terminal node, so try again
                decide(New_Y, New_X_dict, X_nodename)
 Y, X_dict =  import_golf('golf.csv') #import data
root_node = node("root", color = "blue") #Create the first node
decide(Y, X_dict, root_node) #start the tree
 
 
Item 1409: 
"Look Tommy! Isn't that a cute dog? Look at the cute dog. What color is the cute dog? Tommy, what color is the dog? It's white, isn't it? It's a cute white dog, Tommy."
 Where do we go from here?
 
 
Item 1410: 
@api_app.route(“/do_measurement”, methods=[‘GET’])
def do_measurement():
 try:
 x = float(request.args.get(‘x’, 0))
 except:
 return make_error_response(400, “The x-value needs to be a float number.”)
 if -2 < x < -1 or 1 < x < 2:
 y = None
 else:
 a_rnd = random.gauss(6, 1)
 b_rnd = random.gauss(5, 0.5)
 y = x ** 4–5 * x ** 2 + b_rnd * x — a_rnd
 value = {
 ‘y’: y,
 ‘x’: x
 }
 return make_ok_response(data=value)
 res = re.match(r’b\’{“data”: {“y”: (-?[0–9\.]+), “x”: -?[0–9\.]+}}’, text)
 
 
Item 1412: 
df = scrape_reviews(PATH = 'https://www.trustpilot.com/review/flixbus.com?languages=all&page=',
                   n_pages = 80)
 
 
Item 1414: 
FemaleStudents <- data.frame(Names=c("Alice", "Betty", "Carol", "Denise", "Erica", "Frances", "Gina", "Helen", "Iris", "Julie", "Katherine",
                           "Lisa", "Michelle", "Ngaire", "Olivia", "Penelope", "Rachel", "Sarah", "Trudy", "Uma"))
 set.seed(5)
FemaleStudents$Group <- round(runif(20, 1, 5))
 table(FemaleStudents$Group)
 1 2 3 4 5 
2 6 5 4 3
 set.seed(5)
runif(20,1,5)
 [1] 1.800858 3.740874 4.667503 2.137598 1.418601 3.804230 3.111840 4.231741 4.826001 1.441812 2.093140 2.962053 2.273616 3.236691 2.050373
[16] 1.807501 2.550103 4.551479 3.219690 4.368718
 set.seed(5)
FemaleStudents$Sample <- sample(1:5, nrow(FemaleStudents), replace=TRUE)
 OurGroups <- rep(1:5, each=4)
set.seed(5)
FemaleStudents$Sample <- sample(OurGroups, nrow(FemaleStudents), replace=FALSE)
 table(FemaleStudents$Sample)
 1 2 3 4 5 
4 4 4 4 4
 OurNamedGroups <- rep(c("Up", "Down", "Charmed", "Strange", "Top"), each=4)
set.seed(5)
FemaleStudents$Sample2 <- sample(OurNamedGroups, nrow(FemaleStudents), replace=FALSE)
table(FemaleStudents$Sample2)
 Charmed    Down Strange     Top      Up 
      4       4       4       4       4
 table(FemaleStudents$Sample,FemaleStudents$Sample2)
   
    Charmed Down Strange Top Up
  1       0    0       0   0  4
  2       0    4       0   0  0
  3       4    0       0   0  0
  4       0    0       4   0  0
  5       0    0       0   4  0
 set.seed(5)
FemaleStudents$Library <- sample(c("Test", "Not test"), nrow(FemaleStudents), replace=TRUE, prob=c(4/20,16/20))
table(FemaleStudents$Library)
 Not test     Test 
      15        5
 1 2 3 4 5 
2 7 4 2 5
 
 
Item 1415: 
2.5 x 2.5 x 2.5 x 2.5 x 2.5 = 97.66
 97.66 ^(1/5) = 2.5
 2 x 0.5 x 2 x 0.5 x 2 x 0.5 = 1
 1 ^ (1/6) = 1
 
 
Item 1416: 
#ssh login
ssh -o ServerAliveInterval=60 -i yourkey.pem ec2-user@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com
 #ssh tunneling
ssh -o ServerAliveInterval=60 -i yourkey.pem ec2-user@ec2-xx-xxx-xx-xxx.us-west-2.compute.amazonaws.com -L9000:127.0.0.1:8888
 Host <any name you like> <elasticIP>.us-west-2.compute.amazonaws.com
 HostName <elasticIP>.us-west-2.compute.amazonaws.com
 User ec2-user
 IdentityFile <location of pem file>
 IdentitiesOnly yes
 ForwardAgent yes
 LocalForward 9000 localhost:8888
 ssh DL-box
 
 
Item 1420: 
MATCH (list:Listing)-[:IN_NEIGHBORHOOD]->(:Neighborhood {neighborhood_id:'78704'}) 
WHERE exists(list.bedrooms) 
  AND exists(list.bathrooms)
  AND exists(list.price) 
  AND (:Review)-[:REVIEWS]->(list)
WITH regression.linear.split(collect(id(list)), 0.75) AS trainingIDs
MATCH (list:Listing) WHERE id(list) in trainingIDs 
SET list:Train
 MATCH (list:Listing)-[n:IN_NEIGHBORHOOD]->(hood:Neighborhood {neighborhood_id:'78704'})
WHERE exists(list.bedrooms) 
  AND exists(list.bathrooms)
  AND exists(list.price) 
  AND (:Review)-[:REVIEWS]->(list) 
  AND NOT list:Train 
SET list:Test
 MATCH (:Review)-[r:REVIEWS]->(list) 
WHERE list:Train OR list:Test
WITH list, count(r) AS num_reviews
SET list.num_reviews = num_reviews
 MATCH (list) 
WHERE list:Test OR list:Train
WITH collect(list.num_reviews) as reviews, 
     collect(list.bedrooms + list.bathrooms) as rooms
RETURN regression.linear.correlation(reviews, rooms)
 CALL regression.linear.create(
          'mlr rental prices', 'Multiple', true, 2)
 MATCH (list:Train)
WHERE NOT list:Seen 
CALL regression.linear.add(
          'mlr rental prices', 
          [list.bedrooms + list.bathrooms, list.num_reviews], 
          list.price) 
SET list:Seen RETURN count(list)
 CALL regression.linear.train('mlr rental prices')
 MATCH (list:Test) 
WHERE NOT list:Seen
CALL regression.linear.add(
          'mlr rental prices', 
          [list.bedrooms + list.bathrooms, list.num_reviews], 
          list.price, 
          'test') 
SET list:Seen 
RETURN count(list)
 CALL regression.linear.test('mlr rental prices')
 CALL regression.linear.info('mlr rental prices') 
 
 
Item 1424: 
def l2_diff(x): return K.pow(x[0] - x[1], 2)
 
 
Item 1425: 
//Approach 2
import CreateMLUI
 let builder= MLImageClassifierBuilder()
 builder.showInLiveView()
 
 
Item 1426: 
@privacy  entity(match: "privacy|gdpr|personal data")

dialog trigger: @privacy do
  invoke privacy
end

dialog privacy do
  say "please read our privacy policy carefully at https://www.botsquad.com/privacy"
end
 dialog main do
  invoke hello
  invoke privacy
  invoke menu
end
 dialog privacy do
  say "please read our privacy policy carefully at http://www.mydomain.com/privacy"
end
   user.first_name = ask "what is your name?"
  remember user.first_name
  forget user.first_name
 @forget entity(match: "(delete|forget)(.*)(my|personal)(.*)data")

dialog trigger: @forget do
  say "I will forget your personal data"
  forget user.first_name
end
 @mydata entity(match: "(show|get|present)(.*)(my|personal)(.*)data")

dialog trigger: @mydata do
  say "This is what I remembered about you"
  say user.first_name
  say user.email
end
 dialog trigger: @mydata do
  mail user.email,
    "your personal data",
    "This is the data I have collected: " +
    join([user.first_name, user.email], ", ")

  say "I've sent you an e-mail with all personal data we have."
end
   say "To be able to use the CalendarBot, it needs to be connected to your Google Calendar."
  say "This means CalendarBot will ask you to login using your Google Account, at which it will retrieve your name, e-mail address and profile picture from Google. It will also retrieve an access token which it will use from time to time to check your availability (free or busy). It will never retrieve the contents of your calendar except for the events that are explicitly scheduled using this CalendarBot."

  ask "Do you agree to all this?", expecting: [:yes, :no]
 
 
Item 1428: 
persistence:
  data:
    data1:
      mountPath: "/data/1"
      hostPath: "/path/to/data"
      readOnly: true
    data2:
      mountPath: "/data/2"
      existingClaim: "data-2-pvc"
    data-foo:
      mountPath: "/data/foo"
      existingClaim: "data-foo-pvc"
  outputs:
    outputs1:
      mountPath: "/outputs/1"
      hostPath: "/path/to/outputs"
      readOnly: true
    outputs2:
      mountPath: "/outputs/2"
      existingClaim: "outputs-2-pvc"
    outputs-foo:
      mountPath: "/outputs/foo"
      existingClaim: "outputs-foo-pvc"
 environment:
  persistence:
    data: ['data1', 'data-foo']
 environment:
  persistence:
    outputs: 'outputs-foo'
 
 
Item 1437: 
model_dict = {}
 locs = [‘HN’,”HCM”]
 for i in range(len(model_data)):
 model_data_ = model_data[i]
 for e in range(3):
 X_y = model_data_[[‘demand_lag_day’,’ma_week’,”demand_lag_week”, “demand_lag_month”,”before_holiday”,”new_users”,’sales’,’demand’]]
 X_y = X_y.dropna()
 X_ = X_y[[‘demand_lag_day’,’demand_lag_week’,’ma_week’, “demand_lag_month”,”before_holiday”,”new_users”,’sales’]]
 y_ = X_y[‘demand’]
 X = X_[X_.index.str[-1]==’{}’.format(e+1)]
 y = y_[y_.index.str[-1]==’{}’.format(e+1)]
 performance = …
 mod = …
 X_1, X_test, y_1, y_test = train_test_split(X, y, test_size = 0.2)
 for g in range(…):
 X_train, X_valid, y_train, y_valid = train_test_split(X_1, y_1,test_size = 0.1)
 model = linear_model.LinearRegression()
 model.fit(X_train, y_train)
 if np.percentile(np.abs(model.predict(X_valid) — y_valid), 90) < performance:
 performance = np.percentile(np.abs(model.predict(X_valid) — y_valid), 90)
 mod = model
 performance = np.percentile(np.abs(model.predict(X_test) — y_test), 90)
 model_dict[‘{0}_{1}’.format(locs[i], e+1)] = [mod, performance]
 
 
Item 1443: 
Relative To = CALCULATE(MAX(USvideos[trending_date]), ALL(USvideos))
 Total Days = IF(HASONEVALUE(‘Date Ranges’[Business Ranges]), VALUES(‘Date Ranges’[Days Ranges]))
 Total Comparison = IF(HASONEVALUE(‘Date Ranges’[Business Ranges]), VALUES(‘Date Ranges’[Comparison Ranges]))
 Slicer date check = DATEDIFF(‘USvideos Dates’[trending_date], [Relative To], DAY)
 Comparison = IF(MIN([Slicer date check]) <= [Total Days] && MIN([Slicer date check]) > 0, 1, 0)
 Comparison Change = IF(MIN([Slicer date check]) > [Total Days] && MIN([Slicer date check]) <= [Total Comparison], 1, 0)
 Slicer text views = SWITCH(TRUE(), 
 VALUES(‘Date Ranges’[Business Ranges]) = “ Day”, “VIEWS YESTERDAY”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Week”, “VIEWS THIS WEEK”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Month”, “VIEWS THIS MONTH”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Quarter”, “VIEWS THIS QUARTER”,
 “”)
 Slicer text relative = SWITCH(TRUE(), 
 VALUES(‘Date Ranges’[Business Ranges]) = “ Day”, “VS PREVIOUS DAY”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Week”, “VS PREVIOUS WEEK”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Month”, “VS PREVIOUS MONTH”,
 VALUES(‘Date Ranges’[Business Ranges]) = “ Quarter”, “VS PREVIOUS QUARTER”,
 “”)
 Number of Views Sliced = 
var calc = CALCULATE(SUM(USvideos[views]), FILTER(‘USvideos Dates’, [Comparison] = 1))
 Return
IF(ISBLANK(calc), 0, calc)
 Number of Views Change = 
VAR Down = UNICHAR(9660)
VAR Up = UNICHAR(9650)
Var Constant = UNICHAR(9654)
VAR Change = IFERROR(
 ([Number of Views Sliced] — CALCULATE(SUM(USvideos[views]), FILTER(‘USvideos Dates’, [Comparison Change] = 1)))/CALCULATE(SUM(USvideos[views]), FILTER(‘USvideos Dates’, [Comparison Change] = 1)),
 BLANK())
 RETURN
SWITCH(TRUE(),
 ISBLANK(Change), “-”,
 Change < 0, Down & FORMAT(Change, “0.0%”),
 Change > 0, Up & FORMAT(Change, “0.0%”),
 Constant & Format(Change, “0.0%”))
 
 
Item 1446: 
(Input) X = 1,2,3,4,5
(Prediction) Y = 1,2,3,4,5
 (x1,y1) = (1, 4) 
(x2,y2) = (5, 80)
 
 
Item 1449: 
my_tpu_estimator = tf.contrib.tpu.TPUEstimator(
    model_fn=my_model_fn,
    config=tf.contrib.tpu.RunConfig()
    use_tpu=True)
 paperspace jobs create --machineType TPU --container gcr.io/tensorflow/tensorflow:1.6.0 --command "python main.py"
 
 
Item 1451: 
Partition Train/Test split
Train CNN with Train data
 Evaluate CNN on Train data outside of training loop
Select misclassified instances and 25% of the data
 Train CNN on this new subset
 With future predictions weight the output of CNN1 80% & CNN2 20%
 Evaluate model
 
 
Item 1455: 
Number of colors selected on slicer = IF(ISFILTERED(flagInfo[details]), COUNTROWS(ALLSELECTED(flagInfo[details])), 0)
 PBISlicer Check = 
IF([Number of colors selected on slicer] = 0, 1,
 IF(DISTINCTCOUNT(flagInfo[details]) = [Number of colors selected on slicer], 1, 0))
 PBISlicer Country Count = 
CALCULATE(COUNTROWS(countryInfo), 
FILTER(countryInfo, [PBISlicer Check] = 1))
 Number of variables selected on slicer = IF(ISFILTERED(flagInfo[variable]), COUNTROWS(ALLSELECTED(flagInfo[variable])), 0)
 HierarchySlicer Check = 
IF([Number of variables selected on slicer] = 0, 1,
 IF(DISTINCTCOUNT(flagInfo[variable]) = [Number of variables selected on slicer], 1, 0))
 HierarchySlicer Country Count = 
CALCULATE(COUNTROWS(countryInfo), 
FILTER(countryInfo, [HierarchySlicer Check] = 1))
 HierarchySlicer population = 
CALCULATE(SUM(countryInfo[population]), 
FILTER(countryInfo, [HierarchySlicer Check] = 1))
 
 
Item 1458: 
> getwd()
[1] "C:/Users/eoppe/Documents"
 > 3+5
[1] 8
> 4-6
[1] -2
> 4*7
[1] 28
> 13/2
[1] 6.5
 > "I love hockey... I think"
[1] "I love hockey... I think"
> print("I love hockey... I think")
[1] "I love hockey... I think"
> print("We tend to overrate the potential of CHL prospects")
[1] "We tend to overrate the potential of CHL prospects"
 > my_message <- "I'm learning to analyze hockey data in R!"
>
 > my_message
[1] "I'm learning to analyze hockey data in R!"
 goals <- c(16, 26, 25)
 > goals
[1] 16 26 25
 > assists <- c(37, 20, 19)
> assists
[1] 37 20 19
 > names <- c("Mats Zuccarello", "Mika Zibanejad", "Kevin Hayes")
> names
[1] "Mats Zuccarello" "Mika Zibanejad"  "Kevin Hayes"
 points <- goals + assists
points
 > points <- goals + assists
> points
[1] 53 46 44
 player_stats <- data.frame(names, goals, assists, points)
player_stats
 > player_stats <- data.frame(names, goals, assists, points)
> player_stats
            names goals assists points
1 Mats Zuccarello    16      37     53
2  Mika Zibanejad    26      20     46
3     Kevin Hayes    25      19     44
 View(player_stats)
 
 
Item 1461: 
A = 1.8
B = 16
Y = ((round(A) - sqrt(B)) * round(A)) + abs((round(A) - sqrt(B)))
 
 
Item 1463: 
last_date < max(last_date) - weeks(4)
 max(last_date) - weeks(4)
 last_date < max(last_date) - weeks(4)
 
 
Item 1467: 
#include <stdio.h>
#include <ctype.h>
int main(void)
{
    int c;
    do {
        do {
            c = getchar();
        } while(c == ' ');
        while(c != EOF && !isspace(c) && c != '\n') {
            putchar(c);
            c = getchar();
        }
        putchar('\n');
        while(c != EOF && c != '\n')
            c = getchar();
    } while(c != EOF);
    return 0;
}
 #include <stdio.h>
enum states { before, inside, after };
void step(enum states *state, int c)
{
    if(c == '\n') {
        putchar('\n');
        *state = before;
    } else
    switch(*state) {
        case before:
            if(c != ' ') {
                putchar(c);
                *state = inside;
            }
            break;
        case inside:
            if(c == ' ') {
                *state = after;
            } else {
                putchar(c);
            }
            break;
        case after:
            break;
    }
} 
int main(void)
{
    int c;
    enum states state = before;
    while((c = getchar()) != EOF) {
        step(&state, c);
    }
    if(state != before)
        putchar('\n');
    return 0;
}
 
 
Item 1469: 
"What's the weather going to be like in Berlin tomorrow"
 { "location": "Berlin", "time": "tomorrow" }
 Could you help me find an [empanada](food) place?
 language: "en" 
 pipeline: 
 - name: "nlp_spacy"
 - name: "tokenizer_spacy"
 - name: "intent_entity_featurizer_regex"
 - name: "ner_crf"
   features: [ 
               ["low", "title", "upper"],
               ["bias", "low", "prefix5", "prefix2", "suffix5",
                "suffix3", "suffix2", "upper", "title", "digit", 
                "pattern"], 
               ["low", "title", "upper"] 
             ]
 python run_lookup.py food
 mapo tofu chana masala sushi pizza ...
 ## lookup:food
   data/food/food.txt
 "lookup_tables": [
    {
        "name": "food",
        "elements": "data/food/food.txt"
    }
]
 I work for the [New York Times](company)
 python run_lookup.py company
 {
    "rasa_nlu_data": {
        "lookup_tables": [ 
                {
                    "name": "company",
                    "elements": "data/company/startups.csv"
                }
        ],
        "common_examples": [
             { 
                 "text": "I wanna talk to your sales people.",
                 "intent": "contact_sales",
                 "entities": []
             },
         ... 
        ] 
    } 
}
 python filter_lookup.py <lookup_in> <cross_list> <lookup_out>
 python filter_lookup.py data/company/startups.csv data/company/english_scrabble.txt data/company/startups_filtered.csv
 Take me to [123 Washington Street](address) please
 "Ban bananas!"
 ("ban", "an_", "n_b", "_ba", "ana", "nan", "nas", "as!")
 ("llc", "corp", "gmbh", "com", "tech", "plex")
 regex_string = '(?i)(\\b' + '\\b|\\b'.join(elements_sanitized) + '\\b)'
 
 
Item 1473: 
g_loss_G_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))
 g_loss_F_cycle = tf.reduce_mean(tf.abs(real_X — genF_back)) + tf.reduce_mean(tf.abs(real_Y — genG_back))
 Gz = generator(z_in) #Generates images from random z vectors (noise) 
Dx = discriminator(real_in) #Produces probabilities for real images
Dg = discriminator(Gz) #Produces probabilities for generator images

#These functions together define the optimization objective of the GAN.
d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.
 
 
Item 1477: 
Centroid 0[  51.38661738 -125.27254198    7.51493849   16.8132888    -1.17574106
   67.95416645]
Centroid 1[  50.61259827 -126.25917862    6.32184038   14.17993615   -1.63282112
  340.64034668]
Centroid 2[  50.42396495 -125.2544494     7.2744194    16.09718498   -1.19140013
  222.65626168]
Centroid 3[  50.42534148 -126.39001822    6.72726471   14.53652612   -1.31210139
  507.65982791]
Centroid 4[  49.89467361 -122.37195052   16.63158605   29.73554202    5.95873865
   28.06162618]
Centroid 5[  52.27917741 -121.73550782   -3.83946568    8.19646576  -19.93363911
   45.30064963]
Centroid 6[  50.2945556  -125.21241436    9.15306962   18.32174182    0.93638661
  119.43331396]
Centroid 7[  50.70282538 -125.32884059    8.00965749   17.614526     -0.77992491
  151.77106456]
Centroid 8[  50.31580334 -126.23307623    6.5871719    14.64225657   -1.44068436
  660.70782628]
Centroid 9[  50.28874072 -125.06866063    7.49467144   16.55379438   -0.84254229
  251.73801799]
Centroid 10[  50.4262205  -126.2629756     6.58541814   14.36302394   -1.36671652
  399.90931304]
Centroid 11[  50.35982435 -125.51021147    6.32659523   14.73932271   -1.89873252
  294.050496  ]
Centroid 12[  51.24356927 -121.64231971    6.18712464   18.67948989   -5.48058692
   36.06758652]
Centroid 13[  50.75675998 -125.60116426    7.48408728   16.37144962   -0.95604598
  190.18377378]
Centroid 14[  51.1337961  -124.85444455    7.9966279    17.1295218    -0.56629512
   94.65084155]
Centroid 15[  50.12150383 -126.04955706    6.7603695    14.43437023   -1.13385479
  461.59884165]
Centroid 16[  50.72982522 -126.73419985    6.88866959   14.84315741   -1.12018377
  586.45036101]
Centroid 17[  51.44904147 -122.37356247   12.74453517   26.10130055    1.58123601
   50.9674971 ]
 
 
Item 1478: 
model = ["/etc/snips/personal_2018_04_19T08_45_59=0.5"]
 
 
Item 1479: 
#Getting tweets from twitter
library(twitteR)
#Twitter-R Authentication
library(ROAuth)
#Text processing
library(stringr)
#Text mining 
library(tm)
#Wordcloud creation
library(wordcloud)
 #Connect to twitter API using the saved credentials
load("twitter authentication.Rdata")
setup_twitter_oauth(credentials$consumerKey, credentials$consumerSecret, credentials$oauthKey, credentials$oauthSecret)
 UserData <- lapply(checkHandles, function(x) getUser(x))
UserData <- twListToDF(UserData)
table(UserData$name, UserData$statusesCount) #Check tweet count
table(UserData$name, UserData$protected) #Check Private Accounts
usernames <- subset(UserData, protected == FALSE) #Public Accounts
usernames <- as.list(usernames$screenName)
 x <- userTimeline(‘realDonaldTrump’,n=3200,includeRts = FALSE)
 #Convert tweets list to dataframe
TrumpData <- twListToDF(x)
 #Wait 5 minutes 
Sys.sleep(300)
 x <- userTimeline(‘PutinRF_Eng’,n=3200,includeRts = FALSE)
PutinData <- twListToDF(x)
Sys.sleep(300)
 x <- userTimeline(‘narendramodi’,n=3200,includeRts = TRUE)
ModiData <- twListToDF(x)
 Trump.df <- data.frame(TrumpData)
Putin.df <- data.frame(PutinData)
Modi.df <- data.frame(ModiData)
 #Now create a dataframe that combines all of the collected tweets
tweets <- data.frame()
tweets <- Trump.df
tweets <- rbind(tweets,Putin.df)
tweets <- rbind(tweets,Modi.df)
 #Convert tweets to ASCII to avoid reading strange characters
iconv(tweets$text, from=”UTF-8", to=”ASCII”, sub=””)
 #Clean text by removing graphic characters            tweets$text=str_replace_all(tweets$text,"[^[:graph:]]", " ")
 #Remove Junk Values and replacement words like fffd which appear because of encoding differences            
tweets$text <- gsub("[^[:alnum:]///' ]", "", tweets$text)
 #Convert all text to lower case            
tweets$text <- tolower(tweets$text)
 #Remove retweet keyword            
tweets$text <- gsub("rt", "", tweets$text)
 #Remove Punctuations            
tweets$text <- gsub("[[:punct:]]", "", tweets$text)
 #Remove links            
tweets$text <- gsub("http\\w+", "", tweets$text)
 #Remove tabs            
tweets$text <- gsub("[ |\t]{2,}", "", tweets$text)
 #Remove blankspaces at begining            
tweets$text <- gsub("^ ", "", tweets$text)
 #Remove blankspaces at the end            
tweets$text <- gsub(" $", "", tweets$text)
 #Remove usernames 
tweets$text <- gsub("@\\w+", "", tweets$text)
 #After preprocessing the data, subset for tweets for each handle
Trump <- subset(tweets, screenName==“realDonaldTrump”, select= text)
Putin <- subset(tweets, screenName== “PutinRF_Eng”, select= text)
Modi <- subset(tweets, screenName== “narendramodi”, select= text)
 #Create corpus of individual twitter handles 
Trump <- Corpus(VectorSource(Trump))
Putin <- Corpus(VectorSource(Putin))
Modi <- Corpus(VectorSource(Modi))
 #Remove English Stopwords from the tweets
Trump <- tm_map(Trump, removeWords, stopwords(“en”))
Putin <- tm_map(Putin, removeWords, stopwords(“en”))
Modi <- tm_map(Modi, removeWords, stopwords(“en”))
 wordcloud(Trump,min.freq = 3, scale=c(6,0.5),colors=brewer.pal(8, “Dark2”),random.color= FALSE, random.order = FALSE, max.words = 110)
 wordcloud(Putin,min.freq = 4, scale=c(7,0.8),colors=brewer.pal(8, “Dark2”),random.color= FALSE, random.order = FALSE, max.words = 100)
 wordcloud(Modi,min.freq = 3, scale=c(6,0.5),colors=brewer.pal(8, “Dark2”),random.color= FALSE, random.order = FALSE, max.words = 110)
 wordcloud(words, freq, min.freq, scale, max.words, random.order, random.color, rot.per,colors, ordered.colors,…)
 wordcloud(df$words,df$freq, min.freq = 3, scale=c(6,0.5), random.color= FALSE, ordered.colors=TRUE, colors=brewer.pal(8, “Dark2”)[factor(df$sentiment)], random.order = FALSE, max.words = 110)
 #Remove numbers if necessary
#Putin <- tm_map(Putin, removeNumbers)
 #Remove specific words if needed
#Trump <- tm_map(Trump, removeWords, c(‘amp’,’will’))
#Modi <- tm_map(Modi, removeWords, c(‘amp’,’will’))
 
 
Item 1484: 
char_to_ix: it's a dictionary to assign a unique number to each unique character
ix_to_char:it's a dictionary to assign a unique character to each number.
We deal with assigned number of each character and predict number of next character and then use this predicted number to find the next character.
 this is what we are doing in 25th,26th line in the code above.
a=[[1], 
   [0],
   [0],
   [0],
   [0],
   [0]]

 Now:
To calculate:
dy: ps[t]-1
 dWhy += : dy•hs[t].T
dh += Why.T•dy + dhnext
 dby += dy (As matrix multiplication term becomes zero in   derivative )
 #backprop in hs[t] now:
dhraw adds derivative w.r.t tanh(derivative of tanh is 1-tanh^2)
dhraw= (1-hs[t]^2)*dh
 dbh += dhraw (because derivative matrix multiplication terms is zero w.r.t dbh)
dWhx += (dhraw•xs[t].T)
dWhh += (dhraw•hs[t-1])
finally:
dhnext += (Whh.T•dhraw)
 line number 5-6 is the way we Adagrad works.
Like in normal gradient descent we do:
theta= theta-lr*grad
1e-8 is used to prevent DivisionByZero exception.
 At epoch zero:Generated text
loss=106.56699692603289
iteration:0
 QZBipoe.M
prb’gxc]QXECCY“f);wqEnJAVV-Dn-
Fl-tXTFTNI[ ?Jpzi”BPM’TxJlhNyFamgIj)wvxJDwBgGbF!D“F‘bU;[)KXrT km*;xwYZIx-
AX
dDl_zk(QlW(KolSenbudmX.yq
H-(uPUl-B:mj]o’E-ZTjzH)USf:!
sCiTkTMcmgUY)rCj
ZaL*rhWVpS----
---------------------------------------------------
l was  beginning begiginning to Alice walicegininn to geteginninato giteginniito geteginninn to geteginninatg gegeginninasto get beginninnninnigw to gicleaaaa  was ginniicg benning to get a wen----
loss=11.115271278781561
iteration:66200
 
 
Item 1485: 
num_of_my_moves - 2 * num_of_opponent_moves
 num_of_blank_spaces / 2 + 2 * num_of_my_moves
 (num_of_my_moves - 2 * num_of_opponent_moves) + (num_of_blank_spaces / 2 + 2 * num_of_my_moves) / 13
 
 
Item 1487: 
#define MAX_SIZE 3
 class BTree {
  private:
    struct node {
      int val;
      node* next;
      node(int v_) : val(v_), next(NULL) {};
    };    
    struct block {
      node* head;
      block* up;
      block* down[MAX_SIZE];
      block(node* n_, block* b_) : up(b_), head(n_) {
        for(int i=0; i<MAX_SIZE; i++) { down[i] = NULL; }
      }
       ...//block routines//...
     };
}
 const int insert(node* n) {
  int i = 0;
  node* prev = NULL;
  node* cur = head;
  while( (cur) && (n->val > cur->val) )
    prev = cur, cur = cur->next, i++;
  (prev) ? prev->next = n : head = n;
  n->next = cur;
  return i;
}
 const int size() {
  int count = 0;
  node* n = head;
  while(n && ++count) { n = n->next; }
  return count;
}
 const bool isLeaf() {
  for(int i=0;i<MAX_SIZE;i++)
    if(down[i])
      return false;
  return true;
}
 node* splitOnMedian() {
  node* n = head;
  node* p = NULL;
  const int offset = (MAX_SIZE%2) ? (MAX_SIZE/2) : (MAX_SIZE/2-1);
  for(int i = 0; i<(offset); i++)
    p = n, n = n->next;
  p->next = NULL;
  return n;
}
 void addNode(int v_) {
  if(!root)
    root = new block(new node(v_),NULL);
  else {
    block* b = search(v_);
    ...
  }
}
 block* search(int v_) {
  block* b = root;
  node* n = root->head;
  int i = 0;
  while( !(*b).isLeaf() ) {
    if( !n && b->down[i] )
      b = b->down[i], n = b->head, i = 0;
    else if(v_ == n->val)
      return b;
    else if(v_ < n->val)
      b = b->down[i], n = b->head, i = 0;
    else if(n)
      n = n->next, i++;
    }
    return b;
}
 void addNode(int v_) {
  if(!root)
    root = new block(new node(v_),NULL);
  else {
    block* b = search(v_);
    if( !(*b).isLeaf() )
      std::cout<<"Warning: value is already in the tree";           
    else {
      node* n = new node(v_);
      (*b).insert(n);
      block* carry = NULL;
      while( (*b).size() >= MAX_SIZE ) {
        ...       
      }
    }
  }
}
 void addNode(int v_) {
  if(!root)
    root = new block(new node(v_),NULL);
  else {
    block* b = search(v_);
    if( !(*b).isLeaf() )
     std::cout<<"Warning: value is already in the tree";           
    else {
      node* n = new node(v_);
      (*b).insert(n);
      block* carry = NULL;
      while( (*b).size() >= MAX_SIZE ) {
        n = (*b).splitOnMedian();
        block* s = new block(n->next,b->up);
        n->next = NULL;
        if(carry) {
          const int offset = (MAX_SIZE%2 == 0) ? (MAX_SIZE/2) :(MAX_SIZE/2+1);
          for(int i=(offset); i<MAX_SIZE; i++) {
            s->down[i-(offset)] = b->down[i];
            s->down[i-(offset)]->up = s;
            b->down[i] = NULL;
          }
          s->down[(MAX_SIZE/2)] = carry;
          s->down[(MAX_SIZE/2)]->up = s;
          carry = NULL;
        }
        if( b == root ) {
          root = new block(n,NULL);
          root->down[0] = b, b->up = root;
          root->down[1] = s, s->up = root;
        }
        else {
          for(int i = (*b->up).insert(n) + 1; i<MAX_SIZE; i++) {         
            block* t = b->up->down[i];
            b->up->down[i] = s;
            s = t;
          }
          if(s)
            carry = s;
        }
        b = b->up;
      }  
    }
  }
}  
 
 
Item 1488: 
struct entry {
  std::string word;
  int64_t count;
  entry_type type;
  std::vector<int32_t> subwords;
};
 
 
Item 1493: 
begin
    count = 0
    initialize population
    evaluate population
    while not termination condition do
    begin
        count = count + 1
        select individuals for reproduction
        apply variation operators
        evaluate offspring
    end
end
 
 
Item 1495: 
input_img = Input(shape=(120,208,1))
x = Conv2D(filters=128,kernel_size=3, activation='relu', padding='same')(input_img)
x = MaxPooling2D(pool_size=2)(x)
x = Conv2D(filters=64,kernel_size=3, activation='relu', padding='same')(x)
x = MaxPooling2D(pool_size=2)(x)
x = Conv2D(filters=32,kernel_size=3, activation='relu', padding='same')(x)
x = MaxPooling2D(pool_size=2)(x)
shape = K.int_shape(x)
x = Flatten()(x)
x = Dense(128,kernel_initializer='glorot_uniform')(x)
 z_mean = Dense(latent_dim)(x)
z_log_var = Dense(latent_dim)(x)
z = Lambda(sampling, output_shape=(latent_dim,), name="z")([z_mean,z_log_var])
 encoder = Model(input_img, [z_mean, z_log_var,z], name="encoder")
 latent_inputs = Input(shape=(latent_dim,), name='z_sampling')
x = Dense(shape[1] * shape[2] * shape[3], kernel_initializer='glorot_uniform',activation='relu')(latent_inputs)
x = Reshape((shape[1],shape[2],shape[3]))(x)
x = Dense(128,kernel_initializer='glorot_uniform')(x)
x = Conv2D(filters=32, kernel_size=3, activation='relu', padding='same')(x)
x = UpSampling2D(size=(2,2))(x)
x = Conv2D(filters=64,kernel_size=3, activation='relu', padding='same')(x)
x = UpSampling2D(size=(2,2))(x)
x = Conv2D(filters=128,kernel_size=3, activation='relu', padding='same')(x)
x = UpSampling2D(size=(2,2))(x)
x = Conv2D(filters=1,kernel_size=3, activation='sigmoid', padding='same')(x)
 decoder = Model(latent_inputs,x,name='decoder')
 inputs = Input(shape=(128,))
x = Reshape((1,128))(inputs)
x = LSTM(512, return_sequences=True,input_shape=(1,128))(x)
x = Dropout(0.40)(x)
x = LSTM(512, return_sequences=True)(x)
x = Dropout(0.40)(x)
x = LSTM(512)(x)
x = Dropout(0.40)(x)
x = Dense(1000,activation='relu')(x)
outputs = mdn.MDN(outputDim, numComponents)(x)
model = Model(inputs=inputs,outputs=outputs)
 
 
Item 1496: 
docker pull lab41/poseidonml
 docker run –v <path_to_pcap>:/pcaps/eval.pcap lab41/poseidonml
 docker run –v <path_to_pcap>:/pcaps/eval.pcap lab41/poseidonml:onelayer
 Message: {“98:01:xx:xx:xx:xx”: {“classification”: {“confidences”: [0.6856065894902689, 0.2727088338248893, 0.022470232107183397], “labels”: [“Developer workstation”, “Unknown”, “Active Directory controller”]}, “timestamp”: 1498669414.355485, “valid”: false, “decisions”: {“behavior”: “normal”, “investigate”: false}}}
 DeviceName-deviceID-time-duration-flags.pcap
 DevWorkstation-User1-Tues1520–60mins-n00.pcap
 Fileserver-a-unk-unk-n00.pcap
 GPULaptop-user1-Fri0409–60mins-n00.pcap
 Iphone-b-Wed0945–5days-n00.pcap
 BusinessWorkstation-user2-Mon-3days-00.pcap
 {
“DevWorkstation”: “Developer workstation”,
 “Iphone”: “Smartphone”,
 “GPULaptop”: “GPU laptop”,
 “Fileserver”: “File server”,
 “BusinessWorkstation”: “Business workstation”
}
 docker run -v <path-to-dataset>:/pcaps -v <path-to-save-model>:/models/lab41/poseidonml train
 docker run -v <path-to-dataset>:/pcaps -v <path-to-save-model>:/models/lab41/poseidonml testing
  — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Results with unknowns
  — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 F1 of 0.XX for Developer workstation
 F1 of 0.XX for Smartphone
 F1 of 0.XX for Unknown
 …
 Mean F1: 0.XX
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Results forcing decisions
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 F1 of 0.XX for Developer workstation
 F1 of 0.XX for Smartphone
 …
 Mean F1: 0.XX
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Analysis statistics
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Evaluated X pcaps in X seconds
 Total data: X Mb
 Total capture time: X hours
 Data processing rate: X Mb per second
 time per 15 minute capture X seconds
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 
 
Item 1497: 
i s i t
i s it
i si t
i sit
is i t
is it
isi t
isit
 input:                 isit
Norvig segment:        is it             -> no spelling correction
WordSegmentation ed=0: is it             -> no spelling correction
WordSegmentation ed=1: visit             -> spelling correction
LookupCompound ed=0:   i sit             -> no spelling correction
LookupCompound ed=1:   visit             -> spelling correction
 input:                 independend
Norvig segment:        in depend end      -> no spelling correction
WordSegmentation ed=0: in depend end      -> no spelling correction
WordSegmentation ed=1: independent        -> spelling correction
LookupCompound ed=0:   independend        -> no spelling correction
LookupCompound ed=1:   independent        -> spelling correction
 input:                 whocouqdn’tread
Norvig segment:        who couqdn’t read  -> no spelling correction
WordSegmentation ed=0: who co uqdn’t read -> no spelling correction
WordSegmentation ed=1: who couldn’t read  -> segmentation+correction
LookupCompound ed=0:   whocouqdn’tread    -> more than 1 space/token
LookupCompound ed=1:   whocouqdn’tread    -> more than 1 space/token
 
 
Item 1502: 
images.zip
|- "train" directory
  |- jpg image files of training data
  |- "via_region_data.json" annotations file of training data
|- "val" directory
  |- jpg image files of validation data
  |- "via_region_data.json" annotations file of validation data
 Using TensorFlow backend. 
Weights:  coco 
Dataset:  dataset/ 
Logs:  /logs
 Configurations: 
BACKBONE                       resnet101 
BACKBONE_STRIDES               [4, 8, 16, 32, 64] 
BATCH_SIZE                     2
...
GPU_COUNT                      1 
GRADIENT_CLIP_NORM             5.0 
IMAGES_PER_GPU                 2
...
Selecting layers to train 
fpn_c5p5               (Conv2D) 
fpn_c4p4               (Conv2D) 
fpn_c3p3               (Conv2D) 
fpn_c2p2               (Conv2D) 
...
 
 
Item 1507: 
@inproceedings{kipf2018neural,
  title={Neural Relational Inference for Interacting Systems},
  author={Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}
 
 
Item 1508: 
...
01-01 00:01:12.596 714-756/com.example.androidthings.imageclassifier D/ImageClassifierActivity: Got the following results from Tensorflow: [[578] remote control (91.3%)]
...
 
 
Item 1509: 
Nb_class = 5
lr = 0.001
 base_model = InceptionV3(weights = 'imagenet', include_top = False, input_shape=(224,224,3))
 x = base_model.output
x = GlobalAveragePooling2D(name='avg_pool')(x)
x = Dense(1024, activation = 'relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(Nb_class, activation='softmax', name='predictions')(x)
 full_model = Model(inputs=base_model.input, outputs=predictions)
 for layer in base_model.layers:
   layer.trainable = False
 sdg = SGD(lr=lr, momentum = 0.9)
 full_model.compile(optimizer=sdg, loss='categorical_crossentropy', metrics = ['accuracy'])
 full_model.fit(X, y
, batch_size = batch_size
,epochs = epochs
,verbose = 1, shuffle = True, callbacks= [csv_logger,checker]
,validation_data = (Xv, yv))
 for layer in base_model.layers[:197]:
   layer.trainable = False
for layer in base_model.layers[197:]:
   layer.trainable = True
 
 
Item 1511: 
pip3 install torchfusion
 
 
Item 1512: 
digits_final = TSNE(perplexity=30).fit_transform(X)
 
 
Item 1514: 
[‘fronting’, ‘snowbank’, ‘business’]
 [(‘Several’, ‘JJ’), (‘men’, ‘NNS’), (‘stand,’, ‘VBD’), (‘and’, ‘CC’), (‘one’, ‘CD’), (‘sits,’, ‘NN’), (‘on’, ‘IN’), (‘the’, ‘DT’), (‘corner’, ‘NN’), (‘boardwalk’, ‘NN’), (‘opposite’, ‘IN’), (‘the’, ‘DT’), (‘hotel.’, ‘NN’)]
 (S
 The/DT
 (NounPhrase tops/NNS)
 of/IN
 (NounPhrase music/NN stands/NNS)
 with/IN
 (NounPhrase lamps/NNS)
 and/CC
 (NounPhrase chair/NN backs/NNS)
 show/VBP
 in/IN
 the/DT
 (NounPhrase foreground./NN))
 diff -u unigrams.csv pos_unigrams.csv
 @@ -3941,30 +3048,20 @@
 “workmen”
 “works”
 “world”
-”worn”
-”wrapped”
 “wrecker”
 “wrigleys”
 Baseline F1: 0.473684
POS Filtered F1: 0.800000
 
 
Item 1518: 
## CODE
tokenizer = Tokenizer()
 texts = ["The sun is shining in June!","September is grey.","Life is beautiful in August.","I like it","This and other things?"]
 tokenizer.fit_on_texts(texts)
 print(tokenizer.word_index)
 tokenizer.texts_to_sequences(["June is beautiful and I like it!"])
 ## OUPUT
 {'sun': 3, 'september': 4, 'june': 5, 'other': 6, 'the': 7, 'and': 8, 'like': 9, 'in': 2, 'beautiful': 11, 'grey': 12, 'life': 17, 'it': 16, 'i': 14, 'is': 1, 'august': 15, 'things': 10, 'shining': 13, 'this': 18}
 [[5, 1, 11, 8, 14, 9, 16]]
 ## Pad Sequences Example
pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]])
array([[0, 1, 2, 3],
       [3, 4, 5, 6],
       [0, 0, 7, 8]], dtype=int32)
 
 
Item 1519: 
{
    "timestamp" : 34512.21,
    "vehicles" : [
        {
            "id"  : 0,
            "x"   : -10.0,
            "y"   : 8.1,
            "v_x" : 8.0,
            "v_y" : 0.0,
            "sigma_x" : 0.031,
            "sigma_y" : 0.040,
            "sigma_v_x" : 0.12,
            "sigma_v_y" : 0.03,
        },
        {
            "id"  : 1,
            "x"   : 10.0,
            "y"   : 12.1,
            "v_x" : -8.0,
            "v_y" : 0.0,
            "sigma_x" : 0.031,
            "sigma_y" : 0.040,
            "sigma_v_x" : 0.12,
            "sigma_v_y" : 0.03,
        },
    ]
}
 {
    "timestamp" : 34512.21,
    "vehicles" : [
        {
            "id" : 0,
            "length": 3.4,
            "width" : 1.5,
            "predictions" : [
                {
                    "probability" : 0.781,
                    "trajectory"  : [
                        {
                            "x": -10.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34512.71
                        },
                        {
                            "x": -6.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34513.21
                        },
                        {
                            "x": -2.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34513.71
                        },
                        {
                            "x": 2.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34514.21
                        },
                        {
                            "x": 6.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34514.71
                        },
                        {
                            "x": 10.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34515.21
                        },
                    ]
                },
                {
                    "probability" : 0.219,
                    "trajectory"  : [
                        {
                            "x": -10.0,
                            "y": 8.1,
                            "yaw": 0.0,
                            "timestamp": 34512.71
                        },
                        {
                            "x": -7.0,
                            "y": 7.5,
                            "yaw": -5.2,
                            "timestamp": 34513.21
                        },
                        {
                            "x": -4.0,
                            "y": 6.1,
                            "yaw": -32.0,
                            "timestamp": 34513.71
                        },
                        {
                            "x": -3.0,
                            "y": 4.1,
                            "yaw": -73.2,
                            "timestamp": 34514.21
                        },
                        {
                            "x": -2.0,
                            "y": 1.2,
                            "yaw": -90.0,
                            "timestamp": 34514.71
                        },
                        {
                            "x": -2.0,
                            "y":-2.8,
                            "yaw": -90.0,
                            "timestamp": 34515.21
                        },
                    ]
                 }
            ]
        },
        {
            "id" : 1,
            "length": 3.4,
            "width" : 1.5,
            "predictions" : [
                {
                    "probability" : 1.0,
                    "trajectory" : [
                        {
                            "x": 10.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34512.71
                        },
                        {
                            "x": 6.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34513.21
                        },
                        {
                            "x": 2.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34513.71
                        },
                        {
                            "x": -2.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34514.21
                        },
                        {
                            "x": -6.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34514.71
                        },
                        {
                            "x": -10.0,
                            "y": 12.1,
                            "yaw": -180.0,
                            "timestamp": 34515.21
                        }
                    ]
                }
            ]
        }
    ]
}
 bool car_ahead = false;
bool car_left = false;
bool car_right = false;
 if(d > 0 && d < 4) {
    check_car_lane = 0;
} else if(d > 4 && d < 8) {
    check_car_lane = 1;
} else if(d > 8 and d < 12) {
    check_car_lane = 2;
}
 if(check_car_lane == lane) {
    //A vehicle is on the same line and check the car is in front of the ego car
    car_ahead |= check_car_s > car_s && (check_car_s - car_s) < 30;										
 } else if((check_car_lane - lane) == -1) {
    //A vehicle is on the left lane and check that is in 30 meter range
    car_left |= (car_s+30) > check_car_s  && (car_s-30) < check_car_s;
 } else if((check_car_lane - lane) == 1) {
    //A vehicle is on the right lane and check that is in 30 meter range
    car_right |= (car_s+30) > check_car_s  && (car_s-30) < check_car_s;
 }
 check_car_s += ((double)prev_size*0.02*check_speed);
 if(car_ahead) {
    if(!car_left && lane > 0) {
        lane--;
    } else if(!car_right && lane !=2) {
        lane++;
    } else if(!car_left && lane !=2) {
        lane++;
    }else {
        ref_vel -= speed_diff;
    }
} else if(ref_vel < max_accel){
    ref_vel += speed_diff;
}
 int prev_size = previous_path_x.size();
 double ref_x = car_x;
double ref_y = car_y;
double ref_yaw = deg2rad(car_yaw);
 if ( prev_size < 2 ) {
     //Use two points thats makes path tangent to the car
    double prev_car_x = car_x - cos(car_yaw);
    double prev_car_y = car_y - sin(car_yaw);
     ptsx.push_back(prev_car_x);
    ptsx.push_back(car_x);
     ptsy.push_back(prev_car_y);
    ptsy.push_back(car_y);
}
 //Redefine the reference point to previous point
ref_x = previous_path_x[prev_size - 1];
ref_y = previous_path_y[prev_size - 1];
 double ref_x_prev = previous_path_x[prev_size - 2];
double ref_y_prev = previous_path_y[prev_size - 2];
ref_yaw = atan2(ref_y-ref_y_prev, ref_x-ref_x_prev);
 ptsx.push_back(ref_x_prev);
ptsx.push_back(ref_x);
 ptsy.push_back(ref_y_prev);
ptsy.push_back(ref_y);
 vector<double> next_wp0 = getXY(car_s + 30, 2 + 4*lane, map_waypoints_s, map_waypoints_x, map_waypoints_y);
vector<double> next_wp1 = getXY(car_s + 60, 2 + 4*lane, map_waypoints_s, map_waypoints_x, map_waypoints_y);
vector<double> next_wp2 = getXY(car_s + 90, 2 + 4*lane, map_waypoints_s, map_waypoints_x, map_waypoints_y);
 ptsx.push_back(next_wp0[0]);
ptsx.push_back(next_wp1[0]);
ptsx.push_back(next_wp2[0]);
 ptsy.push_back(next_wp0[1]);
ptsy.push_back(next_wp1[1]);
ptsy.push_back(next_wp2[1]);
 tk::spline s;
s.set_points(ptsx, ptsy);
 vector<double> next_x_vals;
vector<double> next_y_vals;
//For the smooth transition, we are adding previous path points
for ( int i = 0; i < prev_size; i++ ) {
    next_x_vals.push_back(previous_path_x[i]);
    next_y_vals.push_back(previous_path_y[i]);
}
 double target_x = 30.0;
double target_y = s(target_x);
double target_dist = sqrt(target_x*target_x + target_y*target_y);
 for( int i = 1; i < 50 - prev_size; i++ ) {
              
    double N = target_dist/(0.02*ref_vel/2.24);
    double x_point = x_add_on + target_x/N;
    double y_point = s(x_point);
     x_add_on = x_point;
     double x_ref = x_point;
    double y_ref = y_point;
     //Rotate back to normal after rotating it earlier
    x_point = x_ref * cos(ref_yaw) - y_ref * sin(ref_yaw);
    y_point = x_ref * sin(ref_yaw) + y_ref * cos(ref_yaw);
     x_point += ref_x;
    y_point += ref_y;
     next_x_vals.push_back(x_point);
    next_y_vals.push_back(y_point);
}
 
 
Item 1521: 
Case 0: Standard Manual Back Propagation with ADAM
Case 1: Auto Back Propagation with ADAM
Case 2: Dilated Back Propagation with ADAM Sparse Connection
Case 3: Dilated Back Propagation with ADAM Sparse Connection 2
Case 4: Dilated Back Propagation with ADAM Dense Connection 1
Case 5: Dilated Back Propagation with ADAM Dense Connection 2
Case 6: Dilated Back Propagation with ADAM Dense Connection 3
 Case 0: Red
Case 1: Green
Case 2: Blue
Case 3: Cyan
Case 4: Magenta
Case 5: Yellow
Case 6: Black
 
 
Item 1526: 
void trackPopupUserExperiance(bool userSkippedPopup, String popupId) {
  Bundle params = new Bundle();
  params.putString("popupId", popupId);
  params.putBool("userSkippedPopup", userSkippedPopup);
  mFirebaseAnalytics.logEvent("popup", params);
}
 void trackOptionUserExperiance(String selectedOption, String category) {
  Bundle params = new Bundle();
  params.putString("selectedOption", selectedOption);
  mFirebaseAnalytics.logEvent(category, params);
}
 void trackOrder(User user, String productId) {
  Bundle params = new Bundle();
  params.putString("productId", productId);
  params.putString("gender", user.getGender());
  params.putString("age", user.getAge());
  params.putString("city", user.getAddress().getCity());
  mFirebaseAnalytics.logEvent("order", params);
}
 void trackRecommendedProducts(String productId, bool isRecommended) {
  Bundle params = new Bundle();
  params.putString("productId", productId);
  params.putBool("isRecommended", isRecommended);
  mFirebaseAnalytics.logEvent("recommendedProduct", params);
}
 void trackOrder(String[] productIds) {
  Bundle params = new Bundle();
  for (int i = 0; i < productIds.count(); i++) {
    params.putString(String.valueOf(i), productIds[i]);
  }
  mFirebaseAnalytics.logEvent("order", params);
}
 
 
Item 1530: 
## (1022, 81)
## (438, 81)
 ## [1] 1022   81
## [1] 438  81
 ## (1022, 81)
## (438, 81)
 ## [1] 1024   81
## [1] 436  81
 ## Theta 0: 21905.1315
## Theta 1: 104.0985
 ## [1] "Theta 0: 23102.0244"
## [1] "Theta 1: 103.9400"
 ## Theta 0: 21910.8619
## Theta 1: 104.0947
 ## [1] "Theta 0: 7306.3620"
## [1] "Theta 1: 114.8515"
 ## Theta 0: 21905.1315
## Theta 1: 104.0985
 ## [1] "Theta 0: 23102.0244"
## [1] "Theta 1: 103.9400"
 
 
Item 1531: 
rm(list=ls())
 require(readtext)
require(gutenbergr)
require(quanteda)
 url  <- "http://www.gutenberg.org/cache/epub/4391/pg4391.txt"
data <- texts(readtext(url))
 
 
Item 1539: 
NOT(0) = 1
NOT(1) = 0
 AND(1, 1) = 1
AND(1, 0) = 0
AND(0, 1) = 0
AND(0, 0) = 0
 OR(1, 1) = 1
OR(1, 0) = 1
OR(0, 1) = 1
OR(0, 0) = 0
 XOR(1, 1) = 0
XOR(1, 0) = 1
XOR(0, 1) = 1
XOR(0, 0) = 0
 
 
Item 1540: 
Converting unsupported operation: Variable...
 
 
Item 1542: 

install.packages(“ggplot2”)


 
install.packages(“shiny”)

 
install.packages(“tidyr”)

 
install.packages(“dplyr”)

 
install.packages(“httr”)

 
install.packages(“rvest”)

 
 
Item 1544: 
=== Predictions on test set ===
     inst#     actual  predicted error prediction
        1        1:?      1:yes       0.706
 @ATTRIBUTE animal {aardvark,antelope,bass,bear,boar,buffalo,calf,carp,catfish,cavy,cheetah,chicken,chub,clam,crab,crayfish,crow,deer,dogfish,dolphin,dove,duck,elephant,flamingo,flea,frog,fruitbat,giraffe,girl,gnat,goat,gorilla,gull,haddock,hamster,hare,hawk,herring,honeybee,housefly,kiwi,ladybird,lark,leopard,lion,lobster,lynx,mink,mole,mongoose,moth,newt,octopus,opossum,oryx,ostrich,parakeet,penguin,pheasant,pike,piranha,pitviper,platypus,polecat,pony,porpoise,puma,pussycat,raccoon,reindeer,rhea,scorpion,seahorse,seal,sealion,seasnake,seawasp,skimmer,skua,slowworm,slug,sole,sparrow,squirrel,starfish,stingray,swan,termite,toad,tortoise,tuatara,tuna,vampire,vole,vulture,wallaby,wasp,wolf,worm,wren}
@ATTRIBUTE hair {false, true}
@ATTRIBUTE feathers {false, true}
@ATTRIBUTE eggs {false, true}
@ATTRIBUTE milk {false, true}
@ATTRIBUTE airborne {false, true}
@ATTRIBUTE aquatic {false, true}
@ATTRIBUTE predator {false, true}
@ATTRIBUTE toothed {false, true}
@ATTRIBUTE backbone {false, true}
@ATTRIBUTE breathes {false, true}
@ATTRIBUTE venomous {false, true}
@ATTRIBUTE fins {false, true}
@ATTRIBUTE legs INTEGER [0,9]
@ATTRIBUTE tail {false, true}
@ATTRIBUTE domestic {false, true}
@ATTRIBUTE catsize {false, true}
@ATTRIBUTE type { mammal, bird, reptile, fish, amphibian, insect, invertebrate }
 @DATA
%
% Instances (101):
%
aardvark,true,false,false,true,false,false,true,true,true,true,false,false,4,false,false,true,mammal
 aardvark,true,false,false,true,false,false,true,true,true,true,false,false,4,false,false,true,?
 @RELATION zoo
 @ATTRIBUTE animal {kangaroo}
@ATTRIBUTE hair {false, true}
@ATTRIBUTE feathers {false, true}
@ATTRIBUTE eggs {false, true}
@ATTRIBUTE milk {false, true}
@ATTRIBUTE airborne {false, true}
@ATTRIBUTE aquatic {false, true}
@ATTRIBUTE predator {false, true}
@ATTRIBUTE toothed {false, true}
@ATTRIBUTE backbone {false, true}
@ATTRIBUTE breathes {false, true}
@ATTRIBUTE venomous {false, true}
@ATTRIBUTE fins {false, true}
@ATTRIBUTE legs INTEGER [0,9]
@ATTRIBUTE tail {false, true}
@ATTRIBUTE domestic {false, true}
@ATTRIBUTE catsize {false, true}
@ATTRIBUTE type { mammal, bird, reptile, fish, amphibian, insect, invertebrate }
 @DATA
kangaroo,true,false,false,true,false,false,false,true,true,true,false,false,2,true,false,false,?
 
 
Item 1546: 
#install.packages("devtools") If devtools is not available
devtools::install_github("bnosac/image", subdir = "image.darknet", build_vignettes = TRUE)
 yolo_tiny_voc <- image_darknet_model(type = ‘detect’, 
 model = “tiny-yolo-voc.cfg”, 
 weights = system.file(package=”image.darknet”, “models”, “tiny-yolo-voc.weights”), 
 labels = system.file(package=”image.darknet”, “include”, “darknet”, “data”, “voc.names”))
 x <- image_darknet_detect(file = "/Documents/google-car.png", 
                          object = yolo_tiny_voc,
                          threshold = 0.19)
 
 
Item 1547: 
Layer (type)                 Output Shape              Param #
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688
_________________________________________________________________
global_average_pooling2d_1 ( (None, 512)               0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328
_________________________________________________________________
dropout_1 (Dropout)          (None, 256)               0
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514
=================================================================
Total params: 14,846,530
Trainable params: 131,842
Non-trainable params: 14,714,688
_________________________________________________________________
 
 
Item 1549: 
msf = quandl.get('WIKI/MSFT', start_date="2014-01-01", end_date="2018-08-20")
 For 1..n episodes:
  Until environment says "done":
    Run the current state through the network
    That yields:
      a set of "probabilities" for each action
      a value for how good the state is
     Sample from those probabilities to pick an action
     Act upon the environment with that action
    Save the state, action, reward from environment to a buffer
   Adjust each saved reward by the value of the terminal state
  discounted by gamma each timestep
   Loss for the episode is the sum over all steps of:
    The log probability of the action we took * its reward
    The value of that action compared to what was expected
   Propagate the loss back and adjust network parameters
 [AAPL holdings, 
MSFT holdings, 
cash, 
current timestep's AAPL opening price, 
current timestep's MSFT opening price, 
current portfolio value, 
past 5 day average AAPL opening price, 
past 5 day average MSFT opening price]
 
 
Item 1550: 
Ground truth:        "the fake friend of the family, like the"
Best path decoding:  "the fak friend of the fomly hae tC"
Beam search:         "the fak friend of the fomcly hae tC"
Beam search with LM: "the fake friend of the family, lie th"
 
 
Item 1553: 
Most papers by author:
 1. tong zhang: 8 (tencent labs)
2. lawrence carin: 7 (duke / infinia ml)
3. jun zhu: 6 (tsinghua university)
3. quanquan gu: 6 (uva)
3. le song: 6 (ga tech)
3. remi munos: 6 (nyu / courant institute)
3. pieter abbeel: 6 (uc berkeley)
3. sergey levine: 6 (uc berkeley)
4. bernhard schölkopf: 5 (max planck institute)
4. eric xing: 5 (carnegie mellon)
4. amin karbasi: 5 (yale)
4. shimon whiteson: 5 (oxford)
 Most papers by institution:
 google: 43
berkeley: 33
deepmind: 33
cmu: 32
stanford: 29
mit: 28
microsoft: 28
google ai: 24
facebook: 21
oxford: 20
princeton: 20
inria: 18
epfl: 17
eth zurich: 16
cornell: 16
max planck institute: 15
ut austin: 15
cambridge: 15
ucla: 15
university of toronto: 15
columbia: 13
tsinghua university: 12
amazon: 11
harvard: 11
tencent: 11
usc: 11
georgia tech: 11
duke: 11
peking university: 10
purdue: 10
riken: 10
uber: 10
yale: 9
nyu: 9
university of wisconsin: 9
mila: 8
paristech: 8
imperial college london: 8
johns hopkins university: 8
uiuc: 8
university of virginia: 8
rice: 8
ibm: 7
technion: 7
openai: 7
vector institute: 7
university of washington: 7
cnrs: 7
caltech: 7
google research: 7
university of chicago: 6
university of tubingen: 6
alan turing institute: 6
northwestern: 6
university of montreal: 6
uc davis: 6
university of pennsylvania: 6
university of edinburgh: 6
 Most papers by universities:
 berkeley: 33
cmu: 32
stanford: 29
mit: 28
oxford: 20
princeton: 20
epfl: 17
eth zurich: 16
cornell: 16
ut austin: 15
cambridge: 15
ucla: 15
university of toronto: 15
columbia: 13
tsinghua university: 12
harvard: 11
usc: 11
georgia tech: 11
duke: 11
peking university: 10
purdue: 10
yale: 9
nyu: 9
university of wisconsin: 9
 Total papers by commercial institution:
 google: 43
deepmind: 33
microsoft: 28
google ai: 24
facebook: 21
tencent: 11
amazon: 11
uber: 10
infinia ml: 7
google research: 7
ibm: 7
disney research: 4
apple: 3
yahoo research: 3
data61: 3
ant financial: 2
ntt: 2
criteo: 2
yandex: 2
toyota technological institute: 2
prowler.io: 2
element ai: 2
rj research consulting: 1
qihoo/360: 1
alibaba group: 1
mass general hospital: 1
hellebore capital limited: 1
preferred networks, inc.: 1
dolores technologies: 1
hikvision research institue: 1
mitsubishi electric research labs: 1
labsix: 1
covariant.ai: 1
nvidia: 1
bosch: 1
zalando research: 1
d. e. shaw \& co.: 1
intel: 1
twitter: 1
curai: 1
two sigma investments: 1
bae systems fast labs: 1
aitrics: 1
recruit technologies co. ltd.: 1
siemens ag: 1
ea seed: 1
autodesk: 1
4paradigm: 1
petuum inc: 1
philips research: 1
sentient technologies: 1
snap: 1
orange labs: 1
qualcomm india private limited: 1
rwth: 1
ign: 1
axa im chorus: 1
baidu: 1
afresh technologies: 1
d-wave: 1
telefonica: 1
huawei: 1
saudi aramco: 1
rolls-royce: 1
bytedance inc.: 1
quadrant.ai: 1
magic leap: 1
guo: 1
 Most NIPS + ICML papers:
 lawrence carin (duke university): 17
tong zhang (tencent labs): 12 
le song (georgia institute of technology): 11
michael jordan (university of california, berkeley): 9
nicolas heess (deepmind): 8
razvan pascanu (deepmind): 8
 
 
Item 1556: 
However, I must admit, in that image, a couple of things do not make perfect sense to me. For example, bottom row, third image from the left. It can't or at least shouldn't have the same score as the first two since in that particular scenario, the black rook is at obvious risk. Fifth from the left - same.
 But, to drive my point home, that image serves its purpose, so let us ignore the technicalities here. I am sure there is a logic behind it. I have to admit, I didn't read it much; not a techie.
 
We would skip on Activation function. What does it do? Standardizes output from each neuron, making the model more accurate.

 I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1559: 
print('Master Sentence: %s' % news_headlines[0])
for i, news_headline in enumerate(news_headlines):
    score = sklearn.metrics.pairwise.euclidean_distances([transformed_results[i]], [transformed_results[0]])[0][0]
    print('-----')
    print('Score: %.2f, Comparing Sentence: %s' % (score, news_headline))
 Master Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 0.00, Comparing Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 1.73, Comparing Sentence: Elon Musk's Boring Company to build high-speed Chicago airport link
-----
Score: 4.36, Comparing Sentence: Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport
-----
Score: 4.24, Comparing Sentence: Both apple and orange are fruit
 print('Master Sentence: %s' % news_headlines[0])
for i, news_headline in enumerate(news_headlines):
    score = sklearn.metrics.pairwise.cosine_similarity([transformed_results[i]], [transformed_results[0]])[0][0]
    print('-----')
    print('Score: %.2f, Comparing Sentence: %s' % (score, news_headline))
 Master Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 1.00, Comparing Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 0.87, Comparing Sentence: Elon Musk's Boring Company to build high-speed Chicago airport link
-----
Score: 0.44, Comparing Sentence: Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport
-----
Score: 0.00, Comparing Sentence: Both apple and orange are fruit
 print('Master Sentence: %s' % news_headlines[0])
for i, news_headline in enumerate(news_headlines):
    y_compare = calculate_position(transformed_results[i])
    x1, x2 = padding(y_actual, y_compare)
    score = sklearn.metrics.jaccard_similarity_score(x1, x2)
    print('-----')
    print('Score: %.2f, Comparing Sentence: %s' % (score, news_headline))
 Master Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 1.00, Comparing Sentence: Elon Musk's Boring Co to build high-speed airport link in Chicago
-----
Score: 0.67, Comparing Sentence: Elon Musk's Boring Company to build high-speed Chicago airport link
-----
Score: 0.17, Comparing Sentence: Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport
-----
Score: 0.00, Comparing Sentence: Both apple and orange are fruit
 
 
Item 1561: 
curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-multi.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?model=en-US_NarrowbandModel&speaker_labels=true"
 {
  "results": [
    {
      "alternatives": [
        {
          "timestamps": [
            [
              "hello",
              0.68,
              1.19
            ],
            [
              "yeah",
              1.47,
              1.93
            ],
            [
              "yeah",
              1.96,
              2.12
            ],
            [
              "how's",
              2.12,
              2.59
            ],
            [
              "Billy",
              2.59,
              3.17
            ],
            . . .
          ]
          "confidence": 0.821,
          "transcript": "hello yeah yeah how's Billy "
        }
      ],
      "final": true
    }
  ],
  "result_index": 0,
  "speaker_labels": [
    {
      "from": 0.68,
      "to": 1.19,
      "speaker": 2,
      "confidence": 0.418,
      "final": false
    },
    {
      "from": 1.47,
      "to": 1.93,
      "speaker": 1,
      "confidence": 0.521,
      "final": false
    },
    {
      "from": 1.96,
      "to": 2.12,
      "speaker": 2,
      "confidence": 0.407,
      "final": false
    },
    {
      "from": 2.12,
      "to": 2.59,
      "speaker": 2,
      "confidence": 0.407,
      "final": false
    },
    {
      "from": 2.59,
      "to": 3.17,
      "speaker": 2,
      "confidence": 0.407,
      "final": false
    },
    . . .
  ]
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?keywords=%22colorado%22%2C%22tornado%22%2C%22tornadoes%22&keywords_threshold=0.5"
 {
  "results": [
    {
      "keywords_result": {
        "colorado": [
          {
            "normalized_text": "Colorado",
            "start_time": 4.94,
            "confidence": 0.913,
            "end_time": 5.62
          }
        ],
        "tornadoes": [
          {
            "normalized_text": "tornadoes",
            "start_time": 1.52,
            "confidence": 1.0,
            "end_time": 2.15
          }
        ]
      },
      "alternatives": [
        {
          "confidence": 0.891,
          "transcript": "several tornadoes touch down as a line of severe thunderstorms swept through Colorado on Sunday "
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?max_alternatives=3"
 {
  "results": [
    {
      "alternatives": [
        {
          "confidence": 0.891,
          "transcript": "several tornadoes touch down as a line of severe thunderstorms swept through Colorado on Sunday "
        },
        {
          "transcript": "several tornadoes touched down as a line of severe thunderstorms swept through Colorado on Sunday "
        },
        {
          "transcript": "several tornadoes touch down is a line of severe thunderstorms swept through Colorado on Sunday "
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?word_alternatives_threshold=0.9"
 {
  "results": [
    {
      "word_alternatives": [
        {
          "start_time": 1.01,
          "alternatives": [
            {
              "confidence": 1.0,
              "word": "several"
            }
          ],
          "end_time": 1.52
        },
        {
          "start_time": 1.52,
          "alternatives": [
            {
              "confidence": 1.0,
              "word": "tornadoes"
            }
          ],
          "end_time": 2.15
        },
        {
          "start_time": 3.39,
          "alternatives": [
            {
              "confidence": 0.9634,
              "word": "severe"
            }
          ],
          "end_time": 3.77
        },
        {
          "start_time": 3.77,
          "alternatives": [
            {
              "confidence": 0.991,
              "word": "thunderstorms"
            }
          ],
          "end_time": 4.51
        },
        {
          "start_time": 4.51,
          "alternatives": [
            {
              "confidence": 0.9729,
              "word": "swept"
            }
          ],
          "end_time": 4.81
        }
      ],
      "alternatives": [
        {
          "confidence": 0.891,
          "transcript": "several tornadoes touch down as a line of severe thunderstorms swept through Colorado on Sunday "
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?word_confidence=true"
 {
  "results": [
    {
      "alternatives": [
        {
          "transcript": "several tornadoes touch down as a line of severe thunderstorms swept through Colorado on Sunday ",
          "confidence": 0.891,
          "word_confidence": [
            [
              "several",
              1.0
            ],
            [
              "tornadoes",
              1.0
            ],
            [
              "touch",
              0.52
            ],
            [
              "down",
              0.904
            ],
            . . .
            [
              "on",
              0.311
            ],
            [
              "Sunday",
              0.986
            ]
          ]
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?timestamps=true"
 {
  "results": [
    {
      "alternatives": [
        {
          "timestamps": [
            [
              "several",
              1.01,
              1.52
            ],
            [
              "tornadoes",
              1.52,
              2.15
            ],
            [
              "touch",
              2.15,
              2.5
            ],
            [
              "down",
              2.5,
              2.81
            ],
            . . .
            [
              "on",
              5.62,
              5.74
            ],
            [
              "Sunday",
              5.74,
              6.34
            ]
          ],
          "confidence": 0.891,
          "transcript": "several tornadoes touch down as a line of severe thunderstorms swept through Colorado on Sunday "
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?word_alternatives_threshold=0.99&word_confidence=true&timestamps=true"
 {
  "results": [
    {
      "word_alternatives": [
        {
          "start_time": 0.03,
          "alternatives": [
            {
              "confidence": 1.0,
              "word": "****"
            }
          ],
          "end_time": 0.25
        },
        {
          "start_time": 0.25,
          "alternatives": [
            {
              "confidence": 0.9976,
              "word": "you"
            }
          ],
          "end_time": 0.56
        }
      ],
      "alternatives": [
        {
          "transcript": "**** you",
          "confidence": 0.992,
          "word_confidence": [
            ["****", 0.9999999999999918],
            ["you", 0.986436366840706]
          ],
          "timestamps": [
            ["****", 0.03, 0.25],
            ["you", 0.25, 0.56]
          ]
        }
      ],
      "final": true
    }
  ],
  "result_index": 0
}
 curl -X POST -u {username}:{password}
--header "Content-Type: audio/flac"
--data-binary @{path}audio-file.flac
"https://stream.watsonplatform.net/speech-to-text/api/v1/recognize?smart_formatting=true"
 
 
Item 1562: 
https://api.telegram.org/bot{Token}/setWebhook?url={Kata_Webhook}
 https://api.telegram.org/bot5723XXXXXXXXXXGH5AU/setWebhook?url=https://kanal.kata.ai/xxxxx/xx49705811bcde
 
 
Item 1564: 
Author’s note: If you have read the superb book Life 3.0: Being Human in the Age of Artificial Intelligence by Max Tegmark or the excellent pair of blog posts by Tim Urban at Wait But Why, then you can probably skip this post. They were both very inspirational to this post and my views on where Artificial Intelligence (AI) fits within the context of our current and future world. If you have not yet read these, I encourage you to read on. I want to ensure my friends, family, acquaintances—really everyone—is thinking and talking about this, as it is the most important topic facing us. 
 I know this is a long post. If you can't commit, I have broken this post up into six smaller ones starting with this one.
 As you read this post, keep one thing in mind: while the predicted timeframe may be off by a few years (or decades) in some cases, all of these things are likely to happen. We are at the precipice of an amazing time in human history!
 
 
Item 1571: 
mnist = input_data.read_data_sets(‘MNIST_data’, one_hot=True)
 mnist = input_data.read_data_sets(‘MNIST_data’)
 feature_columns = [tf.feature_column.numeric_column(“x”, shape=[28, 28])]
 classifier = tf.estimator.DNNClassifier(
 feature_columns=feature_columns,
 hidden_units=[256, 32],
 optimizer=tf.train.AdamOptimizer(1e-4),
 n_classes=10,
 dropout=0.1,
 model_dir=”./tmp/mnist_model”
)
 def input(dataset):
 return dataset.images, dataset.labels.astype(np.int32)
 train_input_fn = tf.estimator.inputs.numpy_input_fn(
 x={“x”: input(mnist.train)[0]},
 y=input(mnist.train)[1],
 num_epochs=None,
 batch_size=50,
 shuffle=True
)
 classifier.train(input_fn=train_input_fn, steps=100000)
 test_input_fn = tf.estimator.inputs.numpy_input_fn(
 x={“x”: input(mnist.test)[0]},
 y=input(mnist.test)[1],
 num_epochs=1,
 shuffle=False
)
 accuracy_score = classifier.evaluate(input_fn=test_input_fn)[“accuracy”]
print(“\nTest Accuracy: {0:f}%\n”.format(accuracy_score*100))
 
 
Item 1572: 
dataset = tf.data.Dataset.from_tensor_slices()
 
 
Item 1575: 
array([[1, 3, 5],
       [5, 4, 1],
       [3, 8, 6]])
 array([[ 2.66666667, 0.66666667, -2.66666667],
       [ 0.66666667, 4.66666667, 2.33333333],
       [-2.66666667, 2.33333333, 4.66666667]])
 array([[ 2.66666667, 0.66666667, -2.66666667],
       [ 0.66666667, 4.66666667, 2.33333333],
       [-2.66666667, 2.33333333, 4.66666667]])
 (300, 2)
 array([[ 2.47143516, 1.52704645],
       [ 0.80902431, 1.7111124 ],
       [ 3.43270697, 0.78245452],
       [ 1.6873481 , 3.63779121],
       [ 1.27941127, -0.74213763],
       [ 2.88716294, 0.90556519],
       [ 2.85958841, 2.43118375],
       [ 1.3634765 , 1.59275845],
       [ 2.01569637, 1.1702969 ],
       [-0.24268495, -0.75170595]])
 Covariance matrix:
[[ 0.95171641 -0.0447816 ]
 [-0.0447816 0.87959853]]
 Covariance matrix:
[[ 0.95171641 0.92932561]
 [ 0.92932561 1.12683445]]
 Before:
 Covariance matrix:
[[ 0.95171641 0.92932561]
 [ 0.92932561 1.12683445]]
 After:
 Covariance matrix:
[[ 0.95171641 0.92932561]
 [ 0.92932561 1.12683445]]
 Covariance matrix:
[[ 0.95171641 0.83976242]
 [ 0.83976242 6.22529922]]
 Covariance matrix:
[[ 1.          0.34500274]
 [ 0.34500274  1.        ]]
 Covariance matrix:
[[ 0.95171641  0.83976242]
 [ 0.83976242  6.22529922]]
 Covariance matrix:
[[ 0.95171641 0.83976242]
 [ 0.83976242 6.22529922]]
 Covariance matrix:
[[ 5.96126981e-01 -1.48029737e-16]
 [ -1.48029737e-16 3.15205774e+00]]
 Covariance matrix:
[[ 9.99983225e-01 -1.06581410e-16]
 [ -1.06581410e-16 9.99996827e-01]]
 (50000, 32, 32, 3)
 (1000, 32, 32, 3)
 (1000, 3072)
 X.min() 0.0
X.max() 1.0
 (3072,)
 array([ 0.5234 , 0.54323137, 0.5274 , …, 0.50369804,
 0.50011765, 0.45227451])
 array([ -5.30575583e-16, -5.98021632e-16, -4.23439062e-16, …,
 -1.81965554e-16, -2.49800181e-16, 3.98570066e-17])
 (1000, 1000) (1000,)
 [[ 8.15846654e+00 0.00000000e+00 0.00000000e+00 …, 0.00000000e+00
 0.00000000e+00 0.00000000e+00]
 [ 0.00000000e+00 4.68234845e+00 0.00000000e+00 …, 0.00000000e+00
 0.00000000e+00 0.00000000e+00]
 [ 0.00000000e+00 0.00000000e+00 2.41075267e+00 …, 0.00000000e+00
 0.00000000e+00 0.00000000e+00]
 …, 
 [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 …, 3.92727365e-05
 0.00000000e+00 0.00000000e+00]
 [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 …, 0.00000000e+00
 3.52614473e-05 0.00000000e+00]
 [ 0.00000000e+00 0.00000000e+00 0.00000000e+00 …, 0.00000000e+00
 0.00000000e+00 1.35907202e-15]]
 shape: (1000, 1000)
 min: 0.0
max: 1.0
 
 
Item 1578: 
python -m pysc2.bin.agent \
--map Simple64 \
--agent refined_agent.SparseAgent \
--agent_race T \
--max_agent_steps 0 \
--norender
 
 
Item 1580: 
python -m pysc2.bin.agent \
--map Simple64 \
--agent attack_agent.AttackAgent \
--agent_race T \
--max_agent_steps 0 \
--norender
 
 
Item 1586: 
{
"duration": 60,
"query": ["Bitcoin", "BTC", "Cryptocurrency", "Crypto"],
"translate": false
}
 
 
Item 1588: 
ransac = linear_model.RANSACRegressor()
ransac.fit(X, y)
inlier_mask = ransac.inlier_mask_
outlier_mask = np.logical_not(inlier_mask)
 
 
Item 1589: 
install.packages("twitter")
 install.packages("ROAuth")
 library("twitter")
 library("ROAuth")
 cred <- OAuthFactory$new(consumerKey='XXXXXXXXXXXXXXXXXX',
 consumerSecret='XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX',
 requestURL='https://api.twitter.com/oauth/request_token',
 accessURL='https://api.twitter.com/oauth/access_token',
 authURL='https://api.twitter.com/oauth/authorize')
 tweets$created <- as.Date(tweets$created, format= "%y-%m-%d")
 tweets$text <- as.character(tweets$text)
 #Data Pre-processing
 set.seed(100)
 (n.tweet <- length(tweets))
 ## [1] 17
 corpus<- Corpus(VectorSource(tweets$text))
 corpus[[1]]
 removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
 corpus <- tm_map(corpus, content_transformer(removeURL))
 remove_punct<-function(x)gsub("[^[:alpha:][:space:]]*", "", x)
 corpus <- tm_map(corpus, content_transformer(remove_punct))
 corpus = tm_map(corpus, stripWhitespace)
 removeUsername <- function(x) gsub("@[^[:space:]]*", "", x)
 corpus <- tm_map(corpus, content_transformer(removeUsername))
 writeLines(strwrap(corpus[[750]]$content,60))
 ## RT Nike Dont ask if your dreams are crazy Ask if theyre
 ## crazy enough justdoit
 corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
 writeLines(strwrap(corpus[[750]]$content,60))
 ## rt nike dont ask if your dreams are crazy ask if theyre
 ## crazy enough justdoit
 corpus = tm_map(corpus, removeWords, stopwords('English'))
 inspect(corpus)
 ##    [1] rt nike dont ask   dreams  crazy ask crazy theyre enough justdoit
 ##    [2] rt statecrafthuff paint kap   person    people even   demonizes good cops  trys  sell americans   false narra
 Corpus<-tm_map(corpus, removeWords, c("keep", "check", "can","just","isnt","hey","ask","theyr","dont","theyre","cmon","htt","everything","even","enough","rt"))
 removeSingle <- function(x) gsub(" . ", " ", x)
 corpus <- tm_map(corpus, content_transformer(removeSingle))
 writeLines(strwrap(corpus[[750]]$content,60))
 ## nike dreams crazy crazy justdoit
 dtm <- TermDocumentMatrix(corpus)
 barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "blue",main = "nike",border = "black")
 barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "yellow",main = "justdoit",border = "black")
 
 
Item 1591: 
## Markdown Code to create an interactive heatmap using d3heatmap
 ```
library(knitr) 
library(d3heatmap) 
library(flexdashboard)
 url <- "http://datasets.flowingdata.com/ppg2008.csv" 
nba_players <- read.csv(url, row.names = 1) 
```
 ### Stats by Player {data-width=650}  
```{r} 
d3heatmap(nba_players, scale = "column") 
```
 
 
Item 1593: 
Stochastic Gradient Descent - Algorithm
 For each example in the data
    - find the value predicted by the neural network 
    - calculate the loss from the loss function 
    - find partial derivatives of the loss function, these partial derivatives produce gradients
    - use the gradients to update the values of weights and biases
 
 
Item 1594: 
#installing magick package from CRAN
install.packages('magick') 
#from Github ropensci library - note: requires RTools
devtools::install_github('ropensci/magick')
 #Loading magick package
library(magick)
 #reading a png image frink image
frink <- image_read("https://jeroen.github.io/images/frink.png")
#reading a jpg image ex-President Barack Obama from Wikipedia
obama <- image_read('https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/President_Barack_Obama.jpg/800px-President_Barack_Obama.jpg')
 #image details
image_info(obama)
image_info(frink)
image_info(obama)
  format width height colorspace filesize
1   JPEG   800    999       sRGB   151770
 image_info(frink)
format width height colorspace filesize
1    PNG   220    445       sRGB    73494
 #displaying the image
print(obama)
 #Rendering JPG image into SVG
image_write(obama, path = 'obama.svg', format = 'svg')
 #Applying Charcoal effect to Obama's image 
#and compositing it with frink's image
#and finally annotating it with a text
image_charcoal(obama) %>% 
  image_composite(frink)  %>%
  image_annotate("CONFIDENTIAL", size = 50, color = "red", boxcolor = "pink",
                 degrees = 30, location = "+100+100") %>%
  image_rotate(30) %>%
  image_write('obama_with_frink.png','png')
 library(ggplot2)
img <- image_graph(600, 400, res = 96)
p <- ggplot(iris) + geom_point(aes(Sepal.Length,Petal.Length))
print(p)
dev.off()
img %>% image_annotate("CONFIDENTIAL", size = 50, color = "red", boxcolor = "pink",
                       degrees = 30, location = "+100+100")  %>%
  image_write('conf_ggplot.png','png')
 
 
Item 1595: 
CALL embedding.deepgl("NodeLabel" ,"RelationshipType", {
  nodeFeatures: [string]
  pruningLambda: double,
  diffusions: integer, 
  iterations: integer,
  writeProperty: string
})
 
 
Item 1598: 
+--------+--------------+-------------+
| Person | Height(inch) | Weight(lbs) |
+--------+--------------+-------------+
| A      |           72 |         186 |
| B      |           69 |         205 |
| C      |           70 |         201 |
| D      |           62 |         125 |
| E      |           57 |          89 |
+--------+--------------+-------------+
 height_df = pd.DataFrame(data=[['A',72],['B',69],['C',70],['D',62],['E',57]], columns=['Person','Height'])
weight_df = pd.DataFrame(data=[['A',186],['B',205],['C',201],['D',125],['E',89]], columns=['Person','Weight'])
 height_df.head(n=3)
 weight_df.head()
 height_weight_df = pd.merge(height_df,weight_df,on='Person')
 height_weight_df.describe()
 height_weight_df.info()
 <class 'pandas.core.frame.DataFrame'>
Int64Index: 5 entries, 0 to 4
Data columns (total 3 columns):
Person    5 non-null object
Height    5 non-null int64
Weight    5 non-null int64
dtypes: int64(2), object(1)
memory usage: 160.0+ bytes
 height_weight_df.set_index(['Person'])
 height_weight_df.set_index(['Person'],inplace=True)
 <class 'pandas.core.frame.DataFrame'>
Index: 5 entries, A to E
Data columns (total 2 columns):
Height    5 non-null int64
Weight    5 non-null int64
dtypes: int64(2)
memory usage: 120.0+ bytes
 Person
A    72
B    69
C    70
D    62
E    57
Name: Height, dtype: int64
 Height     72
Weight    186
Name: A, dtype: int64
 missing_df = pd.DataFrame(data=[[65,121],[60]],index=['F','G'],columns=['Height','Weight'])
 updated_df = height_weight_df.append(missing_df)
 updated_df['Height'] += 5
updated_df['Weight'] -= 5
 updated_df.dropna(inplace=True)
 lbs_to_kg_ratio = 0.453592
inch_to_meter_ratio = 0.0254
 updated_df['Height'] *= inch_to_meter_ratio
updated_df['Weight'] *= lbs_to_kg_ratio
 updated_df['BMI'] = updated_df['Weight']/(updated_df['Height']**2)
 updated_df.to_csv('BMI.csv',index_label='Person')
 travel_df = pd.read_csv('travel-times.csv')
 travel_df.head()
 travel_df.describe()
 travel_df.info()
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 205 entries, 0 to 204
Data columns (total 13 columns):
Date              205 non-null object
StartTime         205 non-null object
DayOfWeek         205 non-null object
GoingTo           205 non-null object
Distance          205 non-null float64
MaxSpeed          205 non-null float64
AvgSpeed          205 non-null float64
AvgMovingSpeed    205 non-null float64
FuelEconomy       188 non-null object
TotalTime         205 non-null float64
MovingTime        205 non-null float64
Take407All        205 non-null object
Comments          24 non-null object
dtypes: float64(6), object(7)
memory usage: 20.9+ KB
 travel_df.drop(labels=['Comments'],axis=1,inplace=True)
 travel_df.head()
 travel_df.info()
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 205 entries, 0 to 204
Data columns (total 12 columns):
Date              205 non-null object
StartTime         205 non-null object
DayOfWeek         205 non-null object
GoingTo           205 non-null object
Distance          205 non-null float64
MaxSpeed          205 non-null float64
AvgSpeed          205 non-null float64
AvgMovingSpeed    205 non-null float64
FuelEconomy       188 non-null object
TotalTime         205 non-null float64
MovingTime        205 non-null float64
Take407All        205 non-null object
dtypes: float64(6), object(6)
memory usage: 19.3+ KB
 travel_df.head(n=10)
 travel_df['FuelEconomy'].dtype
 travel_df['FuelEconomy'] = pd.to_numeric(travel_df['FuelEconomy'],errors='coerce')
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 205 entries, 0 to 204
Data columns (total 12 columns):
Date              205 non-null object
StartTime         205 non-null object
DayOfWeek         205 non-null object
GoingTo           205 non-null object
Distance          205 non-null float64
MaxSpeed          205 non-null float64
AvgSpeed          205 non-null float64
AvgMovingSpeed    205 non-null float64
FuelEconomy       186 non-null float64
TotalTime         205 non-null float64
MovingTime        205 non-null float64
Take407All        205 non-null object
dtypes: float64(7), object(5)
memory usage: 19.3+ KB
 travel_df['FuelEconomy'].fillna(travel_df['FuelEconomy'].mean(),inplace=True)
 <class 'pandas.core.frame.DataFrame'>
RangeIndex: 205 entries, 0 to 204
Data columns (total 12 columns):
Date              205 non-null object
StartTime         205 non-null object
DayOfWeek         205 non-null object
GoingTo           205 non-null object
Distance          205 non-null float64
MaxSpeed          205 non-null float64
AvgSpeed          205 non-null float64
AvgMovingSpeed    205 non-null float64
FuelEconomy       205 non-null float64
TotalTime         205 non-null float64
MovingTime        205 non-null float64
Take407All        205 non-null object
dtypes: float64(7), object(5)
memory usage: 19.3+ KB
 Date              1/6/2012
StartTime            16:37
DayOfWeek           Friday
GoingTo               Home
Distance             51.29
MaxSpeed             127.4
AvgSpeed              78.3
AvgMovingSpeed        84.8
FuelEconomy        8.69059
TotalTime             39.3
MovingTime            36.3
Take407All              No
Name: 0, dtype: object
 travel_df_by_date = travel_df.groupby(['Date'])
 travel_df_by_date_combined = travel_df_by_date.sum()
 travel_df_by_date = travel_df.groupby(['Date','DayOfWeek'])
travel_df_by_date_combined = travel_df_by_date.sum()
 travel_df_by_date_combined['MaxSpeed'] = travel_df_by_date['MaxSpeed'].max() travel_df_by_date_combined['AvgSpeed'] = travel_df_by_date['AvgSpeed'].mean() travel_df_by_date_combined['AvgMovingSpeed'] = travel_df_by_date['AvgMovingSpeed'].mean() travel_df_by_date_combined['FuelEconomy'] = travel_df_by_date['FuelEconomy'].mean()
 <class 'pandas.core.frame.DataFrame'>
MultiIndex: 111 entries, (1/2/2012, Monday) to (9/8/2011, Thursday)
Data columns (total 7 columns):
Distance          111 non-null float64
MaxSpeed          111 non-null float64
AvgSpeed          111 non-null float64
AvgMovingSpeed    111 non-null float64
FuelEconomy       111 non-null float64
TotalTime         111 non-null float64
MovingTime        111 non-null float64
dtypes: float64(7)
memory usage: 7.4+ KB
 travel_df_by_date_combined.reset_index(inplace=True)
 travel_df_by_date_combined.tail()
 travel_df_by_date_combined.set_index(['Date'],inplace=True)
 distance_above_ninety = travel_df_by_date_combined['Distance']>90
 distance_below_hundred = travel_df_by_date_combined['Distance']<100
 ninety_to_hundred_df = travel_df_by_date_combined[distance_above_ninety & distance_below_hundred]
 ninety_to_hundred_df.loc[['8/26/2011'],['Distance','FuelEconomy']]
 over_ninety_on_friday = travel_df_by_date_combined[(travel_df_by_date_combined['AvgMovingSpeed']>90) & (travel_df_by_date_combined['DayOfWeek']=='Friday')]
 max_speed_over_one_thirty_five_or_fuel_economy_below_eight = travel_df_by_date_combined[(travel_df_by_date_combined['MaxSpeed']>135) | (travel_df_by_date_combined['FuelEconomy']<8)]
 travel_df.groupby(['DayOfWeek']).sum()
 travel_df.groupby(['DayOfWeek']).max()
 travel_df.groupby(['DayOfWeek'])['AvgMovingSpeed'].describe()
 
 
Item 1599: 
0x317B5D5dCf9bFcC6799BD76571C4171bdB40916a
 [{"constant":false,"inputs":[{"name":"receipent","type":"address"}],"name":"claimTokens","outputs":[],"payable":false,"stateMutability":"nonpayable","type":"function"}]
 0
 recommend at least 200000 (unused gas will be returned by Ethereum Network)
 m/44'/60'/0
 0xbCdfE338D55c061C084D81fD793Ded00A27F226D
 
 
Item 1600: 
{
    17: {
            13: 25,
            16: 50
        }
}
 
 
Item 1602: 
model.compile(loss=’categorical_crossentropy’,
              optimizer=’sgd’,
              metrics=[‘accuracy’])
 model.fit(X_train, y_train, epochs=5, batch_size=100)
 model.add(Dense(10, activation=’relu’, input_shape=(784,)))
 model.add(Dense(512, activation=’relu’, input_shape=(784,)))
 keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0,      
                     nesterov=False)
 
 
Item 1605: 
git clone https://github.com/dariocazzani/World-Models-TensorFlow
 cd stateless_agent/
 mkdir -p ../data
python generate-VAE-data.py
 python train_vae.py
 tensorboard --logdir logdir
 Embedding_size * 2 + bias = 66
 python train-agents.py
 
 
Item 1607: 
python main.py train vgg /sharedfiles/vgg_pretrained.h5
 tensorboard --logdir /tmp/tensorflow
 
 
Item 1613: 
gdal_translate -of GTiff -b 1 -b 10 -b 13 input_sentinel_13_band.tif output_RGB.tif
         +------------------------+-----------------------------+
        |   Bands                |   Result(Accuracy)          |     
        +------------------------+-----------------------------+
        | All Bands              |     0.83                    | 
        | RGB                    |     0.84                    | 
        | High Resolution Bands  |     0.81                    | 
        | Special Bands          |     0.94                    |  
        | RGB+Special Bands      |     0.80                    |
        +------------------------+-----------------------------+
 
 
Item 1620: 
--value-inputs string value_inputs.json for the model. Accepts local file or http(s) url.
 {"data_0": [1, [1, 3, 227, 227]]}
 {"flattened_rescaled_img_28x28": [1, [1, 784]]}
 docker run -it — rm \
 -v “$PWD/models:/models/” \
 -p 9000:9000 \
 sleepsonthefloor/graphpipe-onnx:cpu \
 — value-inputs=/models/dnn_model_pt.value_inputs.json \
 — model=../models/dnn_model_pt.onnx \
 — listen=0.0.0.0:9000
 terminate called after throwing an instance of ‘caffe2::EnforceNotMet’
 what(): [enforce fail at tensor.h:147] values.size() == size_. 784 vs 1229312
 docker run -it — rm \
 -v “$PWD/models:/models/” \
 -p 9000:9000 \
 sleepsonthefloor/graphpipe-tf:cpu \
 — model=/models/dnn_model_tf.pb \
 — listen=0.0.0.0:9000
 INFO[0000] Starting graphpipe-tf version 1.0.0.10.f235920 (built from sha f235920)
INFO[0000] Model hash is ‘e3ee2541642a8ef855d49ba387cee37d5678901f95e8aa0d3ed9a355cf464fb2’ 
INFO[0000] Using default inputs [flattened_rescaled_img_28x28:0] 
INFO[0000] Using default outputs [Softmax:0] 
INFO[0000] Listening on ‘0.0.0.0:9000
 Predicted Label / True Label: 2 == z ? — False !
Predicted Label / True Label: r == r ? — True !
Predicted Label / True Label: 3 == 3 ? — True !
Predicted Label / True Label: h == h ? — True !
Predicted Label / True Label: 2 == 2 ? — True !
Predicted Label / True Label: j == j ? — True !
Predicted Label / True Label: 5 == 5 ? — True !
Predicted Label / True Label: 2 == 2 ? — True !
Predicted Label / True Label: 7 == 7 ? — True !
Predicted Label / True Label: 8 == 8 ? — True !
 …
INFO[0113] Request for / took 773.621µs 
INFO[0113] Request for / took 859.584µs 
INFO[0113] Request for / took 810.67µs 
…
 
 
Item 1621: 
new_gradients = gradients * threshold / l2_norm(gradients)

 tf.clip_by_norm(t, clip_norm, axes=None, name=None)
 
 
Item 1622: 
##Sample values
dbinom(x, size, prob, log = FALSE)
 dbinom(3,4,(1/20))
= 0.000475
 
 
Item 1626: 
5
 writer = tf.summary.FileWriter([logdir], [graph])
 5
 5
 Done with writing the scalar summary
 Done writing the summaries
 Done writing the summaries
 tf.summary.image(name, tensor, max_outputs=3)
 Done writing the summaries
 
 
Item 1631: 
for i in range(iterations_count):          
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
     params -= learning_rate * param_gradients
 NOTE :-
     alpha = 0.9 is randomly chosen weight.        
     t(i)  is temperature at ith day.
     v(i)  is average temperature for ith day averaged over 1/(1 - alpha) days.
 v(0) = 0
v(1) = 0.9 * v(0) + 0.1 * t(1)
v(2) = 0.9 * v(1) + 0.1 * t(2)
...
v(i) = alpha * v(i-1) + (1 - alpha) * t(i)
 moment = 0
for i in range(iterations_count):          
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
    moment = gamma * moment + param_gradients
    params += learning_rate * moment
           (where moment is building moving average of gradients.
                 gamma gives kind of friction = 0.9 or 0.99).
 squared_gradients = 0
for i in range(iterations_count):          
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
    squared_gradients += param_gradients * param_gradients
    params -= learning_rate * param_gradients/
                         (np.sqrt(squared_gradients) + 1e-8)
                           {1e-8 is to avoid divide by zero}
 squared_gradients = 0
for i in range(iterations_count):          
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
    squared_gradients = decay_rate * squared_gradients + (1 -  
                  decay_rate) * param_gradients * param_gradients
    params -= learning_rate * param_gradients/
                              (np.sqrt(squared_gradients) + 1e-8)
 first_moment = 0
second_moment = 0
for step in range(iterations_count):
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
    first_moment = beta_1 * first_moment + (1 - beta_1) *   
                                                    param_gradients
    second_moment = beta_2 * second_moment + (1 - beta_2) * 
                                  param_gradients * param_gradients      
    params -= learning_rate * first_moment/(np.sqrt(second_moment) + 
                                                              1e-8)
 first_moment = 0
second_moment = 0
for step in range(iterations_count):
    param_gradients = evaluate_gradients(loss_function, 
                                         data, 
                                         params)
    first_moment = beta_1 * first_moment + (1 - beta_1) *   
                                                    param_gradients
    second_moment = beta_2 * second_moment + (1 - beta_2) * 
                                  param_gradients * param_gradients      
    first_bias_correction = first_moment/(1 - beta_1 ** step)
    second_bias_correction = second_moment/(1 - beta_2 ** step)
    params -= learning_rate * first_bias_correction/
                           (np.sqrt(second_bias_correction) + 1e-8)
 
 
Item 1632: 
names = ["Mike Gil", "Love Uche", "Femi Krane", "Sarah Jim", "Ahmed Crux", "Sarah Akor", "Openiyi Olu", "Kalu Isa", "Bekyy Buggy", "Itz Mhiz Gold"]
 ages = [34, 24, 18, 43, 51, 33, 22, 18, 29, 31]
 {'Mike Gil': 34, 'Love Uche': 24, 'Femi Krane': 18, 'Sarah Jim': 43, 'Ahmed Crux': 51, 'Sarah Akor': 33, 'Openiyi Olu': 22, 'Kalu Isa': 18, 'Bekyy Buggy': 29, 'Itz Mhiz Gold': 31}
 18
 {'Mike Gil': 34, 'Love Uche': 24, 'Femi Krane': 18, 'Sarah Jim': 43, 'Ahmed Crux': 51, 'Sarah Akor': 33, 'Openiyi Olu': 22, 'Kalu Isa': 18, 'Bekyy Buggy': 29, 'Itz Mhiz Gold': 31, 'Monwe Uda': 45}
 {'Mike Gil': 34, 'Love Uche': 24, 'Femi Krane': 18, 'Sarah Jim': 43, 'Ahmed Crux': 51, 'Sarah Akor': 33, 'Openiyi Olu': 22, 'Kalu Isa': 18, 'Bekyy Buggy': 29, 'Monwe Uda': 45}
 ['ABIA', 'ABUJA', 'ADAMAWA', 'AKWA IBOM', 'ANAMBRA', 'BAUCHI', 'BAYELSA', 'BENUE', 'BORNO', 'CROSS RIVER', 'DELTA', 'EBONYI', 'EDO', 'EKITI', 'ENUGU', 'GOMBE', 'IMO', 'JIGAWA', 'KADUNA', 'KATSINA', 'KEBBI', 'KOGI', 'KWARA', 'KANO', 'LAGOS', 'NASSARAWA', 'NIGER', 'OGUN', 'ONDO', 'OSUN', 'OYO', 'PLATEAU', 'RIVERS', 'SOKOTO', 'TARABA', 'YOBE', 'ZAMFARA']
 37
 ['Mike Gil', 'Love Uche', 'Femi Krane', 'Sarah Jim', 'Ahmed Crux', 'Sarah Akor', 'Kalu Isa']
 ['Mike Gil', 'Love Uche', 'Femi Krane', 'Sarah Jim', 'Ahmed Crux', 'Sarah Akor', 'Kalu Isa']
 #UdacityDataScholar #PoweredByBertelsmann
 
 
Item 1635: 
SET @problem_onset = '2017-08-27';
SET @problem_offset = '2017-08-29';
 SELECT
    userid,
    problemfield
FROM users
WHERE
    created BETWEEN @problem_onset AND @problem_offset
    AND problemfield > 134.2
;
 SELECT
   userid,
   problemfield     AS problemfield_existing,
   problemfield / 2 AS problemfield_proposed
FROM users
...
 BEGIN;
 -- problemfield = -3415
UPDATE users SET problemfield = 100 WHERE userid = 2343;
 -- problemfield = -1324
UPDATE users SET problemfield = 50 WHERE userid = 98234;
 ...
 COMMIT;
 def generate_patch():
    df = detect_problem_cases()
    df_fixed = propose_fixes(df)
    patch = generate_sql_patch(df_fixed)
     save_backup(df_fixed)
    save_patch(patch)
 
 
Item 1637: 
AUROC on train: 0.814208
AUROC on test: 0.826421
 AUROC on train: 0.849
AUROC on test: 0.846
 
 
Item 1644: 
4^n where n is the time stamp going in back wards direction
 
 
Item 1647: 
IN 
df.Helpful_Votes.describe()
 OUT
count    4.756338e+06
mean     5.625667e+00
std      2.663631e+01
min      0.000000e+00
25%      1.000000e+00
50%      2.000000e+00
75%      4.000000e+00
max      2.331100e+04
 IN
def normalize(column):
    upper = column.max()
    lower = column.min()
    y = (column - lower)/(upper-lower)
    return y
 helpful_normalized = normalize(df.Helpful_Votes)
helpful_normalized.describe()
 OUT
count    4.756338e+06
mean     2.413310e-04
std      1.142650e-03
min      0.000000e+00
25%      4.289820e-05
50%      8.579641e-05
75%      1.715928e-04
max      1.000000e+00
 IN
def sigmoid(x):
    e = np.exp(1)
    y = 1/(1+e**(-x))
    return y
 helpful_sigmoid = sigmoid(df.Helpful_Votes)
helpful_sigmoid.describe()
 OUT
count    4.756338e+06
mean     8.237590e-01
std      1.598215e-01
min      5.000000e-01
25%      7.310586e-01
50%      8.807971e-01
75%      9.820138e-01
max      1.000000e+00
 IN
helpful_log = np.log(df.Helpful_Votes)
helpful_log.describe()
 OUT
RuntimeWarning: divide by zero encountered in log
 IN
helpful_log = np.log(df.Helpful_Votes + 1)
helpful_log.describe()
 OUT
count    4.756338e+06
mean     1.230977e+00
std      9.189495e-01
min      0.000000e+00
25%      6.931472e-01
50%      1.098612e+00
75%      1.609438e+00
max      1.005672e+01
Name: Helpful_Votes, dtype: float64
 IN
helpful_log_normalized = normalize(helpful_log)
helpful_log_normalized.describe()
 OUT
count    4.756338e+06
mean     1.224034e-01
std      9.137663e-02
min      0.000000e+00
25%      6.892376e-02
50%      1.092416e-01
75%      1.600360e-01
max      1.000000e+00
Name: Helpful_Votes, dtype: float64
 IN
helpful_cube_root = df.Helpful_Votes**(1/3)
helpful_cube_root.describe()
 OUT
count    4.756338e+06
mean     1.321149e+00
std      8.024150e-01
min      0.000000e+00
25%      1.000000e+00
50%      1.259921e+00
75%      1.587401e+00
max      2.856628e+01
 IN
helpful_cube_root_normalized = normalize(helpful_cube_root)
helpful_cube_root_normalized.describe()
 OUT
count    4.756338e+06
mean     4.624857e-02
std      2.808959e-02
min      0.000000e+00
25%      3.500631e-02
50%      4.410519e-02
75%      5.556906e-02
max      1.000000e+00
 IN
log_max = np.log(df.Helpful_Votes.max())
helpful_log_max_root = df.Helpful_Votes**(1/log_max)
helpful_log_max_root.describe()
 OUT
count    4.756338e+06
mean     9.824853e-01
std      3.712224e-01
min      0.000000e+00
25%      1.000000e+00
50%      1.071355e+00
75%      1.147801e+00
max      2.718282e+00
 IN
helpful_log_max_root_normalized = normalize(helpful_log_max_root)
helpful_log_max_root_normalized.describe()
 OUT
count    4.756338e+06
mean     3.614362e-01
std      1.365651e-01
min      0.000000e+00
25%      3.678794e-01
50%      3.941294e-01
75%      4.222525e-01
max      1.000000e+00
 IN
helpful_hyperbolic_tangent = np.tanh(df.Helpful_Votes)
helpful_hyperbolic_tangent.describe()
 OUT
count    4.756338e+06
mean     7.953343e-01
std      3.033794e-01
min      0.000000e+00
25%      7.615942e-01
50%      9.640276e-01
75%      9.993293e-01
max      1.000000e+00
 IN
size = len(df.Helpful_Votes)-1
helpful_percentile_linearization = df.Helpful_Votes.rank(method=’min’).apply(lambda x: (x-1)/size)
helpful_percentile_linearization.describe()
 OUT
count    4.756338e+06
mean     4.111921e-01
std      3.351097e-01
min      0.000000e+00
25%      1.133505e-01
50%      4.719447e-01
75%      7.059382e-01
max      1.000000e+00
 
 
Item 1660: 
value sub attribute datatype string;
 property sub entity is-abstract
    has value
     plays owned
    plays demand
    plays authorizer
    plays exported
    plays imported
    plays revoker
    plays withdrawer;
 last-name sub property;
first-name sub property;
email sub property;
 match
    $a isa property;
    $p isa person, has identifier 123456;
    (exporter:$x, exported:$a, exported-to:$z);
    (owned:$a, owner:$p);
    get;
 address sub property
    has value;
city sub address;
zip sub address;
street1 sub address;
street2 sub address;
street3 sub address;
 person sub entity
    has timestamp
    has type
    has identifier
     plays identified
    plays imported
    plays importer
    plays exported
    plays exporter
    plays owner;
 match 
    $p isa person;
    $e isa email, has value “member@test.com”;
    ($e, $p) isa belongs;
    get $p;
 match $p isa person has identifier 1; get;
 anonymous sub entity
    has timestamp
    plays incognito;
 system sub entity
    has value
    has icon
     plays importer
    plays exporter
    plays requester
    plays authorized
    plays exported-to
    plays imported-to;
 match 
   $a isa authorization;
   $s isa system;
   (requisite:$a, requester:$system);
   get;
 match 
    $p isa person has identifier 1;
    $i isa anonymous;
    $e isa email;
    $a isa authorization;
    $s isa system;
    ($i, $p) isa identifies;
    ($p, $e) isa belongs;
    ($e, $a) isa needs;
    ($a, $s) isa requires;
    get;
 authorization sub entity
   has name
   has description
   has timestamp
   has expiration-date
    plays needed
   plays requisite
   plays revoked
   plays withdrawn;
 match 
   $p isa person has identifier 1;
   $i isa anonymous;
   $e isa email;
   $s isa system;
   ($i, $p) isa identifies;
   ($p, $e) isa belongs;
   ($e, $s) isa authorizes;
   get;
 >>> git clone https://github.com/idealley/grakn-gdpr <a folder>
 >>> ./load.sh grakn grakn
 
 
Item 1663: 
<SOR>I had the steak, mussels with a side of chicken parmesan. All were very good. We will be back.<EOR>
<SOR>The food, service, atmosphere, and service are excellent. I would recommend it to all my friends<EOR>
<SOR>Good atmosphere, amazing food and great service.Service is also pretty good. Give them a try!<EOR>
 python json_converter.py ./dataset/review.json
python json_converter.py ./dataset/business.json
 "<SOR>Hummus is amazing and fresh! Loved the falafels. I will definitely be back. Great owner, friendly staff<EOR>"
 72662807 / 90000 * 219 /60 / 60/ 24 = 2.0 days
 
 
Item 1665: 
{'start': '75.936', 'dur': '1.302'} 
ELON MUSK: Thank you. Thank you very
 {'start': '77.238', 'dur': '1.887'} 
much for having me. I look forward to
 {'start': '79.125', 'dur': '3.547'} 
talking about the SpaceX Mars
 {'start': '82.672', 'dur': '2.25'} 
architecture. And what I really want to
 {'start': '84.922', 'dur': '3.019'} 
achieve here is to make Mars seem
 {'start': '87.941', 'dur': '2.949'} 
possible, make it seem as though it&#39;s
 {'start': '90.89', 'dur': '2.32'} 
something that we can do in our
 {'start': '93.21', 'dur': '3.11'} 
lifetimes and that you can go. And is
 {'start': '96.32', 'dur': '1.76'} 
there really a way that anyone can go if
 {'start': '98.08', 'dur': '1.55'} 
they wanted to?
 [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn', u"'s", u"n't", u"'m", u"'d"]
 LdaModel(num_terms=1016, num_topics=5, decay=0.5, chunksize=100)
 [(0, u'0.041*"engine" + 0.026*"really" + 0.025*"make" + 0.022*"tank" + 0.016*"rocket" + 0.014*"vehicle" + 0.013*"also" + 0.013*"merlin" + 0.013*"capable" + 0.012*"because"'), 
 (1, u'0.031*"mars" + 0.028*"use" + 0.025*"mission" + 0.024*"carbon" + 0.022*"fiber" + 0.022*"liquid" + 0.018*"thing" + 0.017*"falcon" + 0.016*"day" + 0.015*"very"'), 
 (2, u'0.045*"system" + 0.035*"would" + 0.032*"propel" + 0.028*"go" + 0.026*"time" + 0.024*"solar" + 0.022*"mars" + 0.018*"orbit" + 0.018*"cost" + 0.016*"greater"'), 
 (3, u'0.034*"first" + 0.027*"station" + 0.026*"applause" + 0.024*"dragon" + 0.020*"space" + 0.016*"think" + 0.016*"ton" + 0.015*"mars" + 0.014*"launch" + 0.013*"go"'), 
 (4, u'0.028*"booster" + 0.028*"spaceship" + 0.025*"get" + 0.023*"land" + 0.022*"like" + 0.021*"really" + 0.021*"maybe" + 0.019*"go" + 0.018*"anywhere" + 0.018*"actual"')]
 
 
Item 1672: 
!git clone https://gist.github.com/f7b7c7758a46da49f84bc68b47997d69.git
 !bash pytorch041_cuda92_colab.sh
 
 
Item 1674: 
works by splitting the sample based on the field that provides the maximum information gain. Each subsample defined by the first split is then split again, usually based on a different field, and the process repeats until the subsamples cannot be split any further. Finally, the lowest-level splits are reexamined, and those that do not contribute significantly to the value of the model are removed or pruned.
 
 
Item 1681: 
schema HouseheatState
    Float32 heat_cost,
    Float32 temperature_difference,
    Float32 temperature_difference_t1,
    Float32 temperature_difference_t2,
    Float32 temperature_difference_t3,
    Float32 temperature_difference_t4,
    Float32 temperature_difference_t5,
    Float32 outside_temp_change
end

schema HouseheatAction
    Float32{ 0.0:1:1.0 } heater_on
end

schema HouseheatConfig
    Float32 outside_phase
end

concept thermostat is classifier
   predicts (HouseheatAction)
   follows input(HouseheatState)
   feeds output
end

simulator simulink_sim(HouseheatConfig)
    action (HouseheatAction)
    state (HouseheatState)
end

curriculum my_curriculum
    train thermostat
    with simulator simulink_sim
    objective match_set_temp

        lesson my_first_lesson
            configure
            constrain outside_phase with Float32{0.0:12.0}
            until
                maximize match_set_temp

        lesson my_second_lesson
            configure
            constrain outside_phase with Float32{0.0:24.0}
            until
                maximize match_set_temp


        lesson my_third_lesson
            configure
            constrain outside_phase with Float32{0.0:48.0}
            until
                maximize match_set_temp

end
 bonsai create myMountainCarBrain
 
 
Item 1685: 
""" 
    Author   : Ajinkya Sonawane
"""
 class Node:
    def __init__(self,data,level,fval):
        """ Initialize the node with the data, level of the node and the calculated fvalue """
        self.data = data
        self.level = level
        self.fval = fval
 def generate_child(self):
        """ Generate child nodes from the given node by moving the blank space
            either in the four directions {up,down,left,right} """
        x,y = self.find(self.data,'_')
        """ val_list contains position values for moving the blank space in either of
            the 4 directions [up,down,left,right] respectively. """
        val_list = [[x,y-1],[x,y+1],[x-1,y],[x+1,y]]
        children = []
        for i in val_list:
            child = self.shuffle(self.data,x,y,i[0],i[1])
            if child is not None:
                child_node = Node(child,self.level+1,0)
                children.append(child_node)
        return children
        
    def shuffle(self,puz,x1,y1,x2,y2):
        """ Move the blank space in the given direction and if the position value are out
            of limits the return None """
        if x2 >= 0 and x2 < len(self.data) and y2 >= 0 and y2 < len(self.data):
            temp_puz = []
            temp_puz = self.copy(puz)
            temp = temp_puz[x2][y2]
            temp_puz[x2][y2] = temp_puz[x1][y1]
            temp_puz[x1][y1] = temp
            return temp_puz
        else:
            return None
 def copy(self,root):
        """ Copy function to create a similar matrix of the given node"""
        temp = []
        for i in root:
            t = []
            for j in i:
                t.append(j)
            temp.append(t)
        return temp    
            
    def find(self,puz,x):
        """ Specifically used to find the position of the blank space """
        for i in range(0,len(self.data)):
            for j in range(0,len(self.data)):
                if puz[i][j] == x:
                    return i,j
 class Puzzle:
    def __init__(self,size):
        """ Initialize the puzzle size by the specified size,open and closed lists to empty """
        self.n = size
        self.open = []
        self.closed = []
 def accept(self):
        """ Accepts the puzzle from the user """
        puz = []
        for i in range(0,self.n):
            temp = input().split(" ")
            puz.append(temp)
        return puz
 def f(self,start,goal):
        """ Heuristic Function to calculate hueristic value f(x) = h(x) + g(x) """
        return self.h(start.data,goal)+start.level
 def h(self,start,goal):
        """ Calculates the different between the given puzzles """
        temp = 0
        for i in range(0,self.n):
            for j in range(0,self.n):
                if start[i][j] != goal[i][j] and start[i][j] != '_':
                    temp += 1
        return temp
 def process(self):
        """ Accept Start and Goal Puzzle state"""
        print("Enter the start state matrix \n")
        start = self.accept()
        print("Enter the goal state matrix \n")        
        goal = self.accept()
 start = Node(start,0,0)
        start.fval = self.f(start,goal)
        """ Put the start node in the open list"""
        self.open.append(start)
        print("\n\n")
        while True:
            cur = self.open[0]
            print("")
            print("  | ")
            print("  | ")
            print(" \\\'/ \n")
            for i in cur.data:
                for j in i:
                    print(j,end=" ")
                print("")
            """ If the difference between current and goal node is 0 we have reached the goal node"""
            if(self.h(cur.data,goal) == 0):
                break
            for i in cur.generate_child():
                i.fval = self.f(i,goal)
                self.open.append(i)
            self.closed.append(cur)
            del self.open[0]
 """ sort the opne list based on f value """
            self.open.sort(key = lambda x:x.fval,reverse=False)
 puz = Puzzle(3)
puz.process()
 
 
Item 1686: 
xcrun coremlc generated <path to .mlmodel> <output directory> — language <Swift or Objective-C>
 
 
Item 1687: 
{'location': 17, 'piece': 36, 'offset': 67.5, 'speed': 390, 'self_speed': 400, 'clockwise': 7, 'notification_time': datetime.datetime(2018, 8, 25, 21, 9, 33, 359248), 'is_clockwise': False}
 {'location': 23, 'piece': 57, 'offset': 67.5, 'speed': 422, 'self_speed': 400, 'clockwise': 71, 'notification_time': datetime.datetime(2018, 8, 25, 21, 9, 32, 229689), 'is_clockwise': True}
 ...
 time lap in seconds was 9.046201
...
time lap in seconds was 9.029405
...
time lap in seconds was 9.045055
...
time lap in seconds was 9.044495
...
 time lap in seconds was 4.497926
...
time lap in seconds was 4.502276
...
time lap in seconds was 4.497534
...
 node_app
    node_socket-app
       node_server.js
    package-lock.json
python_app
    py_overdrive_sdk
        py_overdrive.py
    track_images/
    create_track_image.py
    constant_speed_example.py
    track_discovery_example.py
    custom_policy_example.py
    requirements.txt
.gitignore
 
 
Item 1688: 
gcloud dataproc clusters create your_cluster_name --zone southamerica-east1-b --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --project your_project_name
 
 
Item 1689: 
cd data
wget http://files.fast.ai/data/cifar10.tgz
tar -xf cifar10.tgz
 
 
Item 1691: 
error = tf.subtract(self.y, output)
loss = tf.reduce_mean(tf.maximum(q*error, (q-1)*error), axis=-1)
 class QuantileLoss(nn.Module):
    def __init__(self, quantiles):
        super().__init__()
        self.quantiles = quantiles
        
    def forward(self, preds, target):
        assert not target.requires_grad
        assert preds.size(0) == target.size(0)
        losses = []
        for i, q in enumerate(self.quantiles):
            errors = target - preds[:, i]
            losses.append(
                torch.max(
                   (q-1) * errors, 
                   q * errors
            ).unsqueeze(1))
        loss = torch.mean(
            torch.sum(torch.cat(losses, dim=1), dim=1))
        return loss
 clfl = lgb.LGBMRegressor(
    objective = 'quantile',
    alpha = 1 - ALPHA,
    num_leaves = NUM_LEAVES, 
    learning_rate = LEARNING_RATE,
    n_estimators = N_ESTIMATORS,
    min_data_in_leaf=5, 
    reg_sqrt = REG_SQRT,
    max_depth = MAX_DEPTH)
 
 
Item 1694: 
aug_tfms = [RandomRotateZoom(deg=45, zoom=2, stretch=1)] +      [AddPadding(pad=50, mode=cv2.BORDER_CONSTANT)]
 [AddPadding(pad=50, mode=cv2.BORDER_CONSTANT)]
 aug_tfms = [Random Rotate(57)]
 aug_tfms = [Dihedral()]
 aug_tfms = [RandomLighting(b=0.25,c=0.15)]
 aug_tfms = [RandomLighting(b=0.25,c=0.15)] + [RandomDihedral()]
 aug_tfms = [RandomDihedral()] + [RandomRotate(27)]
 aug_tfms = [RandomRotateZoom(deg=45, zoom=2, stretch=1)]
 aug_tfms = [AddPadding(pad=50, mode=cv2.BORDER_CONSTANT)]
 aug_tfms = [Cutout(n_holes=200, length=7.5, tfm_y=TfmType.NO)]
 
 
Item 1695: 
python download_images.py
 
 
Item 1697: 
language: "en" 
 pipeline: 
  - name: "nlp_spacy"              
  - name: "tokenizer_spacy"        
  - name: "ner_crf"            
  - name: "intent_featurizer_spacy"
  - name: "intent_classifier_sklearn"
  - name: "ner_duckling"          
      dimensions: ['number']
 {  
   "intent":{  
      "name":"Default Welcome Intent",
      "confidence":0.8342492802420313
   },
   "entities":[  

   ],
   "intent_ranking":[  
      {  
         "name":"Default Welcome Intent",
         "confidence":0.8342492802420313
      },
      {  
         "name":"thanks",
         "confidence":0.09805256693160122
      },
      {  
         "name":"goodbye",
         "confidence":0.05392708191432759
      },
      {  
         "name":"address",
         "confidence":0.003986386948676723
      },
      {  
         "name":"place_search",
         "confidence":0.0037102872949153686
      },
      {  
         "name":"rating",
         "confidence":0.003059348479049656
      },
      {  
         "name":"opening_hours",
         "confidence":0.0030150481893980153
      }
   ],
   "text":"Hello",
   "project":"current",
   "model":"model_20180827-110057"
}
 ## story_01 
  * Default Welcome Intent 
    utter_greet 
  * place_search{“query”:”restaurant”, “radius”:”600”}
    action_place_search 
    slots{“place_match”:”one”} 
    slots{“address”:”Ars Vini in Sredzkistraße 27, Hagenauer Straße  
          1, Berlin”} 
    slots{“rating”:”4.4”} 
    slots{“opening_hours”:”true”} 
  * opening_hours 
    utter_opening_hours 
  * rating 
    utter_rating
 templates:
  utter_greet: 
    — “Hello! How can I help?” 
  utter_goodbye: 
    — “Talk to you later!” 
  utter_thanks: 
    — “You are very welcome.” 
  utter_what_radius: 
    — “Within what radius?” 
  utter_rating: 
    — “The rating is {rating}.” 
  utter_address: 
    — “The address is {address}.” 
  utter_opening_hours: 
    — “The place is currently {opening_hours}.” 
  utter_no_results: 
    — “Sorry, I couldn’t find anything.”
 python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue --epochs 200
 python -m rasa_core_sdk.endpoint --actions actions
 python -m rasa_core.run -d models/current/dialogue -u models/current/nlu_model --endpoints endpoints.yml
 
 
Item 1698: 
library(viridis)     ## color palette
library(ggjoy)       ## ridges
library(hrbrthemes)  ## plot theme
 pgh_weather <- read.csv("weather_data.csv") ## setwd() to your own
months <- c("December","November","October","September","August","July","June","May","April","March","February","January") ## need this string vector for sorting later
 pgh_weather$months <- as.Date(pgh_weather$CST, format = "%d-%b-%y") %>%
  months() %>%
  as.factor() %>%
  factor(., levels = months)
 #scales
mins <- min(pgh_weather$Min.TemperatureF)
maxs <- max(pgh_weather$Max.TemperatureF)
 ## black and white
ggplot(pgh_weather,aes(x = Mean.TemperatureF,y=months,height=..density..))+
  geom_joy(scale=3) +
  scale_x_continuous(limits = c(mins,maxs))+
  theme_ipsum(grid=F)+
  theme(axis.title.y=element_blank(),
        axis.ticks.y=element_blank(),
        strip.text.y = element_text(angle = 180, hjust = 1))+
  labs(title='Temperatures in Pittsburgh',
       subtitle='Median temperatures (Fahrenheit) by month for 2016\nData: Original CSV from the Weather Underground', x = "Mean Tempterature [ºF]")
 ## in color
ggplot(pgh_weather, aes(x = `Mean.TemperatureF`, y = `months`, fill = ..x..)) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01, gradient_lwd = 1.) +
  scale_x_continuous(expand = c(0.01, 0)) +
  scale_y_discrete(expand = c(0.01, 0)) +
  scale_fill_viridis(name = "Temp. [ºF]", option = "C") +
  labs(title = 'Temperatures in Pittsburgh',
       subtitle = 'Mean temperatures (Fahrenheit) by month for 2016\nData: Original CSV from the Weather Underground', 
       x = "Mean Temperature") +
  theme_ridges(font_size = 13, grid = TRUE) + theme(axis.title.y = element_blank())
 
 
Item 1701: 
IamOptions options = new IamOptions.Builder()
        .apiKey(API_KEY)
        .build();

VisualRecognition visualRecognition = new VisualRecognition("2018-03-19", options);
ClassifyOptions classifyOptions = new ClassifyOptions.Builder()
        .url(etUrl.getText().toString())
        .classifierIds(Collections.singletonList("explicit"))
        .threshold(threshold)
        .owners(Collections.singletonList("me"))
        .build();
ClassifiedImages classifiedImages = visualRecognition.classify(classifyOptions).execute();
 
 
Item 1704: 
val input = sc.parallelize(List(1, 2, 3, 4))
 val lines = sc.textFile(“README.md”)
 sc.setLogLevel(“ERROR”)
 num = sc.parallelize([4, 6, 6, 1, 3, 0, 2, 2, 2])
 result = num.map(lambda x: x**2)
 result.take(10)
 
[16, 36, 36, 1, 9, 0, 4, 4, 4]
 result = num.filter(lambda x: x >= 3)
 result.take(10)
 [4, 6, 6, 3]
 result = num.distinct()
 result.take(10)
 [0, 1, 2, 3, 4, 6]
 num2 = sc.parallelize([5, 5, 8, 2, 2, 1, 7, 3, 3])
 result = num.union(num2)
 result.take(20)
 [4, 6, 6, 1, 3, 0, 2, 2, 2, 5, 5, 8, 2, 2, 1, 7, 3, 3]
 result = num.intersection(num2)
 result.take(20)

 [2, 1, 3]
 result = num.subtract(num2)
 result.take(20)

 [0, 4, 6, 6]
 result = num.cartesian(num2)
 result.take(20)
 [(4, 5), (4, 5), (4, 8), (4, 2), (4, 2), (4, 1), (4, 7), (4, 3), (4, 3), (6, 5), (6,5), (6, 8), (6, 2), (6, 2), (6, 1), (6, 7), (6, 3), (6, 3), (6, 5), (6, 5)]
 num.count()
 9
 num.countByValue()
 defaultdict(<type 'int'>, {0: 1, 1: 1, 2: 3, 3: 1, 4: 1, 6: 2})
 num.collect()
 [4, 6, 6, 1, 3, 0, 2, 2, 2]
 num.top(3)
 [6, 6, 4]
 num.takeOrdered(5)

 [0, 1, 2, 2, 2]
 num.reduce(lambda x, y: x + y)
 [26]
 num.fold(0, lambda x,y : x + y)
 [26]
 num = sc.parallelize([4, 6, 6, 1, 3, 1, 2, 2, 2])
 sumCount = num.aggregate((1, 0),
    (lambda tup, value: (value * tup[0], tup[1] + 1),
    (lambda tup, value_tup: (value_tup[0] * tup[0], value_tup[1] + tup[1])))
 sumCount
 (3456, 9)
 
 
Item 1705: 
#from CRAN
install.packages(“twitteR”)
 #alternatively from the Github
library(devtools)
install_github(“geoffjentry/twitteR”)
 #load library
library(twitteR)
 #load credentials
consumer_key <- “****************”
consumer_secret<- “*******************”
access_token <- “*******************”
access_secret <- “************************”

 
#set up to authenticate
setup_twitter_oauth(consumer_key ,consumer_secret,access_token ,access_secret)
 #fetch tweets associated with that hashtag , 12 tweets-n in 
#(en)glish-lang since the indicated date yy/mm/dd
 tweets <- twitteR::searchTwitter(“#rstats”,n =12,lang =”en”,since = ‘2018–01–01’)
 #strip retweets
strip_retweets(tweets)
 #convert to data frame using the twListtoDF function
df <- twListToDF(tweets)\#extract the data frame save it locally
saveRDS(df, file=”tweets.rds”)
df1 <- readRDS(“mytweets.rds”)
 
library(dplyr)
 #clean up any duplicate tweets from the data frame using #dplyr::distinct
 dplyr::distinct(df1)
 winner <-df1 %>% select(text,retweetCount,screenName,id )%>% filter(retweetCount == max(retweetCount))
View(winner)
 us <- userFactory$new(screenName= winner$screenName)
dmSend(“Thank you for participating in #rstats,Your tweet had the highest retweets” , us$screenName)
 
 
Item 1708: 
Sport             min  max  average 
Tug-Of-War       75.0 118.0  95.61 
Basketball       59.0 156.0  91.68 
Rugby Sevens     65.0 113.0  91.00 
Bobsleigh        55.0 145.0  90.38 
Beach Volleyball 62.0 110.0  89.51
 sns.distplot(sport_weight_height_metrics.Weight.dropna()['mean'])
 lightest: 
Gymnastics:      63.3436047592 
Ski Jumping:     65.2458805355 
Boxing:          65.2962797951 
Trampolining:    65.8378378378 
Nordic Combined: 66.9095595127
 shortest (cm): 
Gymnastics:    167.644438396 
Weightlifting: 169.153061224 
Trampolining:  171.368421053 
Diving:        171.555352242 
Wrestling:     172.870686236 
 tallest (cm): 
Rowing:           186.882697947 
Handball:         188.778373113 
Volleyball:       193.265659955 
Beach Volleyball: 193.290909091 
Basketball:       194.872623574
 Smallest Build (Kg/centimeters) 
Alpine Skiing    0.441989 
Archery          0.431801 
Art Competitions 0.430488 
Athletics        0.410746 
Badminton        0.413997 
Heaviest Build 
Tug-Of-War     0.523977 
Rugby Sevens   0.497754 
Bobsleigh      0.496656 
Weightlifting  0.474433 
Handball       0.473507
 Sport           introduced
Biathlon           1960
Luge               1964
Volleyball         1964
Judo               1964
Table Tennis       1988
Baseball           1992
Short Track Speed Skating 1992
Badminton           1992
Freestyle Skiing    1992
Beach Volleyball    1996
Snowboarding        1998
Taekwondo           2000
Trampolining        2000
Triathlon           2000
Rugby Sevens        2016
 Basque Pelota    1900
Croquet          1900
Cricket          1900
Roque            1904
Jeu De Paume     1908
Racquets         1908
Motorboating     1908
Lacrosse         1908
Tug-Of-War       1920
Rugby            1924
Military Ski Patrol 1924
Polo             1936
Aeronautics      1936
Alpinism         1936
Art Competitions 1948
 
 
Item 1709: 
PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain’d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.
 Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.
 Naturalism and decision for the majority of Arab countries’ capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated
with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal
in the [[Protestant Immineners]], which could be said to be directly in Cantonese
Communication, which followed a ceremony and set inspired prison, training. The
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known
in western [[Scotland]], near Italy to the conquest of India with the conflict.
  <revision>
 <id>15900676</id>
 <timestamp>2002–08–03T18:14:12Z</timestamp>
 <contributor>
 <username>Paris</username>
 <id>23</id>
 </contributor>
 <minor />
 <comment>Automated conversion</comment>
 </revision>
 SEED: War on terror
Good everybody. Thank you very much. God bless the United States of America, and has already began with the world’s gathering their health insurance.
It’s about hard-earned for our efforts that are not continued.
We are all the assumptionion to the streets of the Americas that we are still for everybody and destruction.
We are doing a lot of this.
I know that someone would be prefered to their children to take a million insurance company. We’re watching their people and continued to find ourselves with Republicans — to give up on these challenges and despite the challenges of our country. In the last two years, we must recognise that our borders have access from the world. We’re continuing that this day of things that the United States will clean up it´s allies and prosperity to stand up enough to be a sanctions that we made their faith, and for the country of the Internet to Osama bin Laden.
Thank you. God bless you. Good morning, everybody. And May God loss man. Thank you very much. Thank you very much, everybody.
 
 
Item 1712: 
(10,000–7,500) + (25,000–20,000) + (75,000–60,000) = 2,500 + 5,000 + 15,000 = 22,500
 (10,000–7,500)² + (25,000–20,000)² + (75,000–60,000)² = 256,250,000
 
 
Item 1713: 
conda –V
 conda update conda
 conda env list
 conda create --name myenv python=3.5
 activate myenv
 source activate myenv
 conda install numpy
 deactivate
 source deactivate
 conda remove --name myenv numpy
 conda env remove --name myenv
 
 
Item 1718: 
wget http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_11_06_2017.tar.gz
tar xvzf ssd_mobilenet_v1_coco_11_06_2017.tar.gz
 python3 models/research/object_detection/legacy/train.py --logtostderr --train_dir=data/ --pipeline_config_path=data/ssd_mobilenet_v1_pets.config
 rm -rf object_detection_graph
python3 models/research/object_detection/export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path data/ssd_mobilenet_v1_pets.config \
    --trained_checkpoint_prefix data/model.ckpt-11454 \
    --output_directory object_detection_graph
 
 
Item 1725: 
install.packages("installr")
library(installr)
updateR()
 install.packages('devtools') #assuming it is not already installed
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR(admin_password = 'Admin user password')
 /Library/Frameworks/R.framework/Versions/x.xx/Resources/library
 update.packages(checkBuilt=TRUE)
 version
packageStatus()
 
 
Item 1730: 
{  
   "faces":[  
      {  
         "key":"Al Roker",
         "instances":[  
            {  
               "start":150,
               "end":150,
               "start_ms":5005,
               "end_ms":5005,
               "confidence":0.6983038746995329
            },
            {  
               "start":480,
               "end":660,
               "start_ms":16016,
               "end_ms":22022,
               "confidence":0.6699914024543004
            },
            {  
               "start":780,
               "end":990,
               "start_ms":26026,
               "end_ms":33033,
               "confidence":0.7077699155373681
            }
         ]
      },
      {  
         "key":"Unknown Face 5abc120a9b25d163",
         "instances":[  
            {  
               "start":60,
               "end":60,
               "start_ms":2002,
               "end_ms":2002
            }
         ]
      }
   }
 
 
Item 1735: 
AHCA, American Health Care Act, ACA, Affordable Care Act, Obamacare, Trumpcare, Health care, Healthcare, Medicaid, Medicare, BCRA, Better Care Reconciliation Act, preexisting condition, Planned Parenthood, health insurance
 
 
Item 1737: 
mkdir transcripts && cd transcripts
youtube-dl --all-subs --skip-download 'https://www.youtube.com/channel/UCvze3hU6OZBkB1vkhH2lH9Q'
 
 
Item 1741: 
sanity documents query '*[_type == "post"]{title, "slug": slug.current, publishedAt}'|jq -r '(map(keys) | add | unique) as $cols | map(. as $row | $cols | map($row[.])) as $rows | $cols, $rows[] | @csv'
 
 
Item 1748: 
db = spark.read.text(“s3://bucket/lichess_db_standard_rated_2016–05.pgn”)
db.count()
 
 
Item 1751: 
ROI= (Profit/Cost)*100
 
 
Item 1761: 
Abstract Art
Abstract Expressionism Painting
Acrylics
Ancient Pottery
Animation
Architecture
Art Brut
Art Deco
Art Installation
Art Nouveau
...
 sls deploy
 Art Gallery
Art Museum
People in Museum
Selfie in Museum
Art Installation
Selfie With Art
...
 Murals
Graffiti Art
Performance Art
Street Performers
Textile Art
Religious Art
...
 
 
Item 1762: 
fmt.Println(records)
 …[7.76 24.54 47.92 181 0.05263 0.04362 0 0 0.1587 0.05884 0.3857 1.428 2.548 19.15 0.007189 0.00466 0 0 0.02676 0.002783 9.456 30.37 59.16 268.6 0.08996 0.06444 0 0 0.2871 B]]
  fmt.Println(“Total de dados: “, len(records))
 fmt.Println(“Total de treinamento: “, len(train))
 fmt.Println(“Total de testes: “, len(test))
 fmt.Println(“Total de acertos: “, hits)
 fmt.Println(“Porcentagem de acertos: “, (100 * hits / len(test)),   “%”)
 Total de dados: 569
Total de treinamento: 513
Total de testes: 513
Total de acertos: 484
Porcentagem de acertos: 94 %
 
//fmt.Println(“tumor: “, test[i][columnIndex], “ classificado como: 
 tumor: M classificado como: B
tumor: M classificado como: M
tumor: M classificado como: M
tumor: M classificado como: M
tumor: M classificado como: M
tumor: M classificado como: M
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
tumor: B classificado como: B
Total de dados: 569
Total de treinamento: 513
Total de testes: 513
Total de acertos: 479
Porcentagem de acertos: 93 %
 
 
Item 1764: 
I saw the best minds of my generation contented by cash,
     thriving for corporate initiative,
dragging themselves through the newsfeed all day looking 
     for that next notification fix...
 
 
Item 1766: 
class PNet(Network):
 def _config(self):
 layer_factory = LayerFactory(self)
 layer_factory.new_feed(name='data', layer_shape=(None, None, None, 3))
 layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=10, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu1')
 layer_factory.new_max_pool(name='pool1', kernel_size=(2, 2), stride_size=(2, 2))
 layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=16, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu2')
 layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu3')
 layer_factory.new_conv(name='conv4-1', kernel_size=(1, 1), channels_output=2, stride_size=(1, 1), relu=False)
 layer_factory.new_softmax(name='prob1', axis=3)
 layer_factory.new_conv(name='conv4-2', kernel_size=(1, 1), channels_output=4, stride_size=(1, 1),
 input_layer_name='prelu3', relu=False)
 class RNet(Network):
 def _config(self):
 layer_factory = LayerFactory(self)
 layer_factory.new_feed(name='data', layer_shape=(None, 24, 24, 3))
 layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=28, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu1')
 layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
 layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=48, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu2')
 layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
 layer_factory.new_conv(name='conv3', kernel_size=(2, 2), channels_output=64, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu3')
 layer_factory.new_fully_connected(name='fc1', output_count=128, relu=False)  
 layer_factory.new_prelu(name='prelu4')
 layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)  
 layer_factory.new_softmax(name='prob1', axis=1)
 layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu4')
 class ONet(Network):
 def _config(self):
 layer_factory = LayerFactory(self)
 layer_factory.new_feed(name='data', layer_shape=(None, 48, 48, 3))
 layer_factory.new_conv(name='conv1', kernel_size=(3, 3), channels_output=32, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu1')
 layer_factory.new_max_pool(name='pool1', kernel_size=(3, 3), stride_size=(2, 2))
 layer_factory.new_conv(name='conv2', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu2')
 layer_factory.new_max_pool(name='pool2', kernel_size=(3, 3), stride_size=(2, 2), padding='VALID')
 layer_factory.new_conv(name='conv3', kernel_size=(3, 3), channels_output=64, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu3')
 layer_factory.new_max_pool(name='pool3', kernel_size=(2, 2), stride_size=(2, 2))
 layer_factory.new_conv(name='conv4', kernel_size=(2, 2), channels_output=128, stride_size=(1, 1),
 padding='VALID', relu=False)
 layer_factory.new_prelu(name='prelu4')
 layer_factory.new_fully_connected(name='fc1', output_count=256, relu=False)
 layer_factory.new_prelu(name='prelu5')
 layer_factory.new_fully_connected(name='fc2-1', output_count=2, relu=False)
 layer_factory.new_softmax(name='prob1', axis=1)
 layer_factory.new_fully_connected(name='fc2-2', output_count=4, relu=False, input_layer_name='prelu5')
 layer_factory.new_fully_connected(name='fc2-3', output_count=10, relu=False, input_layer_name='prelu5')
 
 
Item 1771: 
no
don't do it
definitely not
not really
thanks but no
not interested
I don't think so
I disagree
I don't want that
nok
nope
no thanks
 yes
yes please
sure
do it
exactly
confirm
of course
sounds good
that's correct
I don't mind
I agree
ok
 yes, no
start, stop, previous, next, resume
cancel
later
repeat
start over
more
help
 
 
Item 1774: 
{'Name': 'Castle', 'Confidence': 99.12834167480469}
{'Name': 'Housing', 'Confidence': 93.01422882080078}
{'Name': 'Monastery', 'Confidence': 93.01422882080078}
[(1151, 'castles shall be subject to my hand."', 254), (1151, 'known to you whose are these castles and this lordly land?"', 404), (38468, "Caparison'd all richly with silken housings rare", 1988), (30659, 'To where the monastery stood.', 1916), (31314, 'Stands his little monastery,', 6758)]
 
 
Item 1777: 
Images in train-folder:
00s: 788
50s: 768
60s: 754
70s: 771
80s: 760
90s: 779
Images in test-folder:
00s: 198
50s: 194
60s: 187
70s: 192
80s: 199
90s: 197
 Epoch 1/7
145/144 [==============================] - 134s - loss: 2.6112 - acc: 0.2073 - val_loss: 2.3751 - val_acc: 0.2202
Epoch 2/7
145/144 [==============================] - 127s - loss: 1.6243 - acc: 0.3422 - val_loss: 1.6827 - val_acc: 0.3393
Epoch 3/7
145/144 [==============================] - 126s - loss: 1.2459 - acc: 0.5397 - val_loss: 1.8159 - val_acc: 0.3102
Epoch 4/7
145/144 [==============================] - 125s - loss: 0.8281 - acc: 0.7190 - val_loss: 1.7473 - val_acc: 0.4087
Epoch 5/7
145/144 [==============================] - 125s - loss: 0.4775 - acc: 0.8527 - val_loss: 2.0787 - val_acc: 0.4233
Epoch 6/7
145/144 [==============================] - 127s - loss: 0.2678 - acc: 0.9153 - val_loss: 2.6315 - val_acc: 0.4105
Epoch 7/7
145/144 [==============================] - 129s - loss: 0.1877 - acc: 0.9412 - val_loss: 2.7076 - val_acc: 0.4430
 
 
Item 1780: 
{' ': 4,  'a': 2, 'b': 18, 'c': 11, 'd': 13, 'e': 1, 'f': 22, 'g': 14, 'h': 16, 'i': 5, 'j': 26, 'k': 21, 'l': 7, 'm': 17, 'n': 6, 'o': 8, 'p': 15, 'q': 25, 'r': 3, 's': 10, 't': 9,'u': 12, 'v': 23, 'w': 20, 'x': 27, 'y': 19, 'z': 24}
 
 
Item 1785: 
## [[-0.62176132]
##  [-0.01014703]
##  [ 0.01692017]]
 ## Before Sigmoid transform:
## [[-0.41524398]
##  [-0.78135263]
##  [-0.78939634]
##  [-0.65171855]
##  [ 1.03819849]]
## After Sigmoid transform:
## [[0.39765538]
##  [0.31402844]
##  [0.3122983 ]
##  [0.34260237]
##  [0.73850225]]
 ## Before Sigmoid transform:
## [[-0.41524398]
##  [-0.78135263]
##  [-0.78939634]
##  [-0.65171855]
##  [ 1.03819849]]
## After Sigmoid transform:
## [[0.39765538]
##  [0.31402844]
##  [0.3122983 ]
##  [0.34260237]
##  [0.73850225]]
## After Step transform:
## [[0]
##  [0]
##  [0]
##  [0]
##  [1]]
## True condition:
## [[1]
##  [0]
##  [1]
##  [1]
##  [0]]
 ## Thetas from sklearn:
## [[-0.58965696]
##  [-0.01094766]
##  [ 0.01680808]]
## Before Sigmoid transform:
## [[-0.40012884]
##  [-0.80026166]
##  [-0.78308464]
##  [-0.63243339]
##  [ 1.0436855 ]]
## After Sigmoid transform:
## [[0.40128138]
##  [0.30996955]
##  [0.31365546]
##  [0.34695898]
##  [0.7395605 ]]
## After Step transform:
## [[0]
##  [0]
##  [0]
##  [0]
##  [1]]
## True condition:
## [[1]
##  [0]
##  [1]
##  [1]
##  [0]]
 ## [1] "Thetas from glm():"
##                    [,1]
## (Intercept) -0.49961899
## Age         -0.01364591
## Fare         0.01398677
## [1] "Before Sigmoid transform:"
## [1] -0.7763935 -0.7050698 -0.8786190 -0.2014215  0.1313069
## [1] "After Sigmoid transform:"
## [1] 0.3150977 0.3306892 0.2934640 0.4498142 0.5327797
## [1] "After Step transform:"
## [1] 0 0 0 0 1
## [1] "True condition:"
## [1] 0 1 0 0 1
 
 
Item 1788: 
## K-Means Clustering 
 1. Choose the number of clusters(K) and obtain the data points 
2. Place the centroids c_1, c_2, ..... c_k randomly 
3. Repeat steps 4 and 5 until convergence or until the end of a fixed number of iterations
4. for each data point x_i:
       - find the nearest centroid(c_1, c_2 .. c_k) 
       - assign the point to that cluster 
5. for each cluster j = 1..k
       - new centroid = mean of all points assigned to that cluster
6. End 
 
 
Item 1789: 
python google-images-download.py \
--output_directory "fastfood" \
--keywords "McDonalds Logo" \
--limit 100  \
--format jpg

python google-images-download.py \
--output_directory "fastfood" \
--keywords "KFC Logo" \
--limit 100  \
--format jpg

python google-images-download.py \
--output_directory "fastfood" \
--keywords "Burger King Logo" \
--limit 100  \
--format jpg

python google-images-download.py \
--output_directory "fastfood" \
--keywords "Pizza Hut" \
--limit 100  \
--format jpg
 mv fastfood/KFC\ Logo/ fastfood/kfc
mv fastfood/Pizza\ Hut\ Logo/ fastfood/pizzahut
mv fastfood/McDonalds\ Logo/ fastfood/mcdonalds
mv fastfood/Burger\ King\ Logo/ fastfood/burgerking
 
cd tensorflow
mkdir output 
 IMAGE_SIZE=224
ARCHITECTURE="mobilenet_0.50_${IMAGE_SIZE}"
 python retrain.py \
  --image_dir ./fastfood \
  --bottleneck_dir=./output/bottlenecks \
  --how_many_training_steps=500 \
  --model_dir=./output/models/ \
  --summaries_dir=./output/training_summaries/"${ARCHITECTURE}" \
  --architecture="${ARCHITECTURE}" \
  --output_graph ./output/output_graph.pb \
  --output_labels ./output/output_labels.txt \
  --saved_model_dir ./output/saved_model
 python label_image.py \
--graph=./output/output_graph.pb \
--labels=./output/output_labels.txt \
--image=./fastfood/kfc/47.\ kfc-logo.jpg
 Evaluation time (1-image): 0.384s
 kfc 0.98252535
mcdonalds 0.01689682
pizzahut 0.0005701046
burgerking 7.782312e-06
 toco \
--input_file=./output/output_graph.pb \
--output_file=./output/optimized_graph.pb \
--input_format=TENSORFLOW_GRAPHDEF \
--output_format=TENSORFLOW_GRAPHDEF \
--input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \
--input_array=input \
--output_array=final_result
 python label_image.py \
--graph=./output/optimized_graph.pb \
--labels=./output/output_labels.txt \
--image=./fastfood/kfc/2.\ kfc.jpg
 Evaluation time (1-image): 0.174s
 kfc 0.99960905
pizzahut 0.00037051004
burgerking 1.435692e-05
mcdonalds 6.128476e-06
 toco \
  --input_file=./output/optimized_graph.pb \
  --output_file=./output/output_graph.lite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \
  --input_array=input \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_type=FLOAT
 
 
Item 1790: 
[-100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]

 We got 200 inputs ranging from -100 to 100(excluding 100)
 [29550.2, 28957.7, 28371.2, 27790.7, 27216.2, 26647.7, 26085.2, 25528.7, 24978.2, 24433.7, 23895.2, 23362.7, 22836.2, 22315.7, 21801.2, 21292.7, 20790.2, 20293.7, 19803.2, 19318.7, 18840.2, 18367.7, 17901.2, 17440.7, 16986.2, 16537.7, 16095.2, 15658.7, 15228.2, 14803.7, 14385.2, 13972.7, 13566.2, 13165.7, 12771.2, 12382.7, 12000.2, 11623.7, 11253.2, 10888.7, 10530.2, 10177.7, 9831.2, 9490.7, 9156.2, 8827.7, 8505.2, 8188.7, 7878.2, 7573.7, 7275.2, 6982.7, 6696.2, 6415.7, 6141.2, 5872.7, 5610.2, 5353.7, 5103.2, 4858.7, 4620.2, 4387.7, 4161.2, 3940.7, 3726.2, 3517.7, 3315.2, 3118.7, 2928.2, 2743.7, 2565.2, 2392.7, 2226.2, 2065.7, 1911.2, 1762.7, 1620.2, 1483.7, 1353.2, 1228.7, 1110.2, 997.7, 891.2, 790.7, 696.2, 607.7, 525.2, 448.7, 378.2, 313.7, 255.2, 202.7, 156.2, 115.7, 81.2, 52.7, 30.2, 13.7, 3.2, -1.3, 0.2, 7.7, 21.2, 40.7, 66.2, 97.7, 135.2, 178.7, 228.2, 283.7, 345.2, 412.7, 486.2, 565.7, 651.2, 742.7, 840.2, 943.7, 1053.2, 1168.7, 1290.2, 1417.7, 1551.2, 1690.7, 1836.2, 1987.7, 2145.2, 2308.7, 2478.2, 2653.7, 2835.2, 3022.7, 3216.2, 3415.7, 3621.2, 3832.7, 4050.2, 4273.7, 4503.2, 4738.7, 4980.2, 5227.7, 5481.2, 5740.7, 6006.2, 6277.7, 6555.2, 6838.7, 7128.2, 7423.7, 7725.2, 8032.7, 8346.2, 8665.7, 8991.2, 9322.7, 9660.2, 10003.7, 10353.2, 10708.7, 11070.2, 11437.7, 11811.2, 12190.7, 12576.2, 12967.7, 13365.2, 13768.7, 14178.2, 14593.7, 15015.2, 15442.7, 15876.2, 16315.7, 16761.2, 17212.7, 17670.2, 18133.7, 18603.2, 19078.7, 19560.2, 20047.7, 20541.2, 21040.7, 21546.2, 22057.7, 22575.2, 23098.7, 23628.2, 24163.7, 24705.2, 25252.7, 25806.2, 26365.7, 26931.2, 27502.7, 28080.2, 28663.7, 29253.2, 29848.7]
 We got 200 outputs ranging from 0 to 30000
 
 
Item 1793: 
Training epoch: 1
 iter   0:       Loss=2.28,    Training Accuracy=37.0%
 iter 100:       Loss=0.45,    Training Accuracy=85.0%
 iter 200:       Loss=0.21,    Training Accuracy=95.0%
 iter 300:       Loss=0.34,    Training Accuracy=89.0%
 iter 400:       Loss=0.29,    Training Accuracy=90.0%
 iter 500:       Loss=0.18,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 1, validation loss: 0.21, validation accuracy: 94.1%
 ---------------------------------------------------------
 Training epoch: 2
 iter   0:       Loss=0.14,    Training Accuracy=97.0%
 iter 100:       Loss=0.19,    Training Accuracy=94.0%
 iter 200:       Loss=0.11,    Training Accuracy=95.0%
 iter 300:       Loss=0.06,    Training Accuracy=99.0%
 iter 400:       Loss=0.08,    Training Accuracy=96.0%
 iter 500:       Loss=0.12,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 2, validation loss: 0.14, validation accuracy: 96.0%
 ---------------------------------------------------------
 Training epoch: 3
 iter   0:       Loss=0.16,    Training Accuracy=96.0%
 iter 100:       Loss=0.15,    Training Accuracy=96.0%
 iter 200:       Loss=0.08,    Training Accuracy=97.0%
 iter 300:       Loss=0.11,    Training Accuracy=94.0%
 iter 400:       Loss=0.10,    Training Accuracy=96.0%
 iter 500:       Loss=0.09,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 3, validation loss: 0.11, validation accuracy: 96.7%
 ---------------------------------------------------------
 Training epoch: 4
 iter   0:       Loss=0.03,    Training Accuracy=100.0%
 iter 100:       Loss=0.04,    Training Accuracy=99.0%
 iter 200:       Loss=0.11,    Training Accuracy=94.0%
 iter 300:       Loss=0.05,    Training Accuracy=98.0%
 iter 400:       Loss=0.11,    Training Accuracy=97.0%
 iter 500:       Loss=0.11,    Training Accuracy=95.0%
 ---------------------------------------------------------
 Epoch: 4, validation loss: 0.09, validation accuracy: 97.2%
 ---------------------------------------------------------
 Training epoch: 5
 iter   0:       Loss=0.11,    Training Accuracy=97.0%
 iter 100:       Loss=0.04,    Training Accuracy=98.0%
 iter 200:       Loss=0.04,    Training Accuracy=100.0%
 iter 300:       Loss=0.02,    Training Accuracy=100.0%
 iter 400:       Loss=0.11,    Training Accuracy=97.0%
 iter 500:       Loss=0.04,    Training Accuracy=99.0%
 ---------------------------------------------------------
 Epoch: 5, validation loss: 0.08, validation accuracy: 97.7%
 ---------------------------------------------------------
 Training epoch: 6
 iter   0:       Loss=0.04,    Training Accuracy=100.0%
 iter 100:       Loss=0.07,    Training Accuracy=98.0%
 iter 200:       Loss=0.08,    Training Accuracy=98.0%
 iter 300:       Loss=0.04,    Training Accuracy=99.0%
 iter 400:       Loss=0.04,    Training Accuracy=99.0%
 iter 500:       Loss=0.05,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 6, validation loss: 0.07, validation accuracy: 97.9%
 ---------------------------------------------------------
 Training epoch: 7
 iter   0:       Loss=0.04,    Training Accuracy=99.0%
 iter 100:       Loss=0.03,    Training Accuracy=99.0%
 iter 200:       Loss=0.07,    Training Accuracy=99.0%
 iter 300:       Loss=0.06,    Training Accuracy=98.0%
 iter 400:       Loss=0.13,    Training Accuracy=96.0%
 iter 500:       Loss=0.07,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 7, validation loss: 0.07, validation accuracy: 97.8%
 ---------------------------------------------------------
 Training epoch: 8
 iter   0:       Loss=0.03,    Training Accuracy=99.0%
 iter 100:       Loss=0.01,    Training Accuracy=100.0%
 iter 200:       Loss=0.03,    Training Accuracy=99.0%
 iter 300:       Loss=0.02,    Training Accuracy=100.0%
 iter 400:       Loss=0.02,    Training Accuracy=100.0%
 iter 500:       Loss=0.05,    Training Accuracy=98.0%
 ---------------------------------------------------------
 Epoch: 8, validation loss: 0.07, validation accuracy: 97.8%
 ---------------------------------------------------------
 Training epoch: 9
 iter   0:       Loss=0.04,    Training Accuracy=99.0%
 iter 100:       Loss=0.01,    Training Accuracy=100.0%
 iter 200:       Loss=0.07,    Training Accuracy=98.0%
 iter 300:       Loss=0.03,    Training Accuracy=99.0%
 iter 400:       Loss=0.03,    Training Accuracy=99.0%
 iter 500:       Loss=0.09,    Training Accuracy=97.0%
 ---------------------------------------------------------
 Epoch: 9, validation loss: 0.07, validation accuracy: 97.9%
 ---------------------------------------------------------
 Training epoch: 10
 iter   0:       Loss=0.02,    Training Accuracy=100.0%
 iter 100:       Loss=0.03,    Training Accuracy=100.0%
 iter 200:       Loss=0.02,    Training Accuracy=100.0%
 iter 300:       Loss=0.02,    Training Accuracy=100.0%
 iter 400:       Loss=0.01,    Training Accuracy=100.0%
 iter 500:       Loss=0.01,    Training Accuracy=100.0%

 ---------------------------------------------------------
 Epoch: 10, validation loss: 0.07, validation accuracy: 98.1%
 ---------------------------------------------------------
 
 
Item 1794: 
Crowd Classification -> either singles, tens or hundreds crowd prediction -> count of people
 error: Couldn't lookup symbols:
  type metadata for CrowdCountApiMac.FriendlyClassification
  ...
  __swift_FORCE_LOAD_$_swiftCoreMedia
  __swift_FORCE_LOAD_$_swiftCoreAudio
  CrowdCountApiMac.FriendlyPredictor.DensityMapWidth.unsafeMutableAddressor : Swift.Int
  ...
 usage -> more data -> improved usage -> more data -> improved usage -> ...
 
 
Item 1801: 
SELECT
   radio, 
   COUNT(*) c
FROM `fh-bigquery.public_dump.opencellid_201803` 
GROUP BY radio
ORDER BY c DESC
 
 
Item 1803: 
SSFF => parent node
 1. P(slow) => fraction of slow examples in parent node
2. P(fast) => fraction of fast examples in parent node
 Entropy(parent) = - {0.5 log2(0.5) + 0.5 log2(0.5)}
                = - {-0.5 + (-0.5)}
                = 1  
 P(slow) = 2/3 = 0.667
P(fast) = 1/3 = 0.334
 Entropy(SSF) = - {0.667 log2(0.667) + 0.334 log2(0.334)}
             = - {-0.38 + (-0.52)}
             = 0.9
 Total number of examples in parent node: 4
  "      "    "     "     "   left child node: 3
  "      "    "     "     "   right child node: 1
 [Weighted avg]Entropy(children) = 
(no. of examples in left child node) / (total no. of examples in parent node) * (entropy of left node) 
+ 
(no. of examples in right child node)/ (total no. of examples in parent node) * (entropy of right node)
 Information gain(Grade) = 1 - 0.675
                        = 0.325
 [weighted avg.]entropy(children) = 2/4 * 1 + 2/4 * 1
                                 = 1
 Information gain(Bumpiness) = 1 - 1
                            = 0
 IG(Grade) => 0.325
IG(Bumpiness) => 0
 [weighted avg.] entropy(children) = 2/4 *0 + 2/4 *0
                                  = 0
 Information gain(SpeedLimit) = 1 - 0
                             = 1
 IG(Grade) => 0.325
IG(Bumpiness) => 0
IG(SpeedLimit) => 1
 
 
Item 1804: 
TRAIN_DIR = 'Fruit-Images-Dataset-master/Training'
VALIDATION_DIR = 'Fruit-Images-Dataset-master/Validation'
 NUM_CLASSES = 15
TRAIN_SAMPLES = 20
VAL_SAMPLES = 20
 _, all_label_names, _ = next(os.walk(TRAIN_DIR))
 label_names = all_label_names[:NUM_CLASSES]
 train = get_data(TRAIN_DIR, label_names, TRAIN_SAMPLES)
val = get_data(VALIDATION_DIR, label_names, VAL_SAMPLES)
 def queue(label, samples):
  return requests.post(
    'http://localhost:8000/v1/batch_queue/%s' % label, 
    json={
      'batch_images': [{'image': image} for image in samples]
    })
 for label in train:
  print(queue(label, train[label]).json())
 print(requests.post(‘http://localhost:8000/v1/train’).json())
 requests.get(‘http://localhost:8000/v1/train/status').json()
 
 
Item 1806: 
cd
mkdir server
 cd
touch -c .bash_profile
open -a TextEdit .bash_profile
 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/
export SPARK_HOME=/Users/luckspark/server/spark-2.3.0-bin-hadoop2.7
export SBT_HOME=/Users/luckspark/server/sbt
export SCALA_HOME=/Users/luckspark/server/scala-2.11.12
 export PATH=$JAVA_HOME/bin:$SBT_HOME/bin:$SBT_HOME/lib:$SCALA_HOME/bin:$SCALA_HOME/lib:$PATH
 export PATH=$JAVA_HOME/bin:$SPARK_HOME:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
 export PYSPARK_PYTHON=python3
 PATH=”/Library/Frameworks/Python.framework/Versions/3.6/bin:${PATH}”
export PATH
 
 
Item 1812: 
word_embedding = WordEmbedding()
word_embedding.load(source='word2vec', file_path=word2vec_file_path)
 print(word_embedding.get_vector(source='word2vec', word='apple'))
 [-0.06445312 -0.16015625 -0.01208496  0.13476562 -0.22949219  0.16210938
  0.3046875  -0.1796875  -0.12109375  0.25390625 -0.01428223 -0.06396484
...]
 word_embedding = WordEmbedding()
word_embedding.load(source='glove', file_path=glove_file_path)
 print(word_embedding.get_vector(source='glove', word='apple'))
 [ 0.52042001 -0.83139998  0.49961001  1.28929996  0.1151      0.057521
 -1.37530005 -0.97312999  0.18346     0.47672001 -0.15112001  0.35532001
...]
 word_embedding = WordEmbedding()
word_embedding.load(source='fasttext', file_path=fasttext_file_path)
 print(word_embedding.get_vector(source='fasttext', word='apple'))
 [ 0.26407328  0.30484504  0.04900438 -0.44377801  0.16519009 -0.09473443
 -0.01351437 -0.17237368  0.0374852   0.34962645  0.14334701 -0.11134619
...]
 word_embedding.build_visual_metadata(embedding=embedding, words=words, file_dir='./word_embedding')
 
 
Item 1820: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 1822: 
/myprojects/2010/10/thebigstory/01 Import and clean.ipynb
/myprojects/2010/10/thebigstory/02 Analysis.ipynb
 
 
Item 1825: 
library(dplyr)
library(ggplot2)
library(gridExtra)
library(tidyr)
library(reshape2)
library(RColorBrewer)
library(ggrepel)
 df = read.csv(file="/home/akshaj/projects_R/Pokemon/pokemon.csv")
df = tbl_df(df)
colnames(df)[25] <- "classification"
df$capture_rate <- as.numeric(df$capture_rate)
head(df)
 df = select(df, name, classification, hp, weight_kg, 
            height_m, speed, attack, defense,
            sp_attack, sp_defense, type1, type2, 
            abilities, generation,is_legendary, 
            capture_rate)
 head(df)
 density_hp <- ggplot(data=df, aes(hp)) + geom_density(col="white",fill="pink", alpha=0.8) + ggtitle("Density Plot of HP")
 density_speed <- ggplot(data=df, aes(speed)) + geom_density(col="white", fill="darkorchid", alpha=0.8) + ggtitle("Density Plot of Speed Characterstics")
 density_attack <- ggplot(data=df, aes(attack)) + geom_density(col="white", fill="orange", alpha=0.7) + ggtitle("Density Plot of Attack Characterstics")
 density_defense <- ggplot(data=df, aes(defense)) + geom_density(col="white", fill="firebrick", alpha=0.7) + ggtitle("Density Plot of Defense Characterstics")
 density_height <- ggplot(data=df, aes(height_m)) + geom_density(col="white", fill="slateblue1", alpha=0.8) + ggtitle("Density Plot of Height (m) ")
 density_weight <- ggplot(data=df, aes(weight_kg)) + geom_density(col="white", fill="mediumturquoise", alpha=0.8) + ggtitle("Density Plot of Weight (kg)")
 grid.arrange(density_hp, density_speed, density_attack, density_defense, density_height, density_weight, ncol=2)
 type_1_poke <- ggplot(data=df, aes(type1)) + geom_bar(aes(fill=..count..), alpha=0.8) + theme(axis.text.x = element_text(angle = 90, hjust = 0)) + ggtitle("Distribution Based on Type-1") + coord_flip()
 type_2_poke <- ggplot(data=df, aes(type2)) + geom_bar(aes(fill=..count..), alpha=0.8) + theme(axis.text.x = element_text(angle = 90, hjust = 0)) + ggtitle("Distribution Based on Type-2") + coord_flip()
 grid.arrange(type_1_poke, type_2_poke, ncol=2)
 df %>%
    filter(is_legendary==1) %>%
    ggplot(aes(type1)) + geom_bar(aes(fill= ..count..)) +       theme(axis.text.x = element_text(angle=90, hjust=0)) +     ggtitle("Number of Legendary Pokemon Based on Type-1")
 ggplot(data=df, aes(attack, defense))  + geom_point(aes(color=is_legendary), alpha=0.8) + scale_color_gradient(low="darkblue", high="red") + ggtitle("Defense vs Attack Characterstics") + 
geom_label_repel(data=subset(df,attack > 150 | defense >150 |              attack < 25), aes(label=name), 
              box.padding = 0.35, point.padding = 0.5,
              segment.color = 'grey50')
 speed_attack_legendary <- ggplot(na.omit(df), aes(attack, speed)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (attack > 170 | attack < 50 & speed >150 | speed < 50) & is_legendary == 1 | speed > 145), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 weight_attack_legendary <- ggplot(na.omit(df), aes(attack, weight_kg)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (attack > 170 | attack < 50 | weight_kg > 650) & (is_legendary == 1)), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 height_attack_legendary <- ggplot(na.omit(df), aes(attack, height_m)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, ((attack > 170 | attack < 50 | height_m > 7.5) & is_legendary == 1) | height_m > 5 & is_legendary == 0), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 hp_attack_legendary <- ggplot(na.omit(df), aes(attack, hp)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, ((attack > 170 | hp > 190 | attack < 50) & is_legendary == 1) | hp >160), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50')  + geom_smooth(method = "lm")
 grid.arrange(speed_attack_legendary, weight_attack_legendary, height_attack_legendary, hp_attack_legendary, ncol = 2)
 speed_defense_legendary <- ggplot(na.omit(df), aes(defense, speed)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (defense > 130 | defense < 50| speed > 140 | speed < 50) & is_legendary == 1), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 weight_defense_legendary <- ggplot(na.omit(df), aes(defense, weight_kg)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (defense > 160 | defense < 50 | weight_kg > 600) & is_legendary == 1), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 height_defense_legendary <- ggplot(na.omit(df), aes(defense, height_m)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (defense > 150 | defense < 50 | height_m > 6) & is_legendary == 1 | height_m >5 & is_legendary ==0), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 hp_defense_legendary <- ggplot(na.omit(df), aes(defense, hp)) + geom_point(aes(color=is_legendary)) + geom_label_repel(data=subset(df, (defense > 150 | defense < 50 | hp > 150) & is_legendary == 1 | hp > 200), aes(label=name), box.padding = 0.35, point.padding = 0.5, segment.color = 'grey50') + geom_smooth(method = "lm")
 grid.arrange(speed_defense_legendary, weight_defense_legendary, height_defense_legendary, hp_defense_legendary, ncol = 2)
 box_plot_attr <- select(df, type1, is_legendary, hp, defense, attack, sp_attack, sp_defense, speed)
 box_plot_attr_leg <- filter(box_plot_attr, is_legendary == 1)
 box_plot_attr_nor <- filter(box_plot_attr, is_legendary == 0)
 box_plot_attr_leg_long <- gather(box_plot_attr_leg, attribute, value, -c(type1, is_legendary))
 box_plot_attr_nor_long <- gather(box_plot_attr_nor, attribute, value, -c(type1, is_legendary))
 bp_leg <- ggplot(data = box_plot_attr_leg_long, aes(attribute, value)) + geom_boxplot(fill="green4") + ggtitle("Legendary Pokemon")
 bp_nor <- ggplot(data = box_plot_attr_nor_long, aes(attribute, value)) + geom_boxplot(fill = "yellow2") + ggtitle("Normal Pokemon")
 grid.arrange(bp_leg, bp_nor,ncol=2)
 hmap_attr <- select(df, type1, is_legendary, hp, defense, attack, sp_attack, sp_defense, speed)
 hmap_attr_leg <- filter(hmap_attr, is_legendary == 1)
hmap_attr_nor <- filter(hmap_attr, is_legendary == 0)
 hmap_attr_leg <- group_by(hmap_attr_leg, type1)
hmap_attr_nor <- group_by(hmap_attr_nor, type1)
 hmap_attr_leg <- summarise(hmap_attr_leg, hp=median(hp), attack=median(attack), defense=median(defense), sp_attack=median(sp_attack), sp_defense=median(sp_defense), speed=median(speed))
 hmap_attr_leg_m <- melt(hmap_attr_leg)
 hmap_attr_nor <- summarise(hmap_attr_nor, hp=median(hp), attack=median(attack), defense=median(defense), sp_attack=median(sp_attack), sp_defense=median(sp_defense), speed=median(speed))
 hmap_attr_nor_m <- melt(hmap_attr_nor)
 hm.palette <- colorRampPalette(rev(brewer.pal(5, 'RdYlBu')), space='Lab')
 ggplot(data=hmap_attr_leg_m, aes(type1, variable)) + geom_tile(aes(fill=value)) + ggtitle("Legendary Pokemon: Type1 - Attribute") + scale_fill_gradientn(colours = hm.palette(100)) + theme(axis.text.x = element_text(angle=90, hjust=0)) + coord_equal()
 hm.palette <- colorRampPalette(rev(brewer.pal(5, 'RdYlBu')), space='Lab')
 ggplot(data=hmap_attr_nor_m, aes(type1, variable)) + geom_tile(aes(fill=value)) + ggtitle("Non-Legendary Pokemon: Type1 - Attribute") + scale_fill_gradientn(colours = hm.palette(100)) + theme(axis.text.x = element_text(angle=90, hjust=0)) + coord_equal()
 hmap_attr <- select(df, type1, is_legendary, hp, defense, attack, sp_attack, sp_defense, speed)
 hmap_attr_leg <- filter(hmap_attr, is_legendary == 1)
hmap_attr_nor <- filter(hmap_attr, is_legendary == 0)
 hmap_attr_leg <- group_by(hmap_attr_leg, type1)
hmap_attr_nor <- group_by(hmap_attr_nor, type1)
 hmap_attr_leg <- summarise(hmap_attr_leg, hp=median(hp), attack=median(attack), defense=median(defense), sp_attack=median(sp_attack), sp_defense=median(sp_defense), speed=median(speed))
 hmap_attr_nor <- summarise(hmap_attr_nor, hp=median(hp), attack=median(attack), defense=median(defense), sp_attack=median(sp_attack), sp_defense=median(sp_defense), speed=median(speed))
 row.names(hmap_attr_leg) <- hmap_attr_leg$type1
hmap_attr_leg$type1 <- NULL
hmap_attr_leg$is_legendary <- NULL
 row.names(hmap_attr_nor) <- hmap_attr_nor$type1
hmap_attr_nor$type1 <- NULL
hmap_attr_nor$is_legendary <- NULL
 hmap_attr_leg_cor <- cor(hmap_attr_leg)
hmap_attr_leg_cor_m <- melt(hmap_attr_leg_cor)
 hm.palette <- colorRampPalette(rev(brewer.pal(5, 'GnBu')), space='Lab')
 ggplot(data=hmap_attr_leg_cor_m, aes(Var1, Var2)) + geom_tile(aes(fill=value)) + ggtitle("Attribute Correlation - Legendary") + scale_fill_gradientn(colours = hm.palette(100)) + coord_equal()
 hmap_attr_nor_cor <- cor(hmap_attr_nor)
hmap_attr_nor_cor_m <- melt(hmap_attr_nor_cor)
 hm.palette <- colorRampPalette(rev(brewer.pal(5, 'GnBu')), space='Lab')
 ggplot(data=hmap_attr_nor_cor_m, aes(Var1, Var2)) + geom_tile(aes(fill=value)) + ggtitle("Attribute Correlation - Normal") + scale_fill_gradientn(colours = hm.palette(100)) + coord_equal()
 df_fight_against <- select(df, type1, against_bug:against_water)
head(df_fight_against)
 df_fight_against_g <- group_by(df_fight_against, type1)
 df_fight_against_summ <- summarise(df_fight_against_g, 
                      against_bug = median(against_bug), 
                      against_dark = median(against_dark), 
                      against_dragon = median(against_dragon),
                      against_electric = median(against_electric),
                      against_fairy = median(against_fairy),
                      against_fight = median(against_fight),
                      against_fire = median(against_fire),
                      against_flying = median(against_flying),
                      against_ghost = median(against_ghost),
                      against_grass = median(against_grass),
                      against_ground = median(against_ground),
                      against_ice = median(against_ice), 
                      against_normal = median(against_normal),
                      against_poison  = median(against_poison),
                      against_psychic = median(against_psychic),
                      against_rock = median(against_rock),
                      against_steel = median(against_steel),
                      against_water = median(against_water))
 df_fight_against_long <- melt(df_fight_against_summ)
 hm.palette <- colorRampPalette(rev(brewer.pal(9, 'RdYlBu')), space='Lab')
 ggplot(data=df_fight_against_long, aes(type1, variable)) + geom_tile(aes(fill=value)) + scale_fill_gradientn(colours = hm.palette(100)) + coord_equal() + theme(axis.text.x=element_text(angle=90, hjust=0)) + ggtitle("Effectiveness of different types of Pokemon")
 
 
Item 1826: 
"Design de Antecipação é o uso do poder da tecnologia para eliminar decisões das nossas vidas."
 — Jonatas Guerci
 Antes que seja tarde, e a tecnologia se torne um problema perveso.
 
 
Item 1827: 
x_train.shape
# (1600, 100, 100, 3)
 Epoch 1/50
1600/1600 [==============================] - 6s 4ms/step - loss: 2.7115 - acc: 0.3275 - val_loss: 2.3993 - val_acc: 0.4275
Epoch 2/50
1600/1600 [==============================] - 2s 1ms/step - loss: 2.0483 - acc: 0.5375 - val_loss: 1.9218 - val_acc: 0.5050
Epoch 3/50
1600/1600 [==============================] - 2s 1ms/step - loss: 1.8621 - acc: 0.5888 - val_loss: 1.8554 - val_acc: 0.5925
....
....
....
1600/1600 [==============================] - 2s 1ms/step - loss: 0.0560 - acc: 0.8706 - val_loss: 4.4425 - val_acc: 0.6500
Epoch 47/50
1600/1600 [==============================] - 2s 1ms/step - loss: 0.0719 - acc: 0.8700 - val_loss: 4.2263 - val_acc: 0.6950
Epoch 48/50
1600/1600 [==============================] - 2s 1ms/step - loss: 0.0571 - acc: 0.8744 - val_loss: 4.4416 - val_acc: 0.6750
Epoch 49/50
1600/1600 [==============================] - 2s 1ms/step - loss: 0.0615 - acc: 0.8687 - val_loss: 4.1540 - val_acc: 0.6550
Epoch 50/50
1600/1600 [==============================] - 2s 1ms/step - loss: 0.0394 - acc: 0.8638 - val_loss: 4.3329 - val_acc: 0.6800
 infer(x_train, model=model)
"""
[['sea', 'sunset'],
 ['sea'],
 ['mountain'],
 ['sea', 'sunset'],
 ['sunset'],
 ['trees'],
 ['sea'],
 ['trees'],
 ['mountain', 'trees'],
 ['sea'],
 ['sunset'],
 ['desert'],
 ['sea'],
 ['desert'],
 ['desert'],
 ['desert'],
 ['desert'],
.....
"""
 
 
Item 1829: 
explainer = shap.DeepExplainer(pipeline.model, encoded_x_train[:10])
shap_values = explainer.shap_values(encoded_x_test[:1])
 x_test_words = prepare_explanation_words(pipeline, encoded_x_test)
y_pred = pipeline.predict(x_test[:1])
print('Actual Category: %s, Predict Category: %s' % (y_test[0], y_pred[0]))
 shap.force_plot(explainer.expected_value[0], shap_values[0][0], x_test_words[0])
 kernel_explainer = shap.KernelExplainer(pipeline.model.predict, encoded_x_train[:10])
kernel_shap_values = kernel_explainer.shap_values(encoded_x_test[:1])
 x_test_words = prepare_explanation_words(pipeline, encoded_x_test)
y_pred = pipeline.predict(x_test[:1])
print('Actual Category: %s, Predict Category: %s' % (y_test[0], y_pred[0]))
 shap.force_plot(kernel_explainer.expected_value[0], kernel_shap_values[0][0], x_test_words[0])
 
 
Item 1832: 
>>> print("Hi Python")
 >>> print("Hi Python")
 
 
Item 1837: 
python -m pysc2.bin.agent \
--map Simple64 \
--agent sparse_agent.SparseAgent \
--agent_race T \
--max_agent_steps 0 \
--norender
 
 
Item 1838: 
data %>%
    filter(!is.na(State))
 data %>%
    filter(!is.na(State)) %>%
    gather(year, employment, `CY 2009`, `CY 2010`, `CY 2011`, `CY 2012`, `CY 2013`, `CY 2014`, `CY 2015`, na.rm = TRUE, convert = TRUE)
 str_remove(year, "CY ")
 data %>%
    filter(!is.na(State)) %>%
    gather(year, employment, `CY 2009`, `CY 2010`, `CY 2011`, `CY 2012`, `CY 2013`, `CY 2014`, `CY 2015`, na.rm = TRUE, convert = TRUE) 
      mutate(year = parse_number(year))
 data %>%
    filter(!is.na(State)) %>%
    gather(year, employment, `CY 2009`, `CY 2010`, `CY 2011`, `CY 2012`, `CY 2013`, `CY 2014`, `CY 2015`, na.rm = TRUE, convert = TRUE) 
      mutate(year = parse_number(year), State_statecode = statecode(State, output_type = "name"))
 
 
Item 1839: 
def alpha_k(self, k, dim):
    if k == 2:
        ak = 1.0 - 3.0 / (4.0 * dim)
    else:
        ak1 = self.a_k[k - 1]
        ak = ak1 + (1.0 - ak1) / 6.0
    return ak
 def cluster_eval(self, k, dim):
    if k == 1 or self.s_k[k - 1] == 0.0:
        return 1.0

    self.a_k[k] = self.alpha_k(k, dim)
    if self.s_k[k - 1] != 0.0:
        return self.s_k[k] / (self.a_k[k] * self.s_k[k - 1])
    else:
        return 1.0
 def fit_s_k(self, s_k, max_k=50, dim=2):
    """Fits the value of K using the s_k series"""
    self.a_k = dict()
    f_k = np.ones(max_k)

    for k in range(1, max_k + 1):
        i = k - 1
        self.s_k[k] = s_k[k]
        f_k[i] = self.cluster_eval(k, dim)
    k = np.argmin(f_k)

    if f_k[k] <= 0.85:
        self.K = np.argmin(f_k) + 1
    else:
        self.K = 1
    return self.K
 @staticmethod
def distance_to_line(x0, y0, x1, y1, x2, y2):
    """
    Calculates the distance from (x0,y0) to the 
    line defined by (x1,y1) and (x2,y2)
    """
    dx = x2 - x1
    dy = y2 - y1
    return abs(dy * x0 - dx * y0 + x2 * y1 - y2 * x1) / \
           math.sqrt(dx * dx + dy * dy)
 def fit_s_k(self, s_k, tolerance=1e-3):
    """Fits the value of K using the s_k series"""
    max_distance = float('-inf')
    s_k_list = list()
    sk0 = 0

    for k in s_k:
        sk1 = s_k[k]
        s_k_list.append(sk1)
        if k > 2 and abs(sk0 - sk1) < tolerance:
            break
        sk0 = sk1

    s_k = np.array(s_k_list)
    x0 = 1
    y0 = s_k[0]

    x1 = len(s_k)
    y1 = 0

    for k in range(1, len(s_k)):
        dist = self.distance_to_line(k, s_k[k-1], x0, y0, x1, y1)
        if dist > max_distance:
            max_distance = dist
            self.K = k
 def fit_s_k(self, s_k, max_k=50):
    """Fits the value of K using the s_k series"""
    r_k = dict()
    max_val = float('-inf')

    for k in range(1, max_k + 1):
        r_k[k] = 1.0 / s_k[k]

        if k > 1:
            d = (r_k[k] - r_k[k-1]) / math.log(k)
            if d > max_val:
                max_val = d
                self.K = k
    self.s_k = s_k
    return self.K
 def load_data():
    clusters = random.randint(2, 15)
    print('K={0}'.format(clusters))
    X, y = make_blobs(n_samples=100*clusters,
                      centers=clusters,
                      cluster_std=4,
                      n_features=2,
                      center_box=(-100.0, 100.0))
    df = pd.DataFrame(data=X, columns=['x', 'y'])
    return df
 for k in range(1, 51):
    km = KMeans(n_clusters=k).fit(df)
    s_k[k] = km.inertia_
 pham_estimator = PhamDimovNguyen.KEstimator()
asanka_estimator = AsankaPerera.KEstimator()
riddle_estimator = ReciprocalDeltaLog.KEstimator()
 asanka_estimator.fit_s_k(s_k, tolerance=1e-3)
print('Asanka : {0}'.format(asanka_estimator.K))

pham_estimator.fit_s_k(s_k, max_k=40, dim=dim)
print('PhamDN : {0}'.format(pham_estimator.K))

riddle_estimator.fit_s_k(s_k, max_k=40)
print('Riddle : {0}'.format(riddle_estimator.K))
 
 
Item 1841: 
@functools.lru_cache(maxsize=128)
def find_topics(data):
    topics = topicmodelling.form_topics(data)
    return topics
 @app.route('/get_topics', methods=['POST'])
def get_topics():
    topics = find_topics(request.form.to_dict())
    return jsonify(topics)
 @app.route('/get_topics', methods=['POST'])
def get_topics():
    class HDict(dict):
        def __hash__(self):
            return hash(frozenset(self.items()))
    
    hashed_data = HDict(request.form.to_dict())
    topics = find_topics(hashed_data)
    
    return jsonify(topics)
 
 
Item 1842: 
proc quantreg ci=sparsity/iid algorithm=interior(tolerance=1.e-4)
data=new;
class visit MomEdLevel;
model weight = black married boy visit MomEdLevel MomSmoke
cigsperday MomAge MomAge*MomAge
MomWtGain MomWtGain*MomWtGain/
quantile= 0.05 to 0.95 by 0.05
plot=quantplot;
run;
 proc quantreg ci=sparsity/iid algorithm=interior(tolerance=1.e-4)
data=new;
class visit MomEdLevel;
model weight = black married boy visit MomEdLevel MomSmoke
cigsperday MomAge MomAge*MomAge
MomWtGain MomWtGain*MomWtGain/
quantile= 0.05 to 0.95 by 0.05;
output out = predictquant p = predquant;
run;
 data mwtgaingraph;
set predictquant;
where black = 0 and married = 1 and boy = 1 and MomAge = 0 and MomSmoke = 0 and visit = 3 and MomEdLevel = 3;
run;

 proc sort data = mwtgaingraph;
by MomWtGain;
run;
 proc sgplot data = mwtgaingraph;
title ’Quantile fit plot for maternal weight gain’;
yaxis label = “Predicted birth weight”;
series x = MomWtGain y = predquant1 /curvelabel = “5 %tile”;
series x = MomWtGain y = predquant2/curvelabel = “10 %tile”;
series x = MomWtGain y = predquant5/curvelabel = “25 %tile”;
series x = MomWtGain y = predquant10/curvelabel = “50 %tile”;
series x = MomWtGain y = predquant15/curvelabel = “75 %tile”;
series x = MomWtGain y = predquant18/curvelabel = “90 %tile”;
series x = MomWtGain y = predquant19/curvelabel = “95 %tile”;
run;
 
 
Item 1845: 
1. Tensorflow
2. Keras
3. Neon
4. Theano
5. pyTouch
 
 
Item 1850: 
CPU: i7 7700HQ
RAM: 16GB de 2400MHZ
GPU: GTX 1050ti
CUDA: 9.1
 git clone https://github.com/pjreddie/darknet
 cd darknet
 make
 nvidia-smi
 
 
Item 1853: 
Antes: índices == estructuras de datos
Ahora: índices == modelos
 
 
Item 1854: 
tf.image.resize_images(logits, (28, 28))
 
 
Item 1858: 
French Sentence: paris est jamais agréable en décembre , et il est relaxant au mois d' août . 
English Sentence: paris is never nice during december , and it is relaxing in august . 
 French Sentence: elle déteste les pommes , les citrons verts et les citrons . 
English Sentence: she dislikes apples , limes , and lemons . 
 French Sentence: la france est généralement calme en février , mais il est généralement chaud en avril . 
English Sentence: france is usually quiet during february , but it is usually hot in april . 
 French Sentence: la souris était mon animal préféré moins . 
English Sentence: the mouse was my least favorite animal . 
 French Sentence: paris est parfois clémentes en septembre , et il gèle habituellement en août . 
English Sentence: paris is sometimes mild during september , and it is usually freezing in august .
 French Sentence: je vais laver les plats 
English Sentence: ill wash dishes 
 French Sentence: les nouvelles les rendirent heureux 
English Sentence: the news made them happy 
 French Sentence: globalement la conférence internationale fut un succès 
English Sentence: all in all the international conference was a success 
 French Sentence: comment marche cet appareil photo 
English Sentence: how do you use this camera 
 French Sentence: cest ton jour de chance 
English Sentence: this is your lucky day
 Input Sentence: californie est sec en janvier , mais il est généralement occupé en mars . </s> 
 Target Sentence: california is dry during january , but it is usually busy in march .</s> 
 LSTM model output: california is dry during january , but it is usually busy in march . </s> 
 GRU model output: california is dry during january , but it is usually busy in march . </s>
 Input Sentence: new jersey est généralement chaud en juin , et il est parfois merveilleux en hiver . </s> 
 Target Sentence: new jersey is usually hot during june , and it is sometimes wonderful in winter .</s> 
 LSTM model output: new jersey is usually warm during june , and it is sometimes wonderful in winter . </s> 
 GRU model output: new jersey is usually hot during june , and it is sometimes wonderful in winter . </s>
 Input Sentence: <unk> les fraises , les mangues et le pamplemousse . </s> 
 Target Sentence: i like strawberries , mangoes , and grapefruit .</s> 
 LSTM model output: i like strawberries , mangoes , and grapefruit . </s> 
 GRU model output: i like strawberries , mangoes , and grapefruit . </s>
 Input Sentence: jai perdu mon intérêt pour le golf </s> 
 Target Sentence: ive lost interest in golf</s> 
 LSTM model output: i lost my interest golf </s> 
 GRU model output: ive lost interest in golf </s>
 Input Sentence: le livre était meilleur que le film </s> 
 Target Sentence: the book was better than the movie</s> 
 LSTM model output: the book was better than the movie </s> 
 GRU model output: the book was better than the movie </s>
 Input Sentence: quel genre de trucs <unk> le weekend </s> 
 Target Sentence: what sort of things do you do on weekends</s> 
 LSTM model output: what sort of things do you do on weekends </s> 
 GRU model output: what sort of stuff do you do on weekends </s>
 
 
Item 1859: 
https://exploratory.io/tag/Team%20Exploratory
 
 
Item 1861: 
input(symptoms)
If symptoms in disease[symptoms]:
	Add disease in [possible_diseases]
 Input(blood_test_result)
If blood_test_result not in disease in[possible_diseases]
	Remove disease from [possible_diseases]
 X = 0
Start medication(possible_diseases[x])
	Return result;
 while result = null:
	x++
Start medication(possible_diseases[x])
 
 
Item 1862: 
HIDDEN_UNITS=512
SEQUENCE_LEN=100
EMBED_SIZE=300
NUM_EPOCHS=1000
 embedding_layer = Embedding(len(word_index),
                            EMBED_SIZE,
                            weights=[embedding_matrix],
                            input_length=SEQUENCE_LEN,
                            trainable=False)
 input = Input(shape=(SEQUENCE_LEN,), name="input")
embedding_layer = embedding_layer(input)
encoder = LSTM(HIDDEN_UNITS, name="encoder")(embedding_layer)
decoder = RepeatVector(SEQUENCE_LEN)(encoder)
decoder = LSTM(EMBED_SIZE, return_sequences=True)(decoder)
autoencoder = Model(input, decoder)
autoencoder.compile(optimizer="adam", loss="mse")
autoencoder.fit(train, epochs=NUM_EPOCHS, callbacks=[checkpoint])
 encoder = Model(autoencoder.input,autoencoder.get_layer("encoder").output)
 
 
Item 1876: 
{
 "name": "tensorflowjs",
 "version": "1.0.0",
 "dependencies": {
 "express": "latest"
 }}
 npm install
 node server.js
 
 
Item 1885: 
theta_plus = theta
theta_minus = theta
theta_plus[index] = theta_plus[index] + epsilon
theta_minus[index] = theta_minus[index] - epsilon
 
 
Item 1888: 
1
 2
 3
 4
 function [jVal, gradient] = costFunction(theta)
 jVal = [...code to compute J(theta)...];
 gradient = [...code to compute derivative of J(theta)...];
 end
 1
 2
 3
 4
 options = optimset('GradObj', 'on', 'MaxIter', 100);
 initialTheta = zeros(2,1);
 [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta,
 options);
 
 
Item 1891: 
def sigmoid(X, theta):
    return 1 / (1 + np.exp(-np.dot(X, theta[0])))
 def loss(X, theta, y):
    h = sigmoid(X, theta)
    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
 h = sigmoid(X, theta)
gradient_wrt_theta = np.dot(X.T, (h - y)) / y.shape[0]
 h = sigmoid(X, theta)
gradient_wrt_X = np.dot(np.expand_dims((h-y),1),theta)/y.shape[0]
 #Normalizing gradient vectors to make sure step size is consistent
#necessary for our 2-d example, but not called for in the paper
 gradient_magnitudes = np.expand_dims(np.asarray(list(map(lambda x: np.linalg.norm(x), gradient_wrt_X))),1)
grads_norm = gradient_wrt_X/gradient_magnitudes
 #Creating the adversarial perturbation
epsilon = 0.5 #The step size be adjusted 
X_advs = X+grads_norm*epsilon
 
 
Item 1893: 
fastqc sample1.fastq.gz --outdir=/some/other/dir/
 multiqc .
 java -jar picard.jar MarkDuplicates \
      I=input.bam \
      O=marked_duplicates.bam \
      M=marked_dup_metrics.txt
 [HEADER]
#CHROM  POS     ID      REF    ALT      QUAL    FILTER  INFO                    
1	889455	.	G	A	100.0	PASS	AF=0.0005
1	897062	.	C	T	100.0	PASS	AF=0.0005
 #CHROM POS     ID        REF    ALT     QUAL FILTER INFO                    
1	889455	.	G	A	100.0	PASS	AF=0.0005;EFF=STOP_GAINED(HIGH|NONSENSE|Cag/Tag|Q236*|749|NOC2L||CODING|NM_015658|)
1	897062	.	C	T	100.0	PASS	AF=0.0005;EFF=STOP_GAINED(HIGH|NONSENSE|Cag/Tag|Q141*|642|KLHL17||CODING|NM_198317|
 cat variants.vcf | java -jar SnpSift.jar filter " ( QUAL >= 30 )" > filtered.vcf
 
 
Item 1894: 
g.V().has(“name”, “Tesla”).out(“subsidiaries”).and(
     out(‘osha_violation’),
     out(“toxic_release”).has(“carcinogen”, true)
     ).out(“facility”).has(“point”, geoWithin(Geoshape.circle(74.0060, 40.7128, 50)))
 
 
Item 1902: 
                         (State, Action)
                  (State, Action, Reward, State1)
             (State, Action, Reward, State1, Action1)
 
 
Item 1908: 
"Waiter with experience as shift leader"
 waiter, shift leader, senior, responsible, metre, waiter manager
 
 
Item 1910: 
private consumption + business investment + government spending and investment + (exports — imports)
 
 
Item 1913: 
MyCNNClassifier( (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (fc1): Linear(in_features=25088, out_features=1024, bias=True) (fc2): Linear(in_features=1024, out_features=10, bias=True) )
 MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )
 MyCNNClassifier( (conv_block1): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (conv_block2): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )
 MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )
 MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )
 1 32 32 64
 MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): Sequential( (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() (2): Linear(in_features=1024, out_features=10, bias=True) ) )
 MyCNNClassifier( (encoder): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) (decoder): Sequential( (0): Sequential( (0): Linear(in_features=25088, out_features=1024, bias=True) (1): Sigmoid() ) (1): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) )
 MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) ) )
 torch.Size([4, 16]) torch.Size([4, 32]) [None, None]
 Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() )
 MyCNNClassifier( (encoder): MyEncoder( (conv_blokcs): Sequential( (0): Sequential( (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) (1): Sequential( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.01) ) ) ) (decoder): MyDecoder( (dec_blocks): Sequential( (0): Sequential( (0): Linear(in_features=1024, out_features=512, bias=True) (1): Sigmoid() ) ) (last): Linear(in_features=512, out_features=10, bias=True) ) )
 
 
Item 1914: 
['I', 'ii42', 'vii53', 'I', 'vi6', 'ii42', 'V6', 'i42', ...]
 
 
Item 1920: 
!pip update xgboost
 !pip update — user xgboost
 
 
Item 1921: 
template <typename Architecture_t>
inline void evaluateGradients(typename Architecture_t::Matrix_t &dMetricOne, typename Architecture_t::Matrix_t &dMetricTwo,
                              ELossFunction f, const typename Architecture_t::Matrix_t &MetricOne,
                              const typename Architecture_t::Matrix_t &MetricTwo,
                              const typename Architecture_t::Matrix_t &weights)
{
    switch(f)
    {
    case ELossFunction::kKLDivergence: Architecture_t::KLDivergenceGradients(dMetricOne, dMetricTwo, MetricOne, MetricTwo, weights);
    break;
    } 
}
 template <typename AReal>
void TReference<AReal>::KLDivergenceGradients(TMatrixT<AReal> &dMean, TMatrixT<AReal> &dStandardDeviation,
                                               const TMatrixT<AReal> &Mean, const TMatrixT<AReal> &StandardDeviation, 
                                               const TMatrixT<AReal> &weights)
{
   size_t m,n;
   m = Mean.GetNrows();
   n = Mean.GetNcols();
   AReal norm = 1.0 / m ;
 for(size_t i = 0; i < m; i++){
      for(size_t j = 0; j < n; j++){
         dMean(i,j) = -2.0 * norm * Mean(i,j) * weights(i, 0);
         dStandardDeviation(i,j) = 1.0 - (2.0 * std::exp(2.0 * StandardDeviation(i,j)));
         dStandardDeviation(i,j) *= norm * weights(i, 0);
      }
   }
}
 Layout=Encoder={RESHAPE|1|1|4|FLAT,DENSE|2|SIGMOID}Decoder={DENSE|4|LINEAR}
 
 
Item 1922: 
torch.Tensor(x, y)
 torch.Tensor(5, 3).uniform_(-1, 1)
 print(x.size())
 y = torch.rand(5, 3)
print(x + y)
 result = torch.Tensor(5, 3)
torch.add(x, y, out=result)
 y.add_(x)
 print(x[:, 1])
 y = torch.randn(5, 10, 15)
print(y.size())
print(y.view(-1, 15).size())
 a = torch.ones(5)
b = a.numpy()
 if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    x + y
 x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]
 w = Variable(torch.Tensor([1.0]),  requires_grad=True)
 l = loss(x_val, y_val)
l.backward()
 
 
Item 1923: 
The cropping layer have two functional modes:
    - When it has 1 input blob, it crops the input blob based
      on the 4 parameters [left, right, top, bottom]. 
    - When it has 2 input blobs, it crops the first input blob based
      on the dimension of the second blob with an offset.
 
 
Item 1925: 
with open (‘Location History.json’) as f:
 data = json.load(f)
df = pd.DataFrame(data[‘locations’])
df = df[[‘accuracy’,’timestampMs’,’latitudeE7',’longitudeE7']].copy()
df[‘timestampMs’] = df[‘timestampMs’].astype(float)/1000
df[‘datetime’] = df[‘timestampMs’].apply(lambda t: dt.datetime.fromtimestamp(t))
df[‘latitudeE7’] = df[‘latitudeE7’]/10**7
df[‘longitudeE7’] = df[‘longitudeE7’]/10**7
 distance_client_by_date = df.groupby('date', as_index = False)['distance_client'].min().apply(lambda x: x)
trip_days = distance_client_by_date.loc[distance_client_by_date['distance_client']<0.5].copy()
df_trips=df.loc[df['date'].isin(trip_days['date'])].copy()
 np.where((df_trips['distance_home'] > 0.1) & (df_trips_periods['distance_client'] > 0.2) & (df_trips_periods['period']=='Morning'), 1,0 )
 df_trips['arrived'] = np.where((df_trips_periods['smth_dist_home'] < 0.1) & (df_trips_periods['period']=='Evening'), 1,0)
 df_trips['arrival_time'] = df_trips.loc[df_trips['arrived']==1].groupby('peak')['time'].transform('min')
 
 
Item 1926: 
A subset of original ratings provided for UserId: 9
Rated 5.0 for movie What Lies Beneath (2000)
Rated 3.0 for movie General's Daughter, The (1999)
Rated 2.0 for movie Entrapment (1999)
Rated 3.0 for movie Austin Powers: The Spy Who Shagged Me (1999)
Rated 3.0 for movie Return of the Living Dead, The (1985)
Rated 3.0 for movie Psycho (1960)
Rated 3.0 for movie Stigmata (1999)
Rated 4.0 for movie American Pie (1999)
Rated 2.0 for movie Vertical Limit (2000)
Rated 1.0 for movie Traffic (2000)
Rated 2.0 for movie Deep Blue Sea (1999)
Rated 4.0 for movie Fast and the Furious, The (2001)
Rated 4.0 for movie Cast Away (2000)
Rated 2.0 for movie Scary Movie (2000)
Rated 5.0 for movie Hannibal (2001)
Rated 2.0 for movie Creepshow (1982)
Rated 2.0 for movie Urban Legends: Final Cut (2000)
Rated 5.0 for movie Fight Club (1999)
Rated 5.0 for movie Exorcist, The (1973)
Rated 4.0 for movie There's Something About Mary (1998)
 Top recommendations for UserId: 9
Predicting rating 5.0 for movie Shawshank Redemption, The (1994)
Predicting rating 5.0 for movie Silence of the Lambs, The (1991)
Predicting rating 4.9 for movie Pulp Fiction (1994)
Predicting rating 4.9 for movie Braveheart (1995)
Predicting rating 4.8 for movie Schindler's List (1993)
Predicting rating 4.7 for movie Usual Suspects, The (1995)
Predicting rating 4.6 for movie Seven (a.k.a. Se7en) (1995)
Predicting rating 4.6 for movie Zero Motivation (Efes beyahasei enosh) (2014)
Predicting rating 4.5 for movie Fugitive, The (1993)
Predicting rating 4.5 for movie Terminator 2: Judgment Day (1991)
Predicting rating 4.5 for movie Godfather: Part II, The (1974)
Predicting rating 4.4 for movie Saving Private Ryan (1998)
Predicting rating 4.4 for movie Dances with Wolves (1990)
Predicting rating 4.4 for movie Matrix, The (1999)
Predicting rating 4.4 for movie American Beauty (1999)
Predicting rating 4.4 for movie One Flew Over the Cuckoo's Nest (1975)
Predicting rating 4.4 for movie Goodfellas (1990)
Predicting rating 4.3 for movie Apollo 13 (1995)
Predicting rating 4.3 for movie Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)
Predicting rating 4.3 for movie American History X (1998)
 A subset of original ratings provided for UserId: 138494
Rated 4.5 for movie Like Water for Chocolate (Como agua para chocolate) (1992)
Rated 4.0 for movie Mr. Holland's Opus (1995)
Rated 4.0 for movie Sense and Sensibility (1995)
Rated 2.0 for movie Fargo (1996)
Rated 4.0 for movie Pi (1998)
Rated 4.5 for movie Garden State (2004)
Rated 4.0 for movie What's Eating Gilbert Grape (1993)
Rated 1.0 for movie Lemony Snicket's A Series of Unfortunate Events (2004)
Rated 4.0 for movie Apollo 13 (1995)
Rated 4.5 for movie Buffalo '66 (a.k.a. Buffalo 66) (1998)
Rated 4.5 for movie Mission, The (1986)
Rated 4.5 for movie River Runs Through It, A (1992)
Rated 5.0 for movie Spanish Apartment, The (L'auberge espagnole) (2002)
Rated 5.0 for movie Willy Wonka & the Chocolate Factory (1971)
Rated 3.0 for movie Sideways (2004)
Rated 5.0 for movie Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)
Rated 4.0 for movie Bend It Like Beckham (2002)
Rated 1.0 for movie Dumb & Dumber (Dumb and Dumber) (1994)
Rated 4.0 for movie Dead Poets Society (1989)
Rated 4.0 for movie O Brother, Where Art Thou? (2000)
 A subset of original ratings provided for UserId: 138495
Rated 1.0 for movie Indiana Jones and the Kingdom of the Crystal Skull (2008)
Rated 4.5 for movie Mephisto (1981)
Rated 5.0 for movie Life Is Beautiful (La Vita è bella) (1997)
Rated 3.5 for movie Out of Africa (1985)
Rated 4.0 for movie Sense and Sensibility (1995)
Rated 4.5 for movie Virgin Suicides, The (1999)
Rated 1.0 for movie Dumb & Dumber (Dumb and Dumber) (1994)
Rated 4.0 for movie Three Colors: Blue (Trois couleurs: Bleu) (1993)
Rated 1.0 for movie Star Trek: Insurrection (1998)
Rated 4.5 for movie Postman, The (Postino, Il) (1994)
Rated 4.0 for movie Steel Magnolias (1989)
Rated 4.0 for movie Gattaca (1997)
Rated 2.5 for movie Dangerous Minds (1995)
Rated 3.5 for movie Seven (a.k.a. Se7en) (1995)
Rated 0.5 for movie Terminator 2: Judgment Day (1991)
Rated 2.5 for movie Driving Miss Daisy (1989)
Rated 0.5 for movie Home Alone (1990)
Rated 4.0 for movie Love Actually (2003)
Rated 4.0 for movie Leaving Las Vegas (1995)
Rated 5.0 for movie Cinema Paradiso (Nuovo cinema Paradiso) (1989)
 Top combinded recommendations for UserIds: 138494 and 138495
Predicting rating 4.5 for movie Zero Motivation (Efes beyahasei enosh) (2014)
Predicting rating 4.5 for movie Shawshank Redemption, The (1994)
Predicting rating 4.5 for movie Fight Club (1999)
Predicting rating 4.4 for movie Forrest Gump (1994)
Predicting rating 4.3 for movie Usual Suspects, The (1995)
Predicting rating 4.3 for movie Matrix, The (1999)
Predicting rating 4.3 for movie Schindler's List (1993)
Predicting rating 4.3 for movie American Beauty (1999)
Predicting rating 4.3 for movie Memento (2000)
Predicting rating 4.3 for movie Death on the Staircase (Soupçons) (2004)
Predicting rating 4.3 for movie City of God (Cidade de Deus) (2002)
Predicting rating 4.3 for movie Star Wars: Episode V - The Empire Strikes Back (1980)
Predicting rating 4.3 for movie Seven Samurai (Shichinin no samurai) (1954)
Predicting rating 4.3 for movie Spirited Away (Sen to Chihiro no kamikakushi) (2001)
Predicting rating 4.3 for movie Dark Knight, The (2008)
Predicting rating 4.3 for movie Princess Bride, The (1987)
Predicting rating 4.3 for movie O Auto da Compadecida (Dog's Will, A) (2000)
Predicting rating 4.3 for movie Band of Brothers (2001)
Predicting rating 4.3 for movie Rear Window (1954)
Predicting rating 4.3 for movie One Flew Over the Cuckoo's Nest (1975)
 
 
Item 1927: 
fct_relevel(mother_race_name, "White")
 
 
Item 1934: 
def model_optimize(w,b,X,Y):
    #
    m = X.shape[0]
    #
    final_result = np.dot(w, X.T) + b
    cost = (1/m)*np.sum((Y.T - final_result) ** 2)
    #
    dw = (-2/m)*np.sum((np.dot(X.T,(Y.T - final_result).T)))
    db = (-2/m)*np.sum(((Y.T - final_result)))
    #
    grads = {"dw": dw, "db": db}
    
    return grads, cost
 def gradientUpdate(w,b,X, Y, learning_rate, no_iterations):
    costs = []
    for i in range(no_iterations):
        #
        grads, cost = model_optimize(w,b, X,Y)
        #
        dw = grads["dw"]
        db = grads["db"]
        #Weight Update
        w = w - (learning_rate*dw)
        b = b - (learning_rate*db)
        #
        
        costs.append(cost)
    #
    
    coeff = {"w": w, "b": b}
    gradient = {"dw": dw, "db": db}
    
    return coeff, gradient, costs
 def stochasticUpdate(w,b,X, Y, learning_rate, no_iterations):
    #
    n_points = X.shape[0]
    #
    for i in range(no_iterations):
        for i in range(n_points):
            index = np.random.randint(n_points)
            x_pt = X[index:index+1]
            y_pt = Y[index:index+1]
            grads, cost = model_optimize(w,b, x_pt,y_pt)
            #
            dw = grads["dw"]
            db = grads["db"]
            #Weight Update
            w = w - (learning_rate*dw)
            b = b - (learning_rate*db)
        #
            costs.append(cost)
    #
    
    coeff = {"w": w, "b": b}
    gradient = {"dw": dw, "db": db}
    
    return coeff, gradient, costs
 def miniBatchUpdate(w,b,X, Y, learning_rate, no_iterations):
    #
    n_points = X.shape[0]
    #
    for i in range(no_iterations):
        X, y = shuffle(x_train, y_train)
        x_random = X[:40]
        y_random = y[:40]
        
        grads, cost = model_optimize(w,b, x_random,y_random)
        #
        dw = grads["dw"]
        db = grads["db"]
        #Weight Update
        w = w - (learning_rate*dw)
        b = b - (learning_rate*db)
        #
        costs.append(cost)
    #
    
    coeff = {"w": w, "b": b}
    gradient = {"dw": dw, "db": db}
    
    return coeff, gradient, costs
 
 
Item 1935: 
[{‘label’: ‘Name’,
 ‘confidence’: 0.57865334,
 ‘topleft’: {‘x’: 8, ‘y’: 126},
 ‘bottomright’: {‘x’: 150, ‘y’: 170}},
 {‘label’: ‘DOB’,
 ‘confidence’: 0.86376363,
 ‘topleft’: {‘x’: 0, ‘y’: 271},
 ‘bottomright’: {‘x’: 217, ‘y’: 308}},
 {‘label’: ‘ID number’,
 ‘confidence’: 0.42626178,
 ‘topleft’: {‘x’: 26, ‘y’: 172},
 ‘bottomright’: {‘x’: 152, ‘y’: 213}}]
 
 
Item 1940: 
hotel = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]
 
 
Item 1941: 
libraryDependencies += "com.google.cloud" % "google-cloud-language" % "0.24.0-beta"
 "edu.stanford.nlp" % "stanford-corenlp" % "3.8.0",
"edu.stanford.nlp" % "stanford-corenlp" % "3.8.0" classifier "models",
"edu.stanford.nlp" % "stanford-parser" % "3.8.0"
 "org.slf4j" % "slf4j-simple" % "1.6.4",
 
 
Item 1942: 
> library(randomForest)
Error in library(randomForest) : 
  there is no package called ‘randomForest’

> b <- require(randomForest)
Loading required package: randomForest
Warning message:
In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :
  there is no package called ‘randomForest’
> print(b)
[1] FALSE
 > v <- c(1,2,3,NA)
> v
[1]  1  2  3 NA
 > w <- c(1,2,3,NULL)
> w
[1] 1 2 3
 > w <- list(1,2,3,NULL)
> w
[[1]]
[1] 1

[[2]]
[1] 2

[[3]]
[1] 3

[[4]]
NULL
 > w <- c(1,2,3,4,5)
> w[3.5]
[1] 3
> w[-4.9]
[1] 1 2 3 5
 > w <- c(1,2,3,4,5)
> x <- c(T,F)
> x
[1]  TRUE FALSE
> w[x]
[1] 1 3 5
 > a <- list(1,2,3,c(4,5,6))
# a has 1,2,3 and c(4,5,6) as its elements
> a[1]
[[1]]
[1] 1
#Element one as list:Type preservation
 > a[4]
[[1]]
[1] 4 5 6
#Element 4 as list:Type preservation
 > a[[4]]
[1] 4 5 6
#Element 4 as vector, type simplification
 > a[[4]][1]
[1] 4
#First element of 4th element of a, type simplification
 > a[4][1]
[[1]]
[1] 4 5 6
#[] Outputs the whole vector as we did not simplify the data type to vector from list.
 > a <- matrix(c(1:9),ncol=3,nrow=3)
> a
     [,1] [,2] [,3]
[1,]    1    4    7
[2,]    2    5    8
[3,]    3    6    9
> a[,1]
[1] 1 2 3
> class(a[,1])
[1] "integer"
#When you extract a single column, it is converted to a vector
 > a[,1,drop=F]
     [,1]
[1,]    1
[2,]    2
[3,]    3
> class(a[,1,drop=F])
[1] "matrix"
# drop=F helps us retain the matrix form
 > a <- c(1:5)
> a
[1] 1 2 3 4 5
# a has 5 elements as shown above
 > a[7]
[1] NA
#When we try to access 7th element, we get NA instead of an error
 > b <- list(c(1,2,3,4))
> b
[[1]]
[1] 1 2 3 4
#b has 1 element which is a vector of 4 elements
> b[2]
[[1]]
NULL
#When we try to access 2nd element, we get NULL instead of an error
 > a <- c(5:10)
> a
[1]  5  6  7  8  9 10
> if(a%%2==0){x <- "Even"} else{x <- "Odd"}
Warning message:
In if (a%%2 == 0) { :
  the condition has length > 1 and only the first element will be used
> x
[1] "Odd"
 > y <- ifelse(a%%2,"Even","Odd")
> y
[1] "Even" "Odd"  "Even" "Odd"  "Even" "Odd"
 > f <- function(x,y){
           print(x)}
> f(2)
[1] 2
#Function call works with one argument as y is not used.
 > f(2,3)
[1] 2
#Calling with both arguments
 > f()
Error in print(x) : argument "x" is missing, with no default
#Since x is called inside function and is missing, we get error
 #Explicitly checking for both x and y using force()
> f <- function(x,y){force(x,y); print(x)}
> f(2,3)
[1] 2
> f(2)
Error in force(y) : argument "y" is missing, with no default
 library(chron)
library(tseries)
    ‘tseries’ version: 0.10-45

    ‘tseries’ is a package for time series analysis and computational finance.

    See ‘library(help="tseries")’ for details.


Attaching package: ‘tseries’

The following object is masked from ‘package:chron’:

    is.weekend
 is.weekend(x)
 chron::is.weekend(x)
 > f <- function(a1,b1,a2){
    print(paste("a1:",a1,"b1:",b1,"c1:",a2))}
 > f(a1=2,b1=3,a2=5)
[1] "a1: 2 b1: 3 c1: 5"
#Example of Exact match, each argument has same name
 
> f(b1=3,5,2)
[1] "a1: 5 b1: 3 c1: 2"
#Example of exact match and positional match. Since b1 is matched, 5 and 2 are assigned based on position
 > f(3,b=5,2)
[1] "a1: 3 b1: 5 c1: 2"
#Partial name match, b matches to b1. Rest matched based on position
 
> f(3,5,2)
[1] "a1: 3 b1: 5 c1: 2"
> f(3,5,a=2)
Error in f(3, 5, a = 2) : argument 3 matches multiple formal arguments
#Since a matches to both a1 and a2 we get error as R does not know where the value needs to be assigned.
 
 
Item 1944: 
There are 36121 characters and 27 unique characters.

Epoch 0
=======
Sampled name: Nijqikkgzst
Smoothed loss: 23.0709
 Epoch 10
=======
Sampled name: Milton
Smoothed loss: 14.7446
 Epoch 30
=======
Sampled name: Dangelyn
Smoothed loss: 13.8179
 Epoch 70
=======
Sampled name: Lacira
Smoothed loss: 13.3782
 Epoch 99
=======
Sampled name: Cathranda
Smoothed loss: 13.3380
 
 
Item 1948: 
song_grp<-lyrics %>%group_by(artist)%>%summarise(song_cnt=unique(length(song)))%>%arrange(desc(song_cnt))
song_grp[1:10,] %>%
  ungroup(artist, song_cnt) %>%
  mutate(song_cnt = color_bar("lightblue")(song_cnt)) %>%
  mutate(artist = color_tile("green","green")(artist)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Artist With Highest song Count") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                full_width = FALSE)
 lyrics_filtered <- lyrics %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  distinct() %>%
  filter(nchar(word) > 3)
head(lyrics_filtered)
 ##   artist                  song      word
## 1   ABBA Ahe's My Kind Of Girl wonderful
## 2   ABBA Ahe's My Kind Of Girl     means
## 3   ABBA Ahe's My Kind Of Girl   special
## 4   ABBA Ahe's My Kind Of Girl    smiles
## 5   ABBA Ahe's My Kind Of Girl     lucky
## 6   ABBA Ahe's My Kind Of Girl    fellow
 full_word_count <- lyrics %>%
  unnest_tokens(word, text) %>%
  group_by(song) %>%
  summarise(num_words = n()) %>%
  arrange(desc(num_words)) 
 full_word_count[1:10,] %>%
  ungroup(num_words, song) %>%
  mutate(num_words = color_bar("lightblue")(num_words)) %>%
  mutate(song = color_tile("green","green")(song)) %>%
  kable("html", escape = FALSE, align = "c", caption = "Songs With Highest Word Count") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                full_width = FALSE)
 lyrics_filtered %>% 
  filter(word == "angel") %>%
  select(word, song, artist) %>%
  arrange() %>%
  top_n(10,song) %>%
  mutate(song = color_tile("lightblue","lightblue")(song)) %>%
  mutate(word = color_tile("green","green")(word)) %>%
  kable("html", escape = FALSE, align = "c", caption = "angel word per song and artist") %>%
  kable_styling(bootstrap_options = 
                  c("striped", "condensed", "bordered"), 
                full_width = FALSE)
 lyrics_filtered %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(word, n), fill = "blue") +
  theme(legend.position = "none", 
        plot.title = element_text(hjust = 0.5),
        panel.grid.major = element_blank()) +
  xlab("") + 
  ylab("Song Count") +
  ggtitle("Most Frequently Used Words in Lyrics") +
  coord_flip()
 set.seed(12345)
word_corr %>%
  filter(correlation > .75) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "kk") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "blue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
 set.seed(1234) 
row_indexes <- sample(1:nrow(lyrics), 1600, replace = F) 
texts_subsample <-slice(lyrics, row_indexes)
 viewsCorpus <- Corpus(VectorSource(texts_subsample$text)) 
viewsDTM <- DocumentTermMatrix(viewsCorpus)
viewsDTM_tidy <- tidy(viewsDTM)
viewsDTM_tidy_cleaned <- viewsDTM_tidy %>% 
  anti_join(stop_words, by = c("term" = "word")) %>%
  filter(nchar(term) > 3)
top_terms_by_topic_LDA <- function(input_text, 
                                   plot = T, 
                                   number_of_topics = 5) 
{    
  Corpus <- Corpus(VectorSource(input_text))
  DTM <- DocumentTermMatrix(Corpus) 
  unique_indexes <- unique(DTM$i) 
  DTM <- DTM[unique_indexes,] 
  lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
  topics <- tidy(lda, matrix = "beta") 
  top_terms <- topics  %>% 
    group_by(topic) %>% 
    top_n(10, beta) %>%
    ungroup() %>% 
    arrange(topic, -beta) 
  if(plot == T){
    top_terms %>% 
      mutate(term = reorder(term, beta)) %>% 
      ggplot(aes(term, beta, fill = factor(topic))) + 
      geom_col(show.legend = FALSE) + 
      facet_wrap(~ topic, scales = "free") + 
      labs(x = NULL, y = "Beta") + 
      coord_flip() 
  }else{ 
    return(top_terms)
  }
}
top_terms_by_topic_LDA(viewsDTM_tidy_cleaned$term, number_of_topics = 5)
 viewsDTM_tidy_cleaned <- viewsDTM_tidy_cleaned %>% 
  mutate(stem = wordStem(term))
top_terms_by_topic_LDA(viewsDTM_tidy_cleaned$stem, number_of_topics=5)
 
 
Item 1951: 
ValueError: Keras layer '<class 'keras_contrib.layers.normalization.InstanceNormalization'>' not supported.
 
 
Item 1952: 
+----------+--------------+--------------+
|          | Positive     | Negative    | 
+----------+--------------+--------------+
| ...      | ...          | ...          |
| love     | large number | small number |
| NOT_love | small number | large number | 
| ...      | ...          | ...          |
+----------+--------------+--------------+
 
 
Item 1955: 
filter(FL_DATE > as.Date("2016-09-15"))
 filter(FL_DATE >= as.Date("2016-09-15") & FL_DATE <= as.Date("2016-09-20"))
 filter(between(FL_DATE, as.Date("2016-09-15"), as.Date("2016-09-20")))
 mutate(FL_DATE_Plus15 = FL_DATE + days(15))
 filter(between(FL_DATE, as.Date("2016-09-15"), as.Date("2016-09-20")) & between(FL_DATE_Plus15, as.Date("2016-09-15"), as.Date("2016-09-20")))
 filter_at(vars(starts_with("FL_DATE")), all_vars(between(., as.Date("2016-09-03"), as.Date("2016-09-20"))))
 filter_if(is.Date, all_vars(between(., as.Date("2016-09-15"), as.Date("2016-09-20"))))
 filter_if(is.Date, all_vars(between(., as.Date("2016-09-03"), as.Date("2016-09-20"))))
 filter_if(is.Date, any_vars(between(., as.Date("2016-09-03"), as.Date("2016-09-20"))))
 filter_if(is.Date, all_vars(between(., as.Date("2016-09-03"), as.Date("2016-09-20"))))
 filter_if(is.Date, any_vars(between(., as.Date("2016-09-03"), as.Date("2016-09-20"))))
 
 
Item 1956: 
train_x, train_y = preprocessing_as_np(train_data)
test_x, test_y = preprocessing_as_np(test_data)

np.savez(os.path.join(dataset_path, "train"), 
    x=train_x,
    y=train_y)

np.savez(os.path.join(dataset_path, "test"), 
    x=test_x,
    y=test_y)
 with np.load(os.path.join(dataset_path, "train.npz")) as data:
    train_x=data['x']
    train_y=data['y']

with np.load(os.path.join(dataset_path, "test.npz")) as data:
    train_x=data['x']
    train_y=data['y']
 x_ph = tf.placeholder(tf.int32, [None]+
        list(train_x.shape[1:]), name="x")
y_ph = tf.placeholder(tf.int32, [None]+
        list(train_y.shape[1:]), name="y")

train_dataset = tf.data.Dataset.from_tensor_slices 
        ((x_ph, y_ph)).shuffle(buffer_size=10000).batch(BATCH_SIZE)
valid_dataset = tf.data.Dataset.from_tensor_slices
        ((x_ph, y_ph)).batch(BATCH_SIZE)
 iterator = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)
next_elements = iterator.get_next()

training_init_op = iterator.make_initializer(train_dataset, name="training_init_op")
validation_init_op = iterator.make_initializer(valid_dataset, name="validation_init_op")

x, y = next_elements
 class Model(object):
    def __init__(self, x, y,
                learning_rate=1e-4, optimizer=tf.train.AdamOptimizer, run_dir="./run"):
        hidden_layer_0 = tf.layers.dense(x, 1024, activation=tf.nn.relu)
        hidden_layer_1 = tf.layers.dense(hidden_layer_0, 784, activation=tf.nn.relu)
        hidden_layer_2 = tf.layers.dense(hidden_layer_1, 512, activation=tf.nn.relu)
        logits = tf.layers.dense(hidden_layer_2, 10, activation=tf.nn.softmax)
        self._loss = tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), logits)
        self._global_step = tf.Variable(0, trainable=False, name="global_step")
        
        self._train_op = tf.contrib.layers.optimize_loss(loss=self._loss, 
                                                    optimizer=optimizer, 
                                                    global_step=self._global_step, 
                                                    learning_rate=learning_rate, 
                                                    name="train_op",
                                                    summaries=['loss'])
        
        self._summaries = tf.summary.merge_all()
        
        if not os.path.exists(run_dir):
            os.mkdir(run_dir)
        if not os.path.exists(os.path.join(run_dir, "checkpoints")):
            os.mkdir(os.path.join(run_dir, "checkpoints"))
        self._run_dir = run_dir
        self._saver = tf.train.Saver(max_to_keep=1)
 
 
Item 1962: 
[1.,1.,5.],
 
[2.,2.,5.],
 [3.,3.,1.]
 [0.,0.,1.]
 reg_model = linear_model.LinearRegression()
 reg_model.fit([[1.,1.,5.], [2.,2.,5.], [3.,3.,1.]], [0.,0.,1.])
 pickle.dump(reg_model, open('some_model.pkl', 'wb'))
 @app.route('/prediction/api/v1.0/some_prediction', methods=['GET'])
def get_prediction():
 feature1 = float(request.args.get('f1'))
feature2 = float(request.args.get('f2'))
feature3 = float(request.args.get('f3'))
 json(https://6ee957cb.ngrok.io/prediction/api/v1.0/some_prediction?f1=4&f2=4&f3=4).result
 pragma solidity 0.4.20;
import "github.com/oraclize/ethereum-api/oraclizeAPI.sol";
 //This contract was derived from the Orcalize documentation
 contract ExampleSklearnContract is usingOraclize {
     string public inference_result;
    event LogConstructorInitiated(string nextStep);
    event LogInferenceUpdated(string price);
    event LogNewOraclizeQuery(string description); 
 function ExampleSklearnContract () payable{
        
   LogConstructorInitiated("Contract created. Call updateInference() to get a result");
    }

 function __callback(bytes32 myid, string result, bytes proof) {
           require(msg.sender == oraclize_cbAddress()); 
           inference_result = result;
           LogInferenceUpdated(inference_result);
           
    }
 function updateInference() payable {
        
        if (oraclize_getPrice("URL") > this.balance) { 
            LogNewOraclizeQuery("Oraclize query was NOT sent, please add some ETH to cover for the query fee");
        } 
        else {
            LogNewOraclizeQuery("Oraclize query was sent, standing by for the answer.."); 
            oraclize_query("URL", "json(http://<your_url_here>/prediction/api/v1.0/some_prediction?f1=1&f2=1&f3=1).result");
        }
    }
 
 
Item 1963: 
opencv_createsamples -img [image_name.jpg] -num 100 -bg negatives.dat -vec samples.vec -maxxangle 0.6 -maxyangle 0 -maxzangle 0.3 -maxidev 100 -bgcolor 0 -bgthresh 0 -w 24 -h 24
 opencv_haartraining -data [classifier_dir_object_name] -vec samples.vec -bg negatives.dat -nstages 10 -precalcValBufSize 1024 -precalcIdxBufSize 1024 -minhitrate 0.995 -maxfalsealarm 0.5 -npos 100 -nneg 100 -w 24 -h 24 -mode ALL
 [filename] [# of objects] [[x y width height] [… 2nd object] …]
 picture001.jpg 1 140 100 45 45 
picture002.jpg 2 100 200 50 50 50 30 25 25
 git clone - recursive https://github.com/tensorflow/tensorflow
git clone - recursive https://github.com/tensorflow/model
 find . -name “*.jpg” | xargs mogrify -resize 15%
 python3 xml_to_csv.py train
 python3 xml_to_csv.py test
 python3 generate_tfrecord.py -t train — labels_path=labels.txt — csv_input=train_labels.csv — output_path=train.record
 item {
    id: 1
    name: ‘kotlin’
 },
 queue_capacity: 100
 min_after_dequeue: 50
 cd tensorflow/models/reasearch
 export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim (we have to set this path each time we open a new terminal)
 python3 object_detection/train.py — logtostderr — pipeline_config_path=product.config –train_dir=~/training
 python3 object_detection/export_inference_graph.py — input_type image_tensor — pipeline_config_path=products.config — trained_checkpoint_prefix=model.ckpt-10000 — output_directory=~/products/out
 cp /products/out/frozen_inference_graph.pb ~/AndroidStudioProjects/KotlinDetector/assets/frozen_inference_graph.pb
 
 
Item 1965: 
def d_tanh(x):
   return 1 - np.tanh(x) ** 2
 def d_tf_tanh(x):
    return tf.subtract(tf.constant(1.0),tf.square(tf.tanh(x)))
 
 
Item 1967: 
station_id: 3699, longitude -73.98918, latitude 40.763603, bikes: 1, capacity: 30, ratio: 3.33%
station_id: 3700, longitude -73.95334, latitude 40.779408, bikes: 3, capacity: 39, ratio: 7.69%
station_id: 3704, longitude -73.941765, latitude 40.74375, bikes: 1, capacity: 19, ratio: 5.26%
 station_id: [472@1535061600000/1535065200000], turnover: 72.22%
station_id: [473@1535061600000/1535065200000], turnover: 81.40%
station_id: [474@1535061600000/1535065200000], turnover: 5.13%
station_id: [476@1535061600000/1535065200000], turnover: 8.47%
station_id: [477@1535061600000/1535065200000], turnover: 55.32%
 
 
Item 1970: 
+--------+---------+----------+
|        | Positive| Negative |
+--------+---------+----------+
| I      | 1       | 1        |
| enjoyed| 1       | 0        |
| like   | 2       | 0        |
| hate   | 0       | 1        |
| was    | 0       | 1        |
| that   | 1       | 1        |
| movie  | 1       | 1        |
| ...    | .       | .        |
+--------+---------+----------+
 
 
Item 1971: 
'Start Time', 'End Time', 'Minutes Asleep', 'Minutes Awake',
       'Number of Awakenings', 'Time in Bed', 'Minutes REM Sleep',
       'Minutes Light Sleep', 'Minutes Deep Sleep'
 ['Date', 'Calories Burned', 'Steps', 'Distance', 'Floors',
 'Minutes Sedentary', 'Minutes Lightly Active', 'Minutes Fairly Active', 'Minutes Very Active', 'Activity Calories']
 
 
Item 1973: 
base_search | table latitude, longitude [ description| title | (...)
 
 
Item 1979: 
{'abutere': 0, 'ad': 1, 'audacia': 2, 'catilina': 3, 'effrenata': 4, 'eludet': 5, 'etiam': 6, 'finem': 7, 'furor': 8, 'iactabit': 9, 'iste': 10, 'nos': 11, 'nostra': 12, 'patientia': 13, 'quamdiu': 14, 'quem': 15, 'quo': 16, 'sese': 17, 'tandem': 18, 'tuus': 19, 'usque': 20}
 21
 [[0.2508639 0.6186842 0.04858994 0.5210395 0.46944225 0.93606484 0.31613624 0.37244523 0.8245921 0.7652482 0.05056596 0.82652867 0.637517 0.5321804 0.84733844 0.90017974 0.41220248 0.659974 0.7645968 0.5598999 0.40155995 0.06464231 0.8390876 0.139521 0.23042619 0.04655147 0.32764542 0.80585504 0.01360166 0.9290798 0.25056374 0.9695363 0.5877855 0.9006752 0.49083364 0.5052364 0.56793296 0.50847435 0.89294696 0.4142543 0.70229757 0.56847537 0.8818027 0.8013681 0.12879837 0.75869775 0.40932536 0.04723692 0.61465013 0.97508 ] [0.846097 0.8248534 0.5730028 0.32177114 0.37013817 0.71865106 0.2488327 0.88490605 0.6985643 0.8720304 0.4982674 0.75656927 0.34931898 0.20750809 0.16621685 0.38027227 0.23989546 0.43870246 0.49193907 0.9563453 0.92043686 0.9371239 0.3556149 0.08938527 0.28407085 0.29870117 0.44801772 0.21189022 0.48243213 0.946913 0.40073442 0.71190274 0.59758437 0.70785224 0.09750676 0.27404332 0.4761486 0.64353764 0.2631061 0.19715095 0.6992599 0.72724617 0.27448702 0.3829409 0.15989089 0.09099603 0.43427885 0.78103256 0.30195284 0.888047 ]]
 [[0.9134803 0.36847484 0.51816785 0.19543898 0.07610226 0.8685185 0.7445053 0.5340642 0.5453609 0.72966635 0.06846464 0.19424069 0.2804587 0.77481234 0.7343868 0.16347027 0.56002617 0.76706755 0.16558647 0.6719606 0.05563295 0.22389805 0.47797906 0.98075724 0.47506428 0.7846818 0.65209556 0.89036727 0.14960134 0.8801923 0.23688185 0.70695686 0.59664845 0.6206044 0.69665396 0.60709286 0.42249918 0.7317171 0.03822994 0.37915635 0.60433483 0.4168439 0.5516542 0.84362316 0.27857065 0.33540523 0.8601098 0.47720838 0.9827635 0.09320438] [0.27832222 0.8259096 0.5726856 0.96932447 0.21936393 0.26346993 0.38576245 0.60339177 0.03083277 0.665465 0.9077859 0.6219367 0.5185654 0.5444832 0.16380131 0.6688931 0.82876015 0.9705752 0.40097427 0.28450823 0.9425919 0.50802815 0.02394092 0.24661314 0.45858765 0.7080616 0.8434526 0.46829247 0.0329994 0.10844195 0.6812979 0.3505745 0.67980576 0.71404254 0.8574227 0.40939808 0.8668809 0.58524954 0.52820635 0.31366992 0.05352783 0.8875419 0.04600751 0.27407455 0.6398467 0.74402344 0.9710648 0.5717342 0.78711486 0.9209585 ]]
 [[ 0.11656719 -0.21488819 0.04018757 -0.18151578 -0.12417153 -0.00693065 0.27286723 0.00712651 -0.05931629 -0.20677638 0.14741448 -0.24938995 -0.21667814 0.09805503 0.2690411 0.20826831 0.19904876 0.08541816 0.20128882 0.15323257 -0.0386056 0.03025511 0.11573204 0.2161583 -0.02596462 -0.15845075 -0.26478297 -0.13366173 0.27797714 -0.08158416 -0.25292248 -0.16360758 -0.1846793 0.2444193 0.13292032 0.15807101 0.24052963 -0.0346185 0.02243239 0.2350963 -0.0260604 0.12481615 -0.1984439 0.20924723 -0.00630271 -0.26579106 0.04491454 0.10764262 0.170991 0.21768841] [-0.09142873 -0.25572282 0.2879894 -0.2416141 0.0688259 -0.06163606 0.2885336 -0.19590749 -0.04164416 0.28198788 0.18056017 -0.03718823 -0.09900685 0.14315534 -0.25260317 -0.00199199 -0.08959872 0.23495004 -0.18945126 -0.16665417 0.18416747 0.05468053 -0.23341912 0.02287021 0.27363363 0.07707322 -0.02453846 0.08111072 0.12435484 0.12095574 0.2879583 0.12930956 0.09152126 -0.2874632 -0.26153982 -0.10861655 -0.01751739 0.20820773 0.22776482 -0.17411226 -0.10380474 -0.14888035 0.01492503 0.24255303 -0.10528904 0.19635591 -0.22860856 0.2117649 -0.08887576 0.16184562]]
 [[0.19501674 0.954353 0.30957866 0.65923584 0.28241146 0.80623126 0.46677458 0.5877205 0.25624812 0.03041542 0.24185908 0.8056189 0.61915445 0.04368758 0.16852558 0.24910712 0.66250837 0.01929498 0.82387006 0.8489572 0.3970251 0.8156922 0.5550339 0.39991164 0.64657426 0.1980362 0.35962176 0.89992213 0.99705064 0.7636745 0.5627477 0.09286976 0.12509382 0.9644747 0.3412783 0.3238287 0.08844066 0.06885219 0.2377944 0.04519224 0.6535493 0.39360797 0.69070065 0.44310153 0.58286166 0.32064807 0.9180571 0.47852004 0.6686201 0.44279683] [0.0843749 0.77335155 0.14301467 0.23359239 0.77076364 0.3579203 0.95124376 0.03154683 0.11837351 0.622192 0.44682932 0.4268434 0.21531689 0.5922301 0.12666893 0.72407126 0.7601874 0.9128723 0.07651949 0.7025702 0.9072187 0.5582067 0.14753926 0.6066953 0.7564144 0.2200278 0.1666696 0.63408077 0.57941747 0.9417999 0.6540415 0.01334655 0.8736309 0.4756062 0.66136014 0.12366748 0.8578756 0.71376395 0.624522 0.22263229 0.35624254 0.00424874 0.1616261 0.43327594 0.83355534 0.51896024 0.53433514 0.47303247 0.7777432 0.4082179 ]]
 
 
Item 1981: 
brother - boy + girl = sister
 queen - woman + man = king
 biking - today + yesterday = biked
 
 
Item 1983: 
TensorFlow: 1.10.0
Pandas: 0.23.1
Numpy: 1.15.1
Matplotlib: 2.2.3
 > conda update dask
 docker run -it --rm \         
       -v "$(pwd)/models:/models/"  \         
       -p 9000:9000 \         
       sleepsonthefloor/graphpipe-tf:cpu \         
       --model=/models/rnn_ts_model.pb \         
       --listen=0.0.0.0:9000
 fn init --runtime go --trigger http gpfn
 > go get github.com/oracle/graphpipe-go
> go get github.com/fnproject/fdk-go
> go get github.com/fnproject/cloudevent
 [[constraint]]
  branch = "master"
  name = "github.com/oracle/graphpipe-go"
 > fn deploy \ 
     --app gpfnapp \ 
     --registry phx.ocir.io/oicpaas1/ralmuell/fn
 > fn list triggers gpfnapp
 FUNCTION        NAME            TYPE    SOURCE          ENDPOINT
gpfn            gpfn-trigger    http    /gpfn-trigger   http://localhost:8080/t/gpfnapp/gpfn-trigger
 > fn config function  \
>    gpfnapp gpfn GP_SERVER_URL http://localhost:9000
 > fn list config function gpfnapp gpfn
 KEY           VALUE
GP_SERVER_URL http://localhost:9000
 cat ce.json | fn invoke gpfnapp gpfn
 
 
Item 1987: 
> base_dir
  > classname 1
    > image 1
    > image 2
    > ...
  > classname 2
    > image 1
    > image 2
    > ...
 encoded_image_string = cv2.imencode(‘.jpg’, image)[1].tostring()
 label = tf.compat.as_bytes(example['classname'])
image = tf.compat.as_bytes(encoded_image_string)
 feature = {
  'train/label':_bytes_feature(label),
  'train/image':_bytes_feature(image)
}

tf_example = tf.train.Example(
  features = tf.train.Features(feature=feature)
)
 writer.write(tf_example.SerializeToString())
 features = tf.parse_single_example(
  serialized_example,
  features={
    'train/label': tf.FixedLenFeature(tf.string),
    'train/image': tf.FixedLenFeature(tf.string)
  }
)
 cv2.imdecode(np.fromstring(img_data, dtype=np.uint8), -1
 
 
Item 1999: 
soup = BeautifulSoup(req.content, ‘html.parser’)
paragraphs = soup.find_all('p')
 .
.
.
 if (j.isupper() and len(j)>1 and ‘:’ in j):
     if j in chars_dict:
         chars_dict[j]=chars_dict[j]+1
         chars_dict[‘TOTAL’]=chars_dict[‘TOTAL’]+1
     else:
         chars_dict[j]=1
         chars_dict[‘TOTAL’]=chars_dict[‘TOTAL’]+1
 characters =[“JERRY”,”GEORGE”,”ELAINE”,”SUSAN”,”KRAMER”,”JACKIE”,”FRANK”,”RAVA”,”NEWMAN”,”TOTAL”]
 df = df[df.TOTAL > 100].sort_values(by=”episode_num”)
 
 
Item 2000: 
complete(Date = seq.Date(<start_date>, <end_date>, by=<date_unit>))
 complete(Date = seq.Date(min(Date), max(Date), by="day"))
 discount_data_df %>%
  mutate(Date = as.Date(Date)) %>%
  complete(Date = seq.Date(min(Date), max(Date), by="day"))
 complete(Date = seq.Date(min(Date), max(Date), by="day"), Product)
 fill(`Discount Rate`)
 discount_data_df %>%
  mutate(Date = as.Date(Date)) %>%
  complete(Date = seq.Date(min(Date), max(Date), by="day")) %>%
  fill(`Discount Rate`)
 discount_data_df %>%
  mutate(Date = as.Date(Date)) %>%
  complete(Date = seq.Date(min(Date), max(Date), by="day")) %>%
  group_by(Product) %>%
  fill(`Discount Rate`)
 
 
Item 2001: 
Yahia Elgamal, yahia.elgamal@booking.com, 
Data Scientist at Booking.com
 Melanie Mueller, melanie.mueller@booking.com,
Data Scientist at Booking.com
 Karlijn Zaanen, karlijn.zaanen@booking.com,
Data Scientist at Booking.com
 
 
Item 2002: 
git clone https://github.com/Inventrom/bolt-vibration-anomaly.git
 vibration_bolt.ino
 
 
Item 2009: 
img00041.png 1
img00042.png 3
img00048.png 7
img00039.png 2
img00046.png 1
...
...
...
 
 
Item 2011: 
Output:
[-6 -5 -4 -3 -2 -1  0  1  2  3  4  5  6]
[41 30 21 14  9  6  5  6  9 14 21 30 41]
 Output:
2*x
 
 
Item 2015: 
M's core problem: Facebook put no bounds on what M could be asked to do. Alexa has proven adept at handling a narrower range of questions, many tied to facts, or Amazon's core strength in shopping.

Another challenge: When M could complete tasks, users asked for progressively harder tasks. A fully automated M would have to do things far beyond the capabilities of existing machine learning technology. Today's best algorithms are a long way from being able to really understand all the nuances of natural language.
 
 
Item 2016: 
def small_model():   
    x = tf.placeholder(tf.float32, shape=[None, 784], name='x')    
    y = tf.placeholder(tf.float32, shape=[None, 10], name='y') 
    layer1 = tf.layers.dense(x, 256, activation=tf.nn.relu)    
    layer2 = tf.layers.dense(layer1, 256, activation=tf.nn.relu)
    out = tf.layers.dense(layer2, 10)
    z = tf.argmax(out, 1, name='out')    
    loss = tf.losses.softmax_cross_entropy(y, out)    
    return loss
 tensorflow_graph = build_graph(small_model)
 spark_model = SparkAsyncDL(    
    inputCol='features',    
    tensorflowGraph=tensorflow_graph,    
    tfInput='x:0',   
    tfLabel='y:0',    
    tfOutput='out:0',    
    tfLearningRate=.001,    
    iters=20)
 
 
Item 2021: 
npm install -g http-server
 http-server dist --cors -p 1234 -s
 pip3 install tensorflowjs
 cd ./tfjs-densenet
tensorflowjs_converter --input_format keras ./model.h5 ./model
 modelLayerShape = layer.outputShape.slice(1)
 
 
Item 2022: 
tensorboard --logdir ./rnn/run1
 
 
Item 2023: 
{
  "type": "",
  "sub_type": "",
  "name": "",
  "description": "",
  "image_url": "",
  "required_docs": [],
  "benefits": [],
  "limit": "",
  "contact_method": "",
  "phone_no": ,
  "website_url": ""
}
 {
  "type": "",
  "name": "",
  "location": "",
  "lat": ,
  "long": ,
  "map_url": "",
  "image_url": "",
  "phone_no": ,
  "opening_hours": ""
}
 bx login
bx api https://api.ng.bluemix.net
bx target -o <your_organization_name> -s <your_space_name>
 bx wsk action create webhook webhook.js
bx wsk action create remember remember.js
bx wsk action create listen listen.js
bx wsk action create preprocess proprocess.js
bx wsk action create process process.js
bx wsk action create memorize memorize.js
bx wsk action create respond respond.js
 bx wsk action create transcribe-voice-recording transcribe-voice-recording.zip --kind nodejs:6
 bx wsk property get
 
 
Item 2026: 
#import necessary libraries
import numpy as np 
import pandas as pd
import plotly
import plotly.plotly as py
import plotly.offline as offline
import plotly.graph_objs as go
 init_notebook_mode(connected=True)
 shaz13_custom_style = "mapbox://styles/shaz13/cjiog1iqa1vkd2soeu5eocy4i"
 #set the geo=spatial data
data = [go.Scattermapbox(
            lat= train['pickup_latitude'] ,
            lon= train['pickup_longitude'],
            customdata = train['key'],
            mode='markers',
            marker=dict(
                size= 4,
                color = 'gold',
                opacity = .8,
            ),
          )]
 #set the layout to plot
layout = go.Layout(autosize=False,
                   mapbox= dict(accesstoken="YOUR_ACCESS_TOKEN",
                                bearing=10,
                                pitch=60,
                                zoom=13,
                                center= dict(lat=40.721319,
                                             lon=-73.987130),
                                style=shaz13_custom_style),
                    width=900,
                    height=600, 
                    title = "Pick up Locations in NewYork")
 fig = dict(data=data, layout=layout)
iplot(fig)
 train['pickup_datetime_month'] = train['pickup_datetime'].dt.month
train['pickup_datetime_year'] = train['pickup_datetime'].dt.year
train['pickup_datetime_day_of_week_name'] 
= train['pickup_datetime'].dt.weekday_name
 train['pickup_datetime_day_of_week'] 
= train['pickup_datetime'].dt.weekday
 train['pickup_datetime_day_of_hour'] = train['pickup_datetime'].dt.hour

 #Weekday
business_train = train[train['pickup_datetime_day_of_week'] < 5 ]
 #Bining time of the day
early_business_hours = business_train[business_train['pickup_datetime_day_of_hour'] < 10]
late_business_hours = business_train[business_train['pickup_datetime_day_of_hour'] > 18]
 data = [go.Scattermapbox(
            lat= early_business_hours['dropoff_latitude'] ,
            lon= early_business_hours['dropoff_longitude'],
            customdata = early_business_hours['key'],
            mode='markers',
            marker=dict(
                size= 5,
                color = 'gold',
                opacity = .8),
            name ='early_business_hours'
          ),
        go.Scattermapbox(
            lat= late_business_hours['dropoff_latitude'] ,
            lon= late_business_hours['dropoff_longitude'],
            customdata = late_business_hours['key'],
            mode='markers',
            marker=dict(
                size= 5,
                color = 'cyan',
                opacity = .8),
            name ='late_business_hours'
          )]
layout = go.Layout(autosize=False,
                   mapbox= dict(accesstoken="YOUR_ACCESS_TOKEN",
                                bearing=10,
                                pitch=60,
                                zoom=13,
                                center= dict(
                                         lat=40.721319,
                                         lon=-73.987130),
                                style= "mapbox://styles/shaz13/cjiog1iqa1vkd2soeu5eocy4i"),
                    width=900,
                    height=600, title = "Early vs. Late Business Days Pickup Locations")
fig = dict(data=data, layout=layout)
iplot(fig)
 weekend_train  = train[train['pickup_datetime_day_of_week'] >= 5 ]
early_weekend_hours = weekend_train[weekend_train['pickup_datetime_day_of_hour'] < 10]
late_weekend_hours = weekend_train[weekend_train['pickup_datetime_day_of_hour'] > 6]
 data = [go.Scattermapbox(
            lat= early_weekend_hours['dropoff_latitude'] ,
            lon= early_weekend_hours['dropoff_longitude'],
            customdata = early_weekend_hours['key'],
            mode='markers',
            marker=dict(
                size= 5,
                color = 'violet',
                opacity = .8),
            name ='early_weekend_hours'
          ),
        go.Scattermapbox(
            lat= late_weekend_hours['dropoff_latitude'] ,
            lon= late_weekend_hours['dropoff_longitude'],
            customdata = late_weekend_hours['key'],
            mode='markers',
            marker=dict(
                size= 5,
                color = 'orange',
                opacity = .8),
            name ='late_weekend_hours'
          )]
layout = go.Layout(autosize=False,
                   mapbox= dict(accesstoken="YOUR_ACCESS_TOKEN",
                                bearing=10,
                                pitch=60,
                                zoom=13,
                                center= dict(
                                         lat=40.721319,
                                         lon=-73.987130),
                                style= "mapbox://styles/shaz13/cjiog1iqa1vkd2soeu5eocy4i"),
                    width=900,
                    height=600, title = "Early vs. Late Weekend Days Pickup Locations")
fig = dict(data=data, layout=layout)
iplot(fig)
 high_fares = train[train['fare_amount'] > train.fare_amount.mean() + 3* train.fare_amount.std()]
 data = [go.Scattermapbox(
            lat= high_fares['pickup_latitude'] ,
            lon= high_fares['pickup_longitude'],
            customdata = high_fares['key'],
            mode='markers',
            marker=dict(
                size= 8,
                color = 'violet',
                opacity = .8),
            name ='high_fares_pick_up'
          ),
        go.Scattermapbox(
            lat= high_fares['dropoff_latitude'] ,
            lon= high_fares['dropoff_longitude'],
            customdata = high_fares['key'],
            mode='markers',
            marker=dict(
                size= 8,
                color = 'gold',
                opacity = .8),
            name ='high_fares_drop_off'
          )]
layout = go.Layout(autosize=False,
                   mapbox= dict(accesstoken="YOUR_ACCESS_TOKEN",
                                bearing=10,
                                pitch=60,
                                zoom=13,
                                center= dict(
                                         lat=40.721319,
                                         lon=-73.987130),
                                style= "mapbox://styles/shaz13/cjk4wlc1s02bm2smsqd7qtjhs"),
                    width=900,
                    height=600, title = "High Fare Locations")
fig = dict(data=data, layout=layout)
iplot(fig)
 
 
Item 2028: 
dataset = read.csv("OnlineRetail.csv", na.strings = c("","NA"))
 attach(dataset)
 #checking if there any missing values, where are they missing, and how many of them are missing
 any(is.na(dataset))
 [1] TRUE
 apply(dataset, 2, function(x) any(is.na(x)))
 InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country
 FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE
 sum(is.na(CustomerID))
 [1] 135080
 sum(is.na(Description))
 [1] 1454
 dataset$InvoiceNo = as.character(InvoiceNo)
trim = function (x) gsub("^\\s+|\\s+$", "", x)
dataset$InvoiceNo = trim(InvoiceNo)
dataset$Description = trim(as.character(Description))
 is_C = function (x) startsWith(x,"C")
dataset2 = dataset[which(!is_C(dataset$InvoiceNo)),] #subsetting
dataset3 = subset(dataset2,!is.na(dataset2$Description)) #subsetting
 buzzwords = c("WRONG","LOST", "CRUSHED", "SMASHED", "DAMAGED", "FOUND", "THROWN", "MISSING", "AWAY", "\\?", "CHECK", "POSTAGE", "MANUAL", "CHARGES", "AMAZON", "FEE", "FAULT", "SALES", "ADJUST", "COUNTED", "LABEL", "INCORRECT", "SOLD", "BROKEN", "BARCODE", "CRACKED", "RETURNED", "MAILOUT", "DELIVERY", "MIX UP", "MOULDY", "PUT ASIDE", "ERROR", "DESTROYED", "RUSTY")
 library(stringr)   #function str_detect
 isUndesirable = function(x){
 c = FALSE   #assume that the string is undesirable (FALSE), and perhaps switch to desirable (TRUE)
 for (i in 1:(length(buzzwords))){
  c = c || ifelse(str_detect(toupper(x),buzzwords[i]),TRUE,FALSE)
}
 #now we know whether or not the string is undesirable
 return(c)
 }
 dataset4 = subset(dataset3, dataset3$Quantity > 0)
 dataset5 = dataset4[which(!isUndesirable2(as.character(dataset4$Description))),]
 Time = format(as.POSIXct(strptime(dataset5$InvoiceDate,"%Y-%m-%d %H:%M",tz="")) ,format = "%H:%M:%S")
 dataset5$InvoiceDate = as.Date(dataset5$InvoiceDate)
dataset5$Description = as.factor(dataset5$Description)
 
 
Item 2029: 
![file, Annual_Figures_Suez_Panama.pdf]
 
 
Item 2034: 
model.add(layers.Dense(64, activation='relu'))
 model.add(layers.Dense(64, use_bias=False)) model.add(layers.BatchNormalization()) model.add(Activation("relu"))
 model.add(layers.Conv2D(64, (3, 3), activation='relu'))
 model.add(layers.Conv2D(64, (3, 3), use_bias=False)) model.add(layers.BatchNormalization()) model.add(layers.Activation("relu"))
 train_and_test(learning_rate=0.001, activation='sigmoid', epochs=3, steps_per_epoch=1875)
 train_and_test(learning_rate=0.01, activation='sigmoid', epochs=3, steps_per_epoch=1875)
 train_and_test(learning_rate=0.01, activation='relu', epochs=3, steps_per_epoch=1875)
 
 
Item 2036: 
0x454a2ab300000000000000000000000000000000000000000000000000000000000871ad
 keccak256(“<function>(<type_of_data_1>,<…>,<type_of_data_N>)”)
 /// Bids on an open auction, completing the auction and
/// transferring ownership of the NFT if enough Ether is supplied.
/// param _tokenID: ID of token to bid on.
function bid (uint256 _tokenId)
 keccak256(“bid(uint256)”) = 454a2ab3c602fd9…
 blockNumber: 0x51968f
topics: [0x0a5311bd2a6608f08a180df2ee7c5946819a649b204b554bb8e39825b2c50ad5]data: 0x0000000000000000000000001b8f7b13b14a59d9770f7c1789cf727046f7e542000000000000000000000000000000000000000000000000000000000009fac1000000000000000000000000000000000000000000000000000000000009f80e000000000000000000000000000000000000000000000000000000000008957200004a50b390a6738697012a030ac21d585b4c8214ae39446194054b98e0b98f
 /// The Pregnant event is fired when two cats successfully breed
/// and the pregnancy timer begins for the matron.
event Pregnant (address owner, uint256 matronId, uint256 sireId, uint256 cooldownEndBlock);
 /// The Birth event is fired whenever a new kitten comes into
/// existence. This obviously includes any time a cat is created
/// through the giveBirth method, but it is also called when
/// a new gen0 cat is created.
event Birth (address owner, uint256 kittyId, uint256 matronId, uint256 sireId, uint256 genes);
 keccak256(“Birth(address,uint256,uint256,uint256,uint256)”) = 0x0a5311bd2a6608f08a180df2ee7c5946819a649b204b554bb8e39825b2c50ad5
 owner: 0000000000000000000000001b8f7b13b14a59d9770f7c1789cf727046f7e542
kittyId:
000000000000000000000000000000000000000000000000000000000009fac1
matronId:
000000000000000000000000000000000000000000000000000000000009f80e
sireId:
0000000000000000000000000000000000000000000000000000000000089572
genes:
00004a50b390a6738697012a030ac21d585b4c8214ae39446194054b98e0b98f
 
 
Item 2050: 
## <class 'pandas.core.frame.DataFrame'>
## RangeIndex: 1704 entries, 0 to 1703
## Data columns (total 6 columns):
## country      1704 non-null object
## continent    1704 non-null object
## year         1704 non-null int64
## lifeExp      1704 non-null float64
## pop          1704 non-null int64
## gdpPercap    1704 non-null float64
## dtypes: float64(2), int64(2), object(2)
## memory usage: 80.0+ KB
 ## (1704, 6)
 ## Index(['country', 'continent', 'year', 'lifeExp', 'pop', 'gdpPercap'], dtype='object')
 ## RangeIndex(start=0, stop=1704, step=1)
 ## [1] 1704    6
 ## [1] 1704
 ## [1] 6
 ## [1] "country"   "continent" "year"      "lifeExp"   "pop"       "gdpPercap"
 ## [1] "1" "2" "3" "4" "5" "6"
 ## 6251013179
 ##      ttl_pop
## 1 6251013179
 ## continent
## Africa       929539692
## Americas     898871184
## Asia        3811953827
## Europe       586098529
## Oceania       24549947
## Name: pop, dtype: int64
 
 
Item 2052: 
dataset = dataset_DJI[1:]- dataset_DJI[:-1]
 
 
Item 2057: 
wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh
Anaconda3-5.2.0-Linux-x86_64.sh
 conda create -n myenv
 source activate myenv
 conda install numpy pandas scipy matplotlib seaborn scikit-learn
 
 
Item 2059: 
Update weight : w += - learning_rate * gradient
 
 
Item 2061: 

### Which cluster corresponds to the most instances per genre? ###
                                                       KMeansLabel
genres_top                                                        
Classical                                  Slow & Somber Acoustics
Electronic                         Happy & Danceable Instrumentals
Folk/Blues                                 Slow & Somber Acoustics
Hip-Hop/Easy Listening           Upbeat Songs With Cheerful Vocals
Jazz/Experimental/International            Slow & Somber Acoustics
Metal                                 Happy & Upbeat Instrumentals
Old-Time / Historic/Spoken                 Slow & Somber Acoustics
Pop/Country                      Upbeat Songs With Cheerful Vocals
Rock/Instrumental/Soul-RnB                       Sad Instrumentals
 ### Which genre corresponds to the most instances per cluster? ###
                                                   genres_top
KMeansLabel                                                  
Fast & Danceable Instrumentals                     Electronic
Fast, Upbeat & Cheerful Songs      Rock/Instrumental/Soul-RnB
Happy & Danceable Instrumentals                    Electronic
Happy & Slow                       Rock/Instrumental/Soul-RnB
Happy & Upbeat Instrumentals       Rock/Instrumental/Soul-RnB
Sad Instrumentals                  Rock/Instrumental/Soul-RnB
Slow & Somber Acoustics                            Folk/Blues
Slow Dance                                         Folk/Blues
Upbeat Songs With Cheerful Vocals  Rock/Instrumental/Soul-RnB
 
### Which cluster corresponds to the most instances per genre? ###
                                    KMeansLabel
subgenre                                       
Indie-Rock    Slow & Cheerful Rock
Pop          Fast & Energetic Rock
Psych-Rock  Slow & Depressing Rock
Punk         Fast & Energetic Rock
Rock         Fast & Energetic Rock
### Which genre corresponds to the most instances per cluster? ###
                                    subgenre
KMeansLabel                                 
Fast & Energetic Rock                   Punk
Slow & Cheerful Rock                    Rock
Slow & Depressing Rock                  Rock
Slow Dance Rock                         Rock
Upbeat Rock with Synths to Dance to     Rock
 
 
Item 2062: 
<?xml version=”1.0" encoding=”UTF-8"?>
<aiml version=”1.0.1" encoding=”UTF-8"?>
<category>
  <pattern> Melhor filme de crime *</pattern>
  <template>
    <random>
      <li> Onze Homens e um Segredo </li>
      <li> Cidade de Deus </li>
      <li> Assalto ao Banco Central </li>
      <li> Inimigos Públicos </li>
    </random>
  </template>
</category>
</aiml>
 
 
Item 2064: 
adorn_totals()
 adorn_totals(where="col")
 adorn_totals(where=c("row","col"))
 
 
Item 2065: 
{
  "fields": ["SquareFeet", "Bedrooms"],
  "values": [squareFeet, numBedrooms]
}
 Authorization: Bearer TOKEN_GOES_HERE
 {
  "fields": ["SquareFeet", "Bedrooms", "features", "prediction"],
  "values": [[2400, 4, [2400.0, 4.0], 137499.99999999968]]
}
 npm install watson-ml-model-utils
 WML_SERVICE_PATH=https://ibm-watson-ml.mybluemix.net
WML_USERNAME=
WML_PASSWORD=
WML_INSTANCE_ID=
WML_MODEL_ID=
WML_DEPLOYMENT_ID=
 const { WatsonMLScoringEndpoint } = require('watson-ml-model-utils');
 let endpoint = new WatsonMLScoringEndpoint(['SquareFeet', 'Bedrooms']);
 endpoint.score([2400, 4])
    .then(response => console.log(response.prediction))
    .catch(err => console.log(err));
 137499.99999999968
 endpoint.scoreMulti([[2400, 4], [2000, 3], [2600, 6]])
    .then(response => console.log(response.predictions))
    .catch(err => console.log(err));
 [ 137499.99999999968, 87500.00000000276, 162500.00000000527 ]
 
 
Item 2067: 
##      score        best_new_music                   author     
##  Min.   : 0.000   Min.   :0.00000   joe tangari       :  815  
##  1st Qu.: 6.400   1st Qu.:0.00000   stephen m. deusner:  725  
##  Median : 7.200   Median :0.00000   ian cohen         :  699  
##  Mean   : 7.006   Mean   :0.05128   brian howe        :  500  
##  3rd Qu.: 7.800   3rd Qu.:0.00000   mark richardson   :  476  
##  Max.   :10.000   Max.   :1.00000   stuart berman     :  445  
##                                     (Other)           :14729  
##               author_type          pub_date      pub_weekday   
##  contributor        :12420   2000-03-31:   15   Min.   :0.000  
##                     : 3904   2000-04-30:   14   1st Qu.:1.000  
##  senior editor      :  486   2001-03-31:   13   Median :2.000  
##  executive editor   :  475   1999-04-20:   12   Mean   :2.107  
##  senior staff writer:  439   2001-02-20:   11   3rd Qu.:3.000  
##  contributing editor:  210   1999-06-08:   10   Max.   :6.000  
##  (Other)            :  455   (Other)   :18314                  
##     pub_day        pub_month         pub_year             genre     
##  Min.   : 1.00   Min.   : 1.000   Min.   :1999   rock        :7815  
##  1st Qu.: 8.00   1st Qu.: 3.000   1st Qu.:2005   electronic  :2900  
##  Median :15.00   Median : 6.000   Median :2009               :2365  
##  Mean   :15.53   Mean   : 6.283   Mean   :2009   rap         :1413  
##  3rd Qu.:23.00   3rd Qu.: 9.000   3rd Qu.:2013   experimental:1141  
##  Max.   :31.00   Max.   :12.000   Max.   :2017   pop/r&b     :1128  
##                                                  (Other)     :1627  
##            label         diversity          length      
##  self-released:  419   Min.   :0.0000   Min.   :   0.0  
##  drag city    :  263   1st Qu.:0.5448   1st Qu.: 498.0  
##  sub pop      :  261   Median :0.5778   Median : 604.0  
##  thrill jockey:  241   Mean   :0.5771   Mean   : 650.1  
##  merge        :  231   3rd Qu.:0.6108   3rd Qu.: 746.0  
##  warp         :  210   Max.   :0.8889   Max.   :3688.0  
##  (Other)      :16764
 ##Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##0.000   6.400   7.200   7.006   7.800  10.000
 subset(reviews, score == 0 & length == 0)
 reviewid   title      artist
605494     shine on   jet
 set.seed(1984)
 myVars = c(‘genre’, ‘score’)
 jazz <- subset(reviews, genre == ‘jazz’)[myVars]
global <- subset(reviews, genre == ‘global’)[myVars]
rock <- subset(reviews, genre == ‘rock’)[myVars][sample(1:nrow(subset(reviews, genre == ‘rock’)), 144,
 replace=FALSE),]
rap <- subset(reviews, genre == ‘rap’)[myVars][sample(1:nrow(subset(reviews, genre == ‘rap’)), 144,
 replace=FALSE),]
electronic <- subset(reviews, genre == ‘electronic’)[myVars][sample(1:nrow(subset(reviews, genre == ‘electronic’)), 144,replace=FALSE),]
pop <- subset(reviews, genre == ‘pop/r&b’)[myVars][sample(1:nrow(subset(reviews, genre == ‘pop/r&b’)), 144,replace=FALSE),]
metal <- subset(reviews, genre == ‘metal’)[myVars][sample(1:nrow(subset(reviews, genre == ‘metal’)), 144,replace=FALSE),]
experimental <- subset(reviews, genre == ‘experimental’)[myVars][sample(1:nrow(subset(reviews, genre == ‘experimental’)), 144,replace=FALSE),]
country <- subset(reviews, genre == ‘folk/country’)[myVars][sample(1:nrow(subset(reviews, genre == ‘folk/country’)), 144,replace=FALSE),]
 testing <- rbind(jazz, global, rap, rock, 
 electronic, pop, metal, experimental, country)
 testing.numbers <- group_by(testing, genre)
 test <- lm(score ~ genre, testing)
 anova(test)
 Call:
lm(formula = score ~ genre, data = testing)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.8465 -0.5593  0.2375  0.8104  3.0722 

Coefficients:
                    Estimate Std. Error t value Pr(>|t|)    
(Intercept)        6.9458333  0.1020924  68.035  < 2e-16 ***
genreexperimental  0.4166667  0.1443805   2.886 0.003962 ** 
genrefolk/country  0.3909722  0.1443805   2.708 0.006852 ** 
genreglobal        0.4854167  0.1443805   3.362 0.000794 ***
genrejazz          0.5134394  0.1260184   4.074 4.87e-05 ***
genremetal        -0.0562500  0.1443805  -0.390 0.696894    
genrepop/r&b       0.0006944  0.1443805   0.005 0.996163    
genrerap          -0.2604167  0.1443805  -1.804 0.071493 .  
genrerock         -0.0180556  0.1443805  -0.125 0.900497    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.225 on 1418 degrees of freedom
Multiple R-squared:  0.04874,	Adjusted R-squared:  0.04337 
F-statistic: 9.081 on 8 and 1418 DF,  p-value: 3.106e-12

Analysis of Variance Table

Response: score
            Df  Sum Sq Mean Sq F value    Pr(>F)    
genre        8  109.04 13.6302  9.0814 3.106e-12 ***
Residuals 1418 2128.26  1.5009
 head(subset(reviews, score == 10), 20)

 artist                  album                               pub_year
beastie boys            paul's boutique                      2002
boards of canada        music has the right to children      2015
bob dylan               blood on the tracks                  2009
bonnie prince billy     i see a darkness                     2004
brian eno               another green world                  2016
bruce springsteen       born to run: 30th anniversary ed...  1999
can                     tago mago [40th anniversary edition] 2016
dangelo                 voodoo                               2005 
david bowie             "heroes"                             2011
david bowie             low                                  2012
dj shadow               endtroducing... [deluxe edition]     2016
elvis costello & the attractions   this year's model         2016
fleetwood mac           rumours                              2005
glenn branca            the ascension                        2002
gza                     liquid swords: chess box deluxe ed...2013
james brown             live at the apollo [expanded ed...]  2003
j dilla                 donuts (45 box set)                  2012
john coltrane           the olatunji concert: the last li... 2004
...and you will kno...  source tags and codes                2002
a tribe called quest    people's instinctive travels and...  2015
 
 
Item 2068: 
#standardSQL
SELECT * FROM
 `project.BCM.AI` t
WHERE
 REGEXP_CONTAINS(LOWER(TO_JSON_STRING(t)), r’caixa marcha’)
 
 
Item 2073: 
pokemon_csv = pd.read_csv('Pokemon.csv')

df = pd.DataFrame(pokemon_csv, columns = ['Name', 'Type 1', 'Type 2', 'Total', 'HP','Attack','Defense','Sp. Atk','Sp. Def','Speed','Generation','Legendary'])
 print(df.apply(lambda x: x.count()))
 df = df.drop(['Name','Type 2'],axis=1)
 i = 0
uniqueItem = dict()
 for item in df['Type 1']:
    if item not in uniqueItem:
        uniqueItem[str(item)] = i
        i+=1
 uniqueItem = {'Grass': 0, 'Fire': 1, 'Water': 2, 'Bug': 3, 
              'Normal': 4, 'Poison': 5, 'Electric': 6, 
              'Ground': 7, 'Fairy': 8, 'Fighting': 9, 
              'Psychic': 10, 'Rock': 11, 'Ghost': 12, 
              'Ice': 13, 'Dragon': 14, 'Dark': 15, 
              'Steel': 16, 'Flying': 17}
 for type in df['Type 1']:
    df = df.replace({type:uniqueItem.get(type)})
 X = np.array(df.iloc[:,0:-1])
Y = np.array([[df['Legendary']]])
 Y = Y.reshape(800)
 Before reshaping Y :  (1, 1, 800)
After reshaping  Y :  (800,)
 X_train, X_test, y_train, y_test = train_test_split(
X, Y, test_size = 0.2, random_state = 10)

model = GaussianNB()
model.fit(X_train, y_train)
 y_pred = model.predict(X_test)
print("Accuracy score of NaiveBayes:",accuracy_score(y_test,y_pred))
 Accuracy score of Naive Bayes:  0.95625
 clf = linear_model.SGDClassifier()
clf.fit(X, Y)

y_pred = clf.predict(X_test)
print("Accuracy score of SDG: ",accuracy_score(y_test,y_pred))
 Accuracy score of SDG:  0.91875
 df = df.drop(['Name'],axis=1)
df = df.dropna(subset=['Type 2'])
 for item in df['Type 2']:
    if item not in uniqueItem2:
        uniqueItem2[str(item)] = i
        i+=1

for type in df['Type 2']:
    if type in uniqueItem2:
        df = df.replace({type:uniqueItem2.get(type)})
 
 
Item 2079: 
library(tidyverse)
library(stringr)
library(lubridate)
library(DT)
 fillColor = “#FFA07A”
fillColor2 = “#F1C40F”
 players = read_csv(“../input/wta/players.csv”)
rankings = read_csv(“../input/wta/rankings.csv”)
matches = read_csv(“../input/wta/matches.csv”)
 matches$tourney_name = str_replace(matches$tourney_name,”Us Open”,”US Open”)
 matches_country_winner = 
 matches %>% 
 select(winner_name,winner_ioc) %>%
 rename ( name = winner_name,ioc = winner_ioc)
 matches_country_loser = 
 matches %>% 
 select(loser_name,loser_ioc) %>%
 rename ( name = loser_name,ioc = loser_ioc)
 matches_country = unique(rbind(matches_country_winner,matches_country_loser))
 matches_country %>%
 group_by(ioc) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(ioc = reorder(ioc,Count)) %>%
 head(20) %>%
 
 ggplot(aes(x = ioc,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor) +
 geom_text(aes(x = ioc, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Country’, 
 y = ‘Count’, 
 title = ‘Country and Count’) +
 coord_flip() +
 theme_bw()
 players_rankings = inner_join(players,rankings)
players_rankings$year = str_sub(players_rankings$ranking_date,1,4)
players_rankings$month = str_sub(players_rankings$ranking_date,5,6)
 players_rankings_rank_one = players_rankings %>% filter(ranking == 1)
 players_rankings_rank_one_year = 
 players_rankings_rank_one %>% 
 group_by(first_name,last_name,year) %>% 
 tally()
players_rankings_rank_one_year = players_rankings_rank_one_year %>% select(-n)
players_rankings_rank_one_year = unique(players_rankings_rank_one_year)
 players_rankings_rank_one_year %>%
 mutate(FullName = paste0(first_name,” “,last_name,””)) %>%
 group_by(FullName) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(FullName = reorder(FullName,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = FullName,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor) +
 geom_text(aes(x = FullName, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Name’, 
 y = ‘Count’, 
 title = ‘Name and Count’) +
 coord_flip() + 
 theme_bw()
 players_rankings_rank_one_year %>%
 group_by(year) %>%
 summarise(Count = n()) %>%
 arrange(desc(year)) %>%
 ungroup() %>%
 head(15) %>%
 
 ggplot(aes(x = year,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor2) +
 geom_text(aes(x = year, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Year’, 
 y = ‘Count’, 
 title = ‘Year and Count’) +
 coord_flip() + 
 theme_bw()
 datatable(head(players_rankings_rank_one_year %>% arrange(desc(year)),10), style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 players_rankings_rank_one_year %>%
 group_by(year) %>%
 summarise(Count = n()) %>%
 arrange(desc(year)) %>%
 ungroup() %>%
 tail(20) %>%
 
 ggplot(aes(x = year,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor2) +
 geom_text(aes(x = year, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Year’, 
 y = ‘Count’, 
 title = ‘Year and Count’) +
 coord_flip() + 
 theme_bw()
 datatable(tail(players_rankings_rank_one_year %>% arrange(desc(year)) ,34), style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 plotTrendsRankings = function(players_rankings,firstname,lastname,plottitle)
{
 players_rankings %>% 
 filter(first_name == firstname) %>%
 filter(last_name == lastname) %>%
 mutate(YearMonth = make_date(year=year,month=month) ) %>%
 
 ggplot(aes(x=YearMonth,y=ranking,group = 1)) +
 geom_point(size=2, color=”red”) +
 
 labs(x = ‘Time’, y = ‘ranking’,title = plottitle) +
 theme_bw() 
 
}
 plotTrendsRankings(players_rankings,”Serena”,”Williams”,”Trend of Ranking for Serena Williams”)
 rankingsAboveThreshold = function(players_rankings,firstname,lastname,threshold)
{
 players_rankings %>% 
 filter(first_name == firstname) %>%
 filter(last_name == lastname) %>%
 filter(ranking >=threshold)
}
 serena100 = rankingsAboveThreshold(players_rankings,”Serena”,”Williams”,100) %>%
 select(year,month,ranking) %>% 
 arrange(desc(ranking))
 datatable(serena100 %>% arrange(desc(year)), style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 plotTrendsRankings(players_rankings,”Steffi”,”Graf”,”Trend of Ranking for Steffi Graf”)
 plotTrendsRankings(players_rankings,”Simona”,”Halep”,”Trend of Ranking for Simona Halep”)
 plotTrendsRankings(players_rankings,”Sania”,”Mirza”,”Trend of Ranking for Sania Mirza”)
 getTournamentWinners = function(tournamentname,roundname)
{
 return(
 matches %>%
 filter(tourney_name == tournamentname) %>%
 filter(round == roundname) %>%
 mutate(agediff = winner_age — loser_age) %>%
 mutate(rankingdiff = loser_rank — winner_rank) %>%
 mutate(htdiff = winner_ht — loser_ht)
 )
}
 getGrandSlamWinners = function(roundname)
{
 return(
 matches %>%
 filter(tourney_level == “G”) %>%
 filter(round == roundname) %>%
 mutate(agediff = winner_age — loser_age) %>%
 mutate(rankingdiff = loser_rank — winner_rank)
 )
}
 plotTournamentWinners = function(tournament,titleName)
{
 tournament %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor2) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = titleName) +
 coord_flip() + 
 theme_bw()
 
}
 ausopen = getTournamentWinners(“Australian Open”,”F”)
plotTournamentWinners(ausopen,’Aus Open Winners Count’)
 summary(ausopen$winner_age)
 ausopen %>%
 ggplot(aes(x = winner_age)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Winner Age’,y = ‘Count’, title = paste(“Distribution of”, ‘ Winner Age ‘)) +
 theme_bw()
 french = getTournamentWinners(“French Open”,”F”) 
plotTournamentWinners(french,’French Open Winners Count’)
 summary(french$winner_age)
 french %>%
 ggplot(aes(x = winner_age)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Winner Age’,y = ‘Count’, title = paste(“Distribution of”, ‘ Winner Age ‘)) +
 theme_bw()
 wimbledon = getTournamentWinners(“Wimbledon”,”F”) 
plotTournamentWinners(wimbledon,’Wimbledon Winners Count’)
 summary(wimbledon$winner_age)
 wimbledon %>%
 ggplot(aes(x = winner_age)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Winner Age’,y = ‘Count’, title = paste(“Distribution of”, ‘ Winner Age ‘)) +
 theme_bw()
 usopen = getTournamentWinners(“US Open”,”F”) 
plotTournamentWinners(usopen,’US Open Winners Count’)
 summary(usopen$winner_age)
 usopen %>%
 ggplot(aes(x = winner_age)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Winner Age’,y = ‘Count’, title = paste(“Distribution of”, ‘ Winner Age ‘)) +
 theme_bw()
 grandslam= rbind(ausopen,french,wimbledon,usopen)
 summary(grandslam$htdiff)
 grandslam %>%
 ggplot(aes(x = htdiff)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Height Difference’,y = ‘Count’, title = paste(“Distribution of”, ‘ Height Difference ‘)) +
 theme_bw()
 grandslam %>%
 ggplot(aes(x = htdiff)) +
 geom_density(fill = fillColor2) +
 labs(x= ‘Height Difference’,y = ‘Count’, title = paste(“Distribution of”, ‘ Height Difference ‘)) +
 theme_bw()
 grandslamhtdiff = grandslam %>% select(winner_name,loser_name,htdiff)
 datatable(grandslamhtdiff, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 summary(grandslam$agediff)
 grandslam %>%
 ggplot(aes(x = agediff)) +
 geom_histogram(binwidth = 1,fill = fillColor) +
 labs(x= ‘Age Difference’,y = ‘Count’, title = paste(“Distribution of”, ‘ Age Difference ‘)) +
 theme_bw()
 grandslam %>%
 ggplot(aes(x = agediff)) +
 geom_density(fill = fillColor2) +
 labs(x= ‘Age Difference’,y = ‘Count’, title = paste(“Distribution of”, ‘ Age Difference ‘)) +
 theme_bw()
 whowon = function(scores,setnumber)
{
 scores2 = str_split(scores,” “)
 set = scores2[[1]][setnumber]
 set_score = str_split(set,”-”)
 winner_score = as.numeric(set_score[[1]][1])
 loser_score =as.numeric(str_split(set_score[[1]][2],””)[[1]][1])
 
 if( (is.na(winner_score)) ||
 (is.na(loser_score))
 )
 {
 setwinner = “”
 }else
 {
 if(winner_score > loser_score)
 {
 setwinner = “winner”
 }else
 {
 setwinner = “loser”
 }
 }
 
 
 return(setwinner)
 
}
 matches$first_set = sapply(matches$score,whowon, setnumber = 1)
matches$second_set = sapply(matches$score,whowon, setnumber = 2)
matches$third_set = sapply(matches$score,whowon, setnumber = 3)
 first_set_loser = matches %>% 
 filter(first_set == “loser”)
 nrow(first_set_loser)/nrow(matches) *100
 first_set_loser %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor2) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = ‘Winner’) +
 coord_flip() + 
 theme_bw()
 second_set_loser = matches %>% 
 filter(second_set == “loser”)
 nrow(second_set_loser)/nrow(matches) *100
 second_set_loser = matches %>% 
 filter(second_set == “loser”)
 second_set_loser %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”, fill = fillColor) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = ‘Winner’) +
 coord_flip() + 
 theme_bw()
 gs_final_firstset_loser = matches %>%
 filter(tourney_level == “G”) %>%
 filter(round == “F”) %>%
 filter(first_set == “loser”)
 gs_final_secondset_loser = matches %>%
 filter(tourney_level == “G”) %>%
 filter(round == “F”) %>%
 filter(second_set == “loser”)
 gs_final_thirdset_loser = matches %>%
 filter(tourney_level == “G”) %>%
 filter(round == “F”) %>%
 filter(third_set == “loser”)
 gs_final = matches %>%
 filter(tourney_level == “G”) %>%
 filter(round == “F”)
 nrow(gs_final_firstset_loser)/nrow(gs_final) *100
 nrow(gs_final_secondset_loser)/nrow(gs_final) *100
 percentWinnersTourney = function(matches,tournamentName)
{
 gs_final_firstset_loser = matches %>%
 filter(tourney_name == tournamentName) %>%
 filter(round == “F”) %>%
 filter(first_set == “loser”)
gs_final_secondset_loser = matches %>%
 filter(tourney_name == tournamentName) %>%
 filter(round == “F”) %>%
 filter(second_set == “loser”)
 gs_final = matches %>%
 filter(tourney_name == tournamentName) %>%
 filter(round == “F”)
 nrow(gs_final_firstset_loser)/nrow(gs_final) *100
 }
 displayGrandSlamWinnersAfterLosingFirstSet = function(matches,tournamentName)
{
 gs_final_firstset_loser = matches %>%
 filter(tourney_name == tournamentName) %>%
 filter(round == “F”) %>%
 filter(first_set == “loser”) %>% 
 select(winner_name,loser_name,year,score)
 
 datatable(gs_final_firstset_loser, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
}
 percentWinnersTourney(matches,”Australian Open”)
displayGrandSlamWinnersAfterLosingFirstSet(matches,”Australian Open”)
 percentWinnersTourney(matches,”Wimbledon”)
displayGrandSlamWinnersAfterLosingFirstSet(matches,”Wimbledon”)
 percentWinnersTourney(matches,”French Open”)
displayGrandSlamWinnersAfterLosingFirstSet(matches,”French Open”)
 percentWinnersTourney(matches,”US Open”)
displayGrandSlamWinnersAfterLosingFirstSet(matches,”US Open”)
 matches %>%
 filter(!is.na(surface)) %>%
 filter(!str_detect(surface,”-”)) %>%
 group_by(surface) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(surface = reorder(surface,Count)) %>%
 
 
 ggplot(aes(x = surface,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor2) +
 geom_text(aes(x = surface, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Surface’, 
 y = ‘Count’, 
 title = ‘Surface and Count’) +
 coord_flip() + 
 theme_bw()
 matches %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = ‘Winner and Count’) +
 coord_flip() + 
 theme_bw()
 surfaceTitle = ‘Country Winning on All Surfaces’
 matches %>%
 group_by(winner_ioc) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_ioc = reorder(winner_ioc,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_ioc,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor) +
 geom_text(aes(x = winner_ioc, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = surfaceTitle, 
 y = ‘Count’, 
 title = surfaceTitle) +
 coord_flip() + 
 theme_bw()
 surfaceWinners = function(surfaceName)
{
 matches %>%
 filter(surface == surfaceName) %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor2) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = ‘Winner and Count’) +
 coord_flip() + 
 theme_bw()
 
}
 countriesSurface = function(surfaceName,surfaceTitle)
{
 matches %>%
 filter(surface == surfaceName) %>%
 group_by(winner_ioc) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_ioc = reorder(winner_ioc,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_ioc,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor) +
 geom_text(aes(x = winner_ioc, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = surfaceTitle, 
 y = ‘Count’, 
 title = surfaceTitle) +
 coord_flip() + 
 theme_bw()
}
 surfaceWinners(‘Hard’)
countriesSurface(‘Hard’,’Country Winning on Hard Surface’)
 surfaceWinners(‘Clay’)
countriesSurface(‘Clay’,’Country Winning on Clay Surface’)
 surfaceWinners(‘Grass’)
countriesSurface(‘Grass’,’Country Winning on Grass Surface’)
 surfaceWinners(‘Carpet’)
countriesSurface(‘Carpet’,’Country Winning on Carpet Surface’)
 matches %>%
 filter(!is.na(tourney_level)) %>%
 group_by(tourney_level) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(tourney_level = reorder(tourney_level,Count)) %>%
 
 
 ggplot(aes(x = tourney_level,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor2) +
 geom_text(aes(x = tourney_level, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Tournament Levels’, 
 y = ‘Count’, 
 title = ‘Tournament Levels and Count’) +
 coord_flip() + 
 theme_bw()
 tournamnentLevelTournaments = function(tournamnentLevel)
{
 matches %>%
 filter(tourney_level == tournamnentLevel) %>%
 group_by(tourney_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(tourney_name = reorder(tourney_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = tourney_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor) +
 geom_text(aes(x = tourney_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Tournament Name’, 
 y = ‘Count’, 
 title = ‘Tournament Name and Count’) +
 coord_flip() + 
 theme_bw()
 
}
 tournamnentLevelWinners = function(tournamnentLevel)
{
 matches %>%
 filter(tourney_level == tournamnentLevel) %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor2) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = ‘Winner’, 
 y = ‘Count’, 
 title = ‘Winner and Count’) +
 coord_flip() + 
 theme_bw()
 
}
 tournamnentLevelWinners(‘G’)
 tournamnentLevelTournaments(‘T1’)
 tournamnentLevelWinners(‘T1’)
 tournamnentLevelTournaments(‘T2’)
 tournamnentLevelWinners(‘T2’)
 tournamnentLevelTournaments(‘T3’)
 tournamnentLevelWinners(‘T3’)
 grandslamupsets = grandslam %>%
 filter(rankingdiff < -10) %>%
 select(winner_name,loser_name,winner_rank,loser_rank,tourney_name,year)
 plotUpsets = function(upsetsData,titleName)
{
 upsetsData %>%
 group_by(winner_name) %>%
 summarise(Count = n()) %>%
 arrange(desc(Count)) %>%
 ungroup() %>%
 mutate(winner_name = reorder(winner_name,Count)) %>%
 head(10) %>%
 
 ggplot(aes(x = winner_name,y = Count)) +
 geom_bar(stat=’identity’,colour=”white”,fill=fillColor2) +
 geom_text(aes(x = winner_name, y = 1, label = paste0(“(“,Count,”)”,sep=””)),
 hjust=0, vjust=.5, size = 4, colour = ‘black’,
 fontface = ‘bold’) +
 labs(x = titleName, 
 y = ‘Count’, 
 title = titleName) +
 coord_flip() + 
 theme_bw()
}
 
plotUpsets(grandslamupsets,’Upset Winner in Grand Slam Finals’)
 datatable(grandslamupsets, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 grandslamSF =getGrandSlamWinners(‘SF’)
 grandslamupsets = grandslamSF %>%
 filter(rankingdiff < -10) %>%
 select(winner_name,loser_name,winner_rank,loser_rank,tourney_name,year)
 plotUpsets(grandslamupsets,’Upset Winner in Grand Slam Semi Finals’)
 datatable(grandslamupsets, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 getTournamentWinners = function(tourney_level_name,roundname)
{
 return(
 matches %>%
 filter(tourney_level == tourney_level_name) %>%
 filter(round == roundname) %>%
 mutate(agediff = winner_age — loser_age) %>%
 mutate(rankingdiff = loser_rank — winner_rank)
 )
}
 T1Winners = getTournamentWinners(‘T1’,’F’)
 T1upsets = T1Winners %>%
 filter(rankingdiff < -10) %>%
 select(winner_name,loser_name,winner_rank,loser_rank,tourney_name,year)
 plotUpsets(T1upsets,’Upset Winner in T1 tournament Finals’)
 datatable(T1upsets, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 T2Winners = getTournamentWinners(‘T2’,’F’)
 T2upsets = T2Winners %>%
 filter(rankingdiff < -10) %>%
 select(winner_name,loser_name,winner_rank,loser_rank,tourney_name,year)
 plotUpsets(T2upsets,’Upset Winner in T2 tournament Finals’)
 datatable(T2upsets, style=”bootstrap”, class=”table-condensed”, options = list(dom = ‘tp’,scrollX = TRUE))
 
 
Item 2080: 
I recommend that Item 'FDW58', be moved from Outlet 'OUT049', to Outlet 'OUT027'.
The adjusted forecast is expected to be 64.0% better (2606.40), than the original forecast of $1589.31.
 Configuration details:
Outlet_Size:  Medium --> Medium
Outlet_Location_Type: Tier 1 --> Tier 3
Outlet_Type: Supermarket Type1 --> Supermarket Type3
Outlet_Age: 19 --> 33
 
 
Item 2082: 
aws cloudformation create-stack –template-body file://<path to master-windows.yaml> — stack-name <name>
 {
   "family": "windows-simple-iis",
   "containerDefinitions": [
   {
     "name": "windows_sample_app",
     "image": "microsoft/iis",
     "cpu": 100,
     "entryPoint":["powershell", "-Command"],
     "command":["New-Item -Path C:\\inetpub\\wwwroot\\index.html -Type file -Value '<html><head><title>Amazon ECS Sample App</title> <style>body {margin-top: 40px; background-color: #333;} </style> </head><body> <div style=color:white;text-align:center><h1>Amazon ECS Sample App</h1> <h2>Congratulations!</h2> <p>Your application is now running on a container in Amazon ECS.</p></body></html>'; C:\\ServiceMonitor.exe w3svc"],
     "portMappings": [
     {
       "protocol": "tcp",
       "containerPort": 80,
       "hostPort": 8080
     }
     ],
     "memory": 500,
     "essential": true
   }
   ]
}
 
 
Item 2084: 
Images*: 
[1] Eeg beta [Image]. (2005, December 22). Retrieved from https://commons.wikimedia.org/wiki/File:Eeg_beta.svg
 [2] Eeg delta [Image]. (2005, December 22). Retrieved from https://commons.wikimedia.org/wiki/File:Eeg_delta.svg
 [3] Fourier series for trig poly high degree [Image]. (2010, November 15). Retrieved from https://commons.wikimedia.org/wiki/File:Fourier_series_for_trig_poly_high_degree.gif
 [4] Sleep Hypnogram [Image]. (2011, December 20). Retrieved from https://commons.wikimedia.org/wiki/File:Sleep_Hypnogram.svg
 [5] The Simple Harmonic Oscillator [Image]. (n.d.). Retrieved July 21, 2018, from The University of Texas at Austin Department of Physics website: https://web2.ph.utexas.edu%2F~coker2%2Findex.files%2Fsho3.html
 Web Pages:
[6] HESSI Scientific Objectives. (n.d.). Retrieved July 21, 2018, from NASA website: https://hesperia.gsfc.nasa.gov/hessi/objectives.htm
 [7] The Seduction of a Sunny Day: How Weather Biases Car Buying Behavior. (2015, May 14). Retrieved July 22, 2018, from Association for Psychological Science website: https://www.psychologicalscience.org/news/motr/the-seduction-of-a-sunny-day-how-weather-biases-car-buying-behavior.html
 [8] [Solar cycles]. (n.d.). Retrieved July 21, 2018, from NASA StarChild website: https://starchild.gsfc.nasa.gov/docs/StarChild/questions/question17.html
 [9] Williams, L. (2015, February 2). What's in a Baby's Name? The Cyclical Trends and Spread of Unusual Names. Retrieved July 22, 2018, from Independent website: https://www.independent.co.uk/life-style/health-and-families/features/baby-names-the-most-popular-cyclical-trends-and-spread-of-unusual-names-10018000.html
 [10] Wilson, C. (2014, May 11). How Popular Will Your Name Be in 25 Years? Retrieved July 22, 2018, from TIME website: http://time.com/93911/baby-name-predictor/
 Reviewed and Published Datasets:
[11] Kaggle. (2017). US Baby Names [data file]. Retrieved from https://www.kaggle.com/kaggle/us-baby-names/home
 [12] Ko, James. (2018). Gun Violence Data [data file]. Retrieved from https://www.kaggle.com/jameslko/gun-violence-data/home
 [13] Opplysningsrådet for Veitrafikken AS, “dmi3kno”. (2018). New Car Sales in Norway [data file]. Retrieved from https://www.kaggle.com/dmi3kno/newcarsalesnorway
 [14] Samaha, Kheiralla. (2017). Solar Flares from RHESSI Mission [data file]. Retrieved from https://www.kaggle.com/khsamaha/solar-flares-rhessi/home
 Scientific Papers:
[15] Geoffroy, P. A., & Amad, A. (2016). Seasonal Influence on Mass Shootings. American Journal of Public Health, 106(5), e15. http://doi.org/10.2105/AJPH.2016.303065
 
 
Item 2085: 
//Serializes the object into a byte array for storage
static byte[] convertToBytes(Object object) throws IOException {
    try (ByteArrayOutputStream bos = new ByteArrayOutputStream();
         ObjectOutput out = new ObjectOutputStream(bos)) {
        out.writeObject(object);
        return bos.toByteArray();
    }
}
 //de serializes the byte array and returns the stored object
static Object convertFromBytes(byte[] bytes) throws IOException, ClassNotFoundException {
    try (ByteArrayInputStream bis = new ByteArrayInputStream(bytes);
         ObjectInput in = new ObjectInputStream(bis)) {
        return in.readObject();
    }
}
 try {
    ResourceIterator<Entity> n = db.execute("MATCH (n:LinReg {ID:$ID}) RETURN n", parameters).columnAs("n");
    modelNode = n.next();
    byte[] model = (byte[])modelNode.getProperty("serializedModel");
    R = (SimpleRegression) convertFromBytes(model);
 } catch (Exception e) {
    throw new RuntimeException("no existing model for specified independent and dependent variables and model ID");
}
 try {
    byte[] byteModel = convertToBytes(R);
    modelNode.setProperty("serializedModel", byteModel);

} catch (IOException e) {
    throw new RuntimeException("something went wrong, model can't be linearized so new model not stored");
}
 private static ConcurrentHashMap<String, LRModel> models = new ConcurrentHashMap<>();
 static LRModel from(String name) {
    LRModel model = models.get(name);
    if (model != null) return model;
    throw new IllegalArgumentException("No valid LR-Model " + name);
}
 
 
Item 2086: 
const conn = new bcdbDriver.Connection(process.env.API_PATH,
     { ‘app_id’: process.env.APP_ID,
       ‘app_key’: process.env.APP_KEY
     });
 type Query {
   project(id: String): Project
   projects: [Project]
}
 {
  data: {
    ns: "ipld.ixo.dix"
  },
  id: "a2befa6e5e2293be64da4d0fad4cc2f3771a52cb830d7cf9e18e2f4b30508e67"
}
 type Project {
    id: String
    data: ProjectData
}
 type ProjectData {
    ns: String
}
 const typeDefs = [`
       type Query {
          project(id: String): Project
          projects: [Project]
       }
        type Project {
          id: String
          data: ProjectData
       }
        type ProjectData {
          ns: String
       }
        schema {
         query: Query
       }
`];
 const resolvers = {
   Query: {
      project: async (root, {id}) => {
         return (
            conn.searchAssets(id).then(project => {
               if(project.length == 1)
                  return project[0];
               return null
            })
         )
      },
       projects: async () => {
         return (
            conn.searchAssets('ipld.ixo.dix').then(projects => {
               return projects
            })
         )
      },
   },
}
 const schema = makeExecutableSchema({
   typeDefs,
   solvers
})
 
 
Item 2088: 
model = pickle.load(open("dtree_model.pkl", "rb"))
>>> model
DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')
 X = pd.read_csv("X_test.csv", header=0)
y = pd.read_csv("y_test.csv", header=0)
 
 
Item 2089: 
## Thetas from Scikit-Learn: 
## [21905.13153846] 
## [[104.0984541]]
## Thetas from manual gradient descent:
## [[7.28896918e-02]
##  [1.16260785e+02]]
 ## [1] "Thetas from lm():"
## (Intercept)   GrLivArea 
##   7298.3633    114.8567 
## [1] "Thetas from manual gradient descent:"
##              [,1]
## [1,]   0.07110339
## [2,] 118.48548260
 ## Thetas from Scikit-Learn:
## [21905.13153846]
## [[104.0984541]]
## Thetas from manual gradient descent:
## [[-2.77625824e-17]
##  [ 6.88445467e-01]]
 ## [1] "Thetas from lm():"
## (Intercept)   GrLivArea 
##   7298.3633    114.8567 
## [1] "Thetas from manual gradient descent:"
##              [,1]
## [1,] 2.124162e-16
## [2,] 7.226880e-01
 ## Thetas from Scikit-Learn:
## [21905.13153846]
## [[104.0984541]]
## Standardized thetas from manual gradient descent:
## [[-2.77625824e-17]
##  [ 6.88445467e-01]] 
## Rescaled thetas from manual gradient descent:
## 22967.755672321568
## 103.39879673499419
 ## [1] "Thetas from lm():"
## (Intercept)   GrLivArea 
##   7298.3633    114.8567 
## [1] "Thetas from manual gradient descent:"
##              [,1]
## [1,] 2.124162e-16
## [2,] 7.226880e-01
## [1] "Rescaled thetas from manual gradient descent:"
## [1] 8482.633
## [1] 114.081
 ## MSE: 2541490406
## MSE(vectorized): 2541490406
 ## [1] "MSE: 3174682821"
## [1] "MSE(vectorized): 3174682821"
 ## MSE: 2541490406
 ## [1] "MSE: 3174682821"
 ## MSE of simple linear regression: 2541490406
## MSE of multiple linear regression: 1930533820
 ## [1] "MSE of simple linear regression: 3174682821"
## [1] "MSE of multiple linear regression: 2535888809"
 ## X_train with degree=2:
## [[1.000000e+00 1.996000e+03 3.984016e+06]
##  [1.000000e+00 1.940000e+03 3.763600e+06]
##  [1.000000e+00 1.967000e+03 3.869089e+06]
##  ...
##  [1.000000e+00 1.968000e+03 3.873024e+06]
##  [1.000000e+00 1.972000e+03 3.888784e+06]
##  [1.000000e+00 1.941000e+03 3.767481e+06]]
## MSE with degree=1: 4223063920
## MSE with degree=2: 3769969724
 ## [1] "MSE with degree=1: 3931896041"
## [1] "MSE with degree=2: 3727047693"
 ## Best degree: 2
 ## [1] "Best degree: 2"
 ## bd_history
## 10.0    48
## 2.0     30
## 3.0     19
## 8.0      1
## 7.0      1
## 6.0      1
## dtype: int64
 ## bd_history
## 10  9  2  7  3  8  4 
## 53 17  9  9  7  4  1
 ## Best degree: 3
 ## [1] "Best degree: 8"
 
 
Item 2090: 
## [[121  10]
##  [ 63  21]]
## Accuracy: 66.05%
 ## [1] "Accuracy: 64.49%"
##    y_pred
##       0   1
##   0 120   2
##   1  74  18
 ## [[121  10]
##  [ 63  21]]
## Accuracy: 66.05%
 ## Binaray classification:
## [0 1]
## Multi-class classification:
## [0 1 2 3 4 5 6 7 8 9]
 ## [2 2 0 ... 9 6 3]
 ## [1]  3  3 10  3  9  8  3  2  1 10
 
 
Item 2094: 
sum (Damage modifier)*(#pokémon of that type) over every type
 
 
Item 2095: 
pip3 install xgboost
 
 
Item 2096: 
P(like | I) = 0.67
P(love | I) = 0.33
P(fruits | like) = P(Science | like) = 0.5
P(Mathematics | love) = 1
 (the, quick) --> brown
(quick, brown) --> fox
(brown, fox) --> jumps ...
 (lazy, dog) --> END
 the --> [quick, lazy]
{quick: 1/2, lazy: 1/2}
 and i should not be a king when you feel em
while he play piano
you better lose yourself and make her fall on her
and its absurd how people hang on every word
off a plank and
look i was but im
teeter totter caught up between being a father and a primadonna
at least once in a while
now who thinks their arms are long enough to one day grow up
but for me to try to get em motivated
 
 
Item 2097: 
similarity = np.exp(-beta * distance / distance.std())
 
 
Item 2098: 
mkdir my-project
cd my-project
 train --model-name baseline \
 --estimator-type DNN_REG \
 --indicators "Age" "Gender" "Occupation" "City_Category" "Stay_In_Current_City_Years" "Marital_Status" "Product_Category_1" \
 --embeddings '{"Product_ID":4, "User_ID":4}' \
 --hidden-units 100 200 100 \
 --training-steps 10000
 train --model-name emb_100 \
 --estimator-type DNN_REG \
 --indicators "Age" "Gender" "Occupation" "City_Category" "Stay_In_Current_City_Years" "Marital_Status" "Product_Category_1" \
 --embeddings '{"Product_ID":100, "User_ID":100}' \
 --hidden-units 100 200 100 \
 --training-steps 10000
 train --model-name emb_100_ageXms \
 --estimator-type DNN_REG \
 --indicators "Age" "Gender" "Occupation" "City_Category" "Stay_In_Current_City_Years" "Marital_Status" "Product_Category_1" \
 --embeddings '{"Product_ID":100, "User_ID":100}' \
 --crossings '["Age", "Marital_Status", "ind", 1000]' \
 --hidden-units 100 200 100 \
 --training-steps 10000
 train --model-name emb_100_ageXms_gendXocc \
 --estimator-type DNN_REG \
 --indicators "Age" "Gender" "Occupation" "City_Category" "Stay_In_Current_City_Years" "Marital_Status" "Product_Category_1" \
 --embeddings '{"Product_ID":100, "User_ID":100}' \
 --crossings '["Age", "Marital_Status", "ind", 1000]' '["Gender", "Occupation", "ind", 1000]' \
 --hidden-units 100 200 100 \
 --training-steps 10000
 train --model-name deeper_model \
 --estimator-type DNN_REG \
 --indicators "Age" "Gender" "Occupation" "City_Category" "Stay_In_Current_City_Years" "Marital_Status" "Product_Category_1" \
 --embeddings '{"Product_ID":100, "User_ID":100}' \
 --crossings '["Age", "Marital_Status", "ind", 1000]' '["Gender", "Occupation", "ind", 1000]' \
 --hidden-units 200 400 400 100 \
 --training-steps 10000
 
 
Item 2101: 
const slicedProducts = products.slice(0,5);
 let quickReplies = slicedProducts.map(product => {
  return {
    "content_type": "text",
    "title": product.title,
    "payload": product.title
  };
});
 quickReplies.push({
  "content_type": "text",
  "title": "Next",
  "payload": "nextslice_5_10"
});
 
 
Item 2103: 
max_depth = 7
regularisation_penality=10.
decay_penality=0.9
 
 
Item 2105: 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
 
Item 2106: 
myfiles = !ls
 
 
Item 2107: 
--------------------------King — Man + Woman = Queen
--------------------------
 
 
Item 2109: 
Schedule a demo for Infrrd OCR and grab your free trial today!
 
 
Item 2125: 
Data Set Characteristics:
    :Number of Instances: 150 (50 in each of three classes)
    :Number of Attributes: 4 numeric, predictive attributes and the class
    :Attribute Information:
        - sepal length in cm
        - sepal width in cm
        - petal length in cm
        - petal width in cm
        - class:
                - Iris-Setosa
                - Iris-Versicolour
                - Iris-Virginica
 Keys of iris_dataset: 
dict_keys(['target', 'data', 'feature_names', 'target_names', 'DESCR'])

Target names: ['setosa' 'versicolor' 'virginica']

Feature names: 
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']

Shape of data: (150, 4)

First 5 columns of data:
[[ 5.1  3.5  1.4  0.2]
 [ 4.9  3.   1.4  0.2]
 [ 4.7  3.2  1.3  0.2]
 [ 4.6  3.1  1.5  0.2]
 [ 5.   3.6  1.4  0.2]]

Shape of target: (150,)

Target:
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]

X_train shape: (112, 4), y_train shape: (112,)
X_test shape: (38, 4), y_test shape: (38,)
 
 
Item 2130: 
model_to_py(params['base_score'], model, 'xgb_model.py')
 
 
Item 2131: 
rest:
 python -m rasa_core.run 
-c rest 
-d data/servicing-bot-en/models/dialogue 
-u data/servicing-bot-en/models/servicing-bot-en/model-en 
— endpoints endpoints.yml 
— credentials credentials.yml
 POST /webhooks/rest/webhook HTTP/1.1
Host: localhost:5005
Content-Type: application/json
Cache-Control: no-cache
Postman-Token: 20a797ac-7b59-28cd-4a6e-0d45a66c18d9
 {
 "sender":"John Doe",
 "message":"i have lost my credit card"
 
}
 [
    {
        "recipient_id": "John Doe",
        "text": "We have confirmed a new credit card for you"
    }
]
 rasa_en_1  | 172.22.0.1 - - [2018-09-19 14:13:11] "POST /webhooks/rest/webhook HTTP/1.1" 200 275 0.209853
action_1   | DEBUG:rasa_core_sdk.executor:Received request to run 'card_lost'
action_1   | DEBUG:rasa_core_sdk.executor:Successfully ran 'card_lost'
nlg_1      | 172.22.0.2 - - [2018-09-19 14:13:20] "POST /nlg HTTP/1.1" 200 163 0.001777
rasa_en_1  | 172.22.0.1 - - [2018-09-19 14:13:20] "POST /webhooks/rest/webhook HTTP/1.1" 200 285 0.037939
action_1   | 172.22.0.2 - - [2018-09-19 14:13:20] "POST /webhook HTTP/1.1" 200 214 0.004351
action_1   | DEBUG:rasa_core_sdk.executor:Received request to run 'card_lost'
action_1   | DEBUG:rasa_core_sdk.executor:Successfully ran 'card_lost'
nlg_1      | 172.22.0.2 - - [2018-09-19 14:17:01] "POST /nlg HTTP/1.1" 200 163 0.001800
rasa_en_1  | 172.22.0.1 - - [2018-09-19 14:17:01] "POST /webhooks/rest/webhook HTTP/1.1" 200 287 0.038043
 rest:
 facebook:
 verify: "rasa-bot"
 secret: "app-secret"
 page-access-token: "pageaccestoken"
 python -m rasa_core.run  
-c rest 
-c facebook  
-d data/servicing-bot-en/models/dialogue 
-u data/servicing-bot-en/models/servicing-bot-en/model-en 
--endpoints endpoints.yml 
--credentials credentials.yml
 ngrok http 5005
 https://779ee0c7.ngrok.io/webhooks/facebook/webhook
 rasa_en_1 | 172.23.0.1 — — [2018–09–19 14:44:43] “POST /webhooks/facebook/webhook HTTP/1.1” 200 154 0.010318
nlg_1 | 172.23.0.2 — — [2018–09–19 14:44:54] “POST /nlg HTTP/1.1” 200 130 0.002343
rasa_en_1 | 2018–09–19 14:44:54 INFO rasa_core.channels.facebook — Sending message: hey there!
rasa_en_1 | 172.23.0.1 — — [2018–09–19 14:44:55] “POST /webhooks/facebook/webhook HTTP/1.1” 200 154 0.548130
action_1 | DEBUG:rasa_core_sdk.executor:Received request to run ‘card_lost’
action_1 | DEBUG:rasa_core_sdk.executor:Successfully ran ‘card_lost’
nlg_1 | 172.23.0.2 — — [2018–09–19 14:45:03] “POST /nlg HTTP/1.1” 200 163 0.001459
rasa_en_1 | 2018–09–19 14:45:03 INFO rasa_core.channels.facebook — Sending message: We have confirmed a new credit card for you
rasa_en_1 | 172.23.0.1 — — [2018–09–19 14:45:04] “POST /webhooks/facebook/webhook HTTP/1.1” 200 154 0.618583
 
 
Item 2137: 
10,000 requests X 550 characters/request = 60,000 units 
60,000 X $0.0001 per unit = $6
 10,000 requests X 550 characters/request = 10,000 units 
10,000 X $1 per 1,000 units = $10
 10,000 requests X 550 characters/request = 10,000 NLUs 
10,000 X $0.003 per NLU = $30
 10,000 requests X 550 characters/request = 10,000 transactions
10,000 transactions requires “Standard S0 pricing tier” = $74.71
 
 
Item 2138: 
brew install python3
 pip3 install virtualenv
 virtualenv -p python3 <path-to-our-dir>
 source <path-to-our-dir>/bin/activate
 get out of the virtual env run:
 pip3 install tensorflow
 pip3 install jupyter
 y = W*x + B
 
 
Item 2139: 
python retrain.py --how_many_training_steps 500 -- output_graph=./retrained_graph.pb --output_labels=./retrained_labels.txt --image_dir=./training_data
 INFO:tensorflow:Looking for images in 'Red Apple'WARNING:tensorflow:WARNING: Folder has less than 20 images, which may cause issues.
INFO:tensorflow:Looking for images in 'Red Ball'
WARNING:tensorflow:WARNING: Folder has less than 20 images, which may cause issues.
>> Downloading inception-2015-12-05.tgz 100.0%
INFO:tensorflow:Successfully downloaded inception-2015-12-05.tgz 88931400 bytes.
Extracting file from ./inception-march-3-2018/inception-2015-12-05.tgz
Model path: ./inception-march-3-2018/classify_image_graph_def.pb
 INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_142.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_143.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_144.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_145.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_146.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_147.jpg_inception_v3.txt
INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_149.jpg_inception_v3.txt
 INFO:tensorflow:Creating bottleneck at ./bottlenecks/Red Apple/redapple_136.jpg_inception_v3.txt
Not a JPEG file: starts with 0x89 0x50
Traceback (most recent call last):
File "retrain.py", line 1486, in <module>
tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 File "/Users/barnhart/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/platform/app.py", line 48, in run
_sys.exit(main(_sys.argv[:1] + flags_passthrough))
File "retrain.py", line 1294, in main
export_model(model_info, class_count, FLAGS.saved_model_dir)
File "/Users/barnhart/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py", line 1214, in __exit__
exec_type, exec_value, exec_tb)
File "/Users/barnhart/anaconda/envs/py35/lib/python3.5/contextlib.py", line 77, in __exit__
self.gen.throw(type, value, traceback)
File "/Users/barnhart/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py", line 3625, in get_controller
yield default
File "retrain.py", line 1187, in main
bottleneck_tensor, FLAGS.architecture)
File "retrain.py", line 500, in cache_bottlenecks
resized_input_tensor, bottleneck_tensor, architecture)
File "retrain.py", line 442, in get_or_create_bottleneck
bottleneck_tensor)
File "retrain.py", line 397, in create_bottleneck_file
str(e)))
 RuntimeError: Error during processing file ./training_data/Red Apple/redapple_136.jpg (Invalid JPEG data, size 4068
 [[Node: DecodeJpeg_1 = DecodeJpeg[acceptable_fraction=1, channels=3, dct_method="", fancy_upscaling=true, ratio=1, try_recover_truncated=false, _device="/job:localhost/replica:0/task:0/cpu:0"](_recv_DecodeJPGInput_0)]]
 INFO:tensorflow:2018-03-05 12:09:05.076961: Step 0: Train accuracy = 61.0%
INFO:tensorflow:2018-03-05 12:09:05.077692: Step 0: Cross entropy = 0.599025
INFO:tensorflow:2018-03-05 12:09:05.213300: Step 0: Validation accuracy = 51.0% (N=100)
INFO:tensorflow:2018-03-05 12:09:06.192184: Step 10: Train accuracy = 100.0%
INFO:tensorflow:2018-03-05 12:09:06.192382: Step 10: Cross entropy = 0.253409
INFO:tensorflow:2018-03-05 12:09:06.296140: Step 10: Validation accuracy = 100.0% (N=100)
INFO:tensorflow:2018-03-05 12:09:07.352922: Step 20: Train accuracy = 100.0%
INFO:tensorflow:2018-03-05 12:09:07.353142: Step 20: Cross entropy = 0.156156
INFO:tensorflow:2018-03-05 12:09:07.464078: Step 20: Validation accuracy = 100.0% (N=100)
INFO:tensorflow:2018-03-05 12:09:08.432503: Step 30: Train accuracy = 100.0%
INFO:tensorflow:2018-03-05 12:09:08.432636: Step 30: Cross entropy = 0.103196
INFO:tensorflow:2018-03-05 12:09:08.526025: Step 30: Validation accuracy = 100.0% (N=100)
INFO:tensorflow:2018-03-05 12:09:09.487891: Step 40: Train accuracy = 100.0%
INFO:tensorflow:2018-03-05 12:09:09.488104: Step 40: Cross entropy = 0.085478
 INFO:tensorflow:2018-03-05 12:09:55.249556: Step 499: Train accuracy = 100.0%
INFO:tensorflow:2018-03-05 12:09:55.249698: Step 499: Cross entropy = 0.007347
INFO:tensorflow:2018-03-05 12:09:55.341729: Step 499: Validation accuracy = 100.0% (N=100)
Model path: ./inception-march-3-2018/classify_image_graph_def.pb
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:Final test accuracy = 100.0% (N=5)
Model path: ./inception-march-3-2018/classify_image_graph_def.pb
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:Froze 2 variables.
Converted 2 variables to const ops.
Model path: ./inception-march-3-2018/classify_image_graph_def.pb
INFO:tensorflow:Restoring parameters from /tmp/_retrain_checkpoint
INFO:tensorflow:No assets to save.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: b'/tmp/saved_models/1/saved_model.pb'
 python label_image.py ~/Dev/experiment/tensorflow/ic_test_data/red_apple/redapple_003.jpg
 red apple (score = 0.99650)
red ball (score = 0.00350)
 red apple (score = 0.60262)
red ball (score = 0.39738)
 
 
Item 2142: 
I am Abhishek. I am here... there.... Everywhere...
 Medium | Twitter | Facebook | Quora | LinkedIn | E-mail
 
 
Item 2150: 
opencv_createsamples.exe -vec VEC_FILE.vec -w 20 -h 20 -info gabarito.data -num 2500
 opencv_traincascade -data OUTPUT_FOLDER -vec VEC_FILE.vec -bg negatives.data -minHitRate 0.999 -maxFalseAlarmRate 0.5 -numPos 2000 -numNeg 4000 -w 20 -h 20 -mode ALL -featureType Haar -maxWeakCount 1000
 
 
Item 2151: 
Todas as vezes em 2016 no qual o COD_ORG_LOTACAO = COD_ORGSUP_LOTACAO e COD_UORG_LOTACAO é nulo

2016-01-31
 76323
 2016-02-29
 75956
 2016-03-31
 79760
 2016-04-30
 80240
 2016-05-31
 79879
 2016-06-30
 79661
 2016-07-31
 79657
 2016-08-31
 79737
 2016-09-30
 79606
 2016-10-31
 79355
 2016-11-30
 79373
 2016-12-31
 79674

Todas as vezes em 2016 no qual o COD_ORG_LOTACAO = COD_ORGSUP_LOTACAO e COD_UORG_LOTACAO é nulo

2017-01-31
 79205
 2017-02-28
 79137

Todas as vezes em 2016 no qual o COD_ORG_LOTACAO = COD_ORGSUP_LOTACAO e COD_UORG_LOTACAO não é nulo

2016-01-31
 123023
 2016-02-29
 122833
 2016-03-31
 122325
 2016-04-30
 121992
 2016-05-31
 121630
 2016-06-30
 121091
 2016-07-31
 120624
 2016-08-31
 120178
 2016-09-30
 119820
 2016-10-31
 119270
 2016-11-30
 118984
 2016-12-31
 118865

Todas as vezes em 2017 no qual o COD_ORG_LOTACAO = COD_ORGSUP_LOTACAO
 e COD_UORG_LOTACAO não é nulo

2017-01-31
 118135
 2017-02-28
 120985
 a) OS VALORES DEMONSTRADOS ACIMA INCLUEM REMUNERAÇÕES PERCEBIDAS
 A TITULO DE APOSENTADORIA/PENSÃO - ocorre quando o servidor possui no campo situação_vinculo
 o valor "aposentado".

 b) SERVIDOR EM EXERCICIO NO EXTERIOR. - ocorre quando um servidor trabalha para algum uorg
 do MINISTÉRIO DE RELAÇÕES EXTERIORES.

 c) SERVIDOR COM DESCONTO QUE INFLUENCIA NO CALCULO DO ABATE TETO - ocorre quando o servidor
 possui a REMUNERAÇÃO_BRUTA acima do teto, tendo um abate dessa remuneração

 d) SERVIDOR JÁ RECEBE PELO TETO CONSTITUCIONAL NO ÓRGÃO DE ORIGEM
 - ocorre quando a remuneração bruta é igual ao abate que ele recebe.

 e) SERVIDOR JÁ RECEBE PELO TETO CONSTITUCIONAL NO ÓRGÃO DE ORIGEM
 - ocorre quando um servidor possui a remuneração total bruta igual ao abate do teto.

 f) ABONO DE PERMANÊNCIA DE MESES ANTERIORES - EMENDA CONSTITUCIONAL Nº 41, ART. 2º, §5º
 - segundo a emenda, ocorre quando o servidor solicita aposentadoria voluntaria,
 o mesmo servidor fica no máximo até 2 meses seguidos com essa observação,
 no entanto continua na tabela de cadastro e remuneração.

 g) LIMITE DE REMUNERAÇÃO EXCEDIDO DEVIDO AO RECEBIMENTO DO ABONO DE PERMANÊNCIA
 DO MÊS CORRENTE - EMENDA CONSTITUCIONAL Nº 41, ART. 2º, §5º - ocorre quando possui
 a remuneração bruta acima do teto e não possui abate, devido a emenda constitucional.

 h) OS VALORES PAGOS PELO ÓRGÃO/EMPRESA DE ORIGEM AO SERVIDOR NÃO ESTÃO INFORMADOS -
 Não foi possível identificar um padrão, no entanto a observação pode ser auto-explicativa

 i) SERVIDOR PERCEBE VENCIMENTOS ACIMA DO TETO CONSTITUCIONAL POR DECISÃO JUDICIAL
 - Servidores recebem a remuneração bruta acima do teto,
 no entanto alguns ainda tem remuneração após deduções abaixo do teto.

 j) OS VALORES DEMONSTRADOS ACIMA INCLUEM REMUNERAÇÕES PERCEBIDAS
 PELO SERVIDOR NO SEU ÓRGÃO/EMPRESA
 DE ORIGEM - Observação com a maior quantidade de aparições, sendo 5360 registros,
 informa apenas que o valor recebido inclui a empresa de origem,
 mas não informa a empresa em questão.
 MINISTÉRIOS
TRIBUNAIS
BANCOS
GOVERNOS ESTADUAIS
PREFEITURAS
EMPRESAS ESTATAIS
ESTADOS / MUNICIPIOS / EMPRESAS
PROCURADORIA GERAL DA REPUBLICA
PRESIDENCIA DA REPUBLICA
SENADO FEDERAL
CAMARA DOS DEPUTADOS
OUTROS
 INTERSEÇÃO DOS ÓRGÃOS DA TABELA DE PAGAMENTOS COM A TABELA DE SERVIDORES

SOMENTE 2016

MINISTERIO DO DESENV,IND. E COMERCIO EXTERIOR
MINISTERIO DO DESENVOLVIMENTO AGRARIO
MINISTERIO DOS TRANSPORTES
MINISTERIO DO TRABALHO E EMPREGO

SOMENTE MÊS 1 DE 2016

MINISTERIO DA PESCA E AQUICULTURA

SOMENTE MÊS 3 DE 2016

M.DAS MULH.,DA IG.RACIAL DA JUVENT.E DIR.HUM.

ATÉ MÊS 7 DE 2016

MINISTERIO DA CIENCIA, TECNOLOGIA E INOVACAO
MINISTERIO DO DESENV. SOCIAL E COMBATE A FOME
MINISTERIO DO PLANEJAMENTO,ORCAMENTO E GESTAO
MINISTERIO DO TRABALHO E PREVIDENCIA SOCIAL

SOMENTE MÊS 08,09,10,11,12 DE 2016

MINISTERIO DO DESENV. SOCIAL E AGRáRIO

MÊS 08-2016 EM DIANTE. 2017 TODO

MINIST. DO PLANEJAMENTO, DESENVOLV. E GESTAO
MINIST.DA CIENCIA,TECNOL.,INOV.E COMUNICACOES
MINISTERIO DO TRABALHO

TODO MÊS 2016 - JAN-2017

MINISTERIO DAS COMUNICACOES

TODO MÊS 2016, 2017 TODO

MINIST. DA AGRICUL.,PECUARIA E ABASTECIMENTO
MINISTERIO DA CULTURA
MINISTERIO DA DEFESA
MINISTERIO DA EDUCACAO
MINISTERIO DA FAZENDA
MINISTERIO DA INTEGRACAO NACIONAL
MINISTERIO DA PREVIDENCIA SOCIAL
MINISTERIO DA SAUDE
MINISTERIO DAS CIDADES
MINISTERIO DAS RELACOES EXTERIORES
MINISTERIO DE MINAS E ENERGIA
MINISTERIO DO ESPORTE
MINISTERIO DO MEIO AMBIENTE
MINISTERIO DO TURISMO
PRESIDENCIA DA REPUBLICA
MINISTERIO DA JUSTICA

ANO 2017

MINIST. DA INDUSTRIA, COM.EXTERIOR E SERVICOS
MINIST. DA TRANSPARENCIA, FISCALIZACAO E CGU
MINIST.DOS TRANSP.,PORTOS E AVIACAO CIVIL
MINISTERIO DA JUSTICA E SEGURANçA PUBLICA
MINISTERIO DO DESENVOLVIM. SOCIAL E AGRARIO
MINISTERIO DO DESENVOLVIM. SOCIAL E AGRARIO
 Geral - Exibir Órgão que gasta mais
 -(ÓRGÃO SUP - subcategorias com órgãos de lotação)

Geral - Quantidade de servidores por ORGÃO.

Geral - Exibir gráfico com os gastos dos meses.

Geral - Exibir gráfico de gastos por Estado.

Específico - Exibir gastos Orgão sup no estado
 (Ex: Quanto o Estado recebe no Ministério do Trabalho,
 Educação)

Específico - Exibir gastos gerais por CARGO
 - (Professores, Médicos) ** INVESTIGAR

Específico - Exibir gastos gerais por CARGO POR ESTADO
 - (Professores, Médicos) ** INVESTIGAR

Geral - Total de gastos com o Exterior (Em dolar)
 (* GRÁFICO) - por mês.

Geral - Quantidade de pessoas por mês.

Específico - Exibir cargos (10...) com maior remuneração
 - (De acordo com o órgão superior
 -(MINISTÉRIO DA EDUCAÇÃO, MINISTÉRIO DA SAÚDE,
 GOVERNO DO ESTADO DO CEARÁ ...) )

Específico Exibir cargos (10...) com maior remuneração
 de acordo com o órgão superior - Por estado.

Geral - Exibir gastos de acordo com a SITUAÇÃO VÍNCULO
 (Contratado, Concursado) * SEGUINDO A HIERARQUIA ACIMA
 - Por ORGÃO, ORGÃO SUP

Geral - Exibir gastos de acordo com a OBSERVAÇÃO:
 pessoas ativos, inativos, Aposentados, 
Pensionistas.

Geral - Exibir gastos gerais em honorários por mês.

Específico - (VER EMPRESAS QUE RECEBEM MAIS)

Geral - Quanto que é o gasto por cargo comissionados
 por função, e somente cargo comissionados que não são
 concursado (Ex: nomeação - Cargos Políticos)
 ** INVESTIGAR

** INVESTIGAR: Fazer uma exploração para entender melhor
 a pergunta.
 Buscar pagamentos por favorecidos;
Buscar pagamentos por intervalo de tempo(datas); 
Buscar a remuneração de acordo com uma função/subfunção; Ex : Administração;
Buscar os gastos total por programa/linguagem cidadã;
Demostrar a diferença de gastos de um orgão_subordinado atraves de meses ou anos;
Buscar todos funcionários de um programa/orgão_subordinado;
Ranks com os orgãos superiores que gastam mais/ subfunções com maior remuneração; 
Buscar por valores em elemento dispesas;
 Ex : "Auxílio Financeiro a Pesquisadores"
 Índice hash no atributo NOME_FAVORECIDO da tabela FAVORECIDO;

Índice Btree nos atributos ANO E MÊS da tabela DATA; 

Índice hash no atributo NOME_SUBFUNCAO da tabela SUBFUNCAO;

Índice Btree no atributo NOME_PROGRAMA da tabela PROGRAMA; 

Índice hash no atributo NOME_ORGAO_SUBORDINADO DA TABELA ORGAO_SUBORDINADO;

Índice Btree no atributo VALOR da tabela PAGAMENTO;
 MINISTÉRIOS
TRIBUNAIS
BANCOS
GOVERNOS ESTADUAIS
PREFEITURAS
EMPRESAS ESTATAIS
ESTADOS / MUNICIPIOS / EMPRESAS
PROCURADORIA GERAL DA REPUBLICA
PRESIDENCIA DA REPUBLICA
SENADO FEDERAL
CÂMARA DOS D
 MINISTÉRIOS:
 15000.0,MINISTERIO DA EDUCACAO
 40108.0,"MINIST.DA CIENCIA, TECNOLOGIA E INOVACAO"
 25000.0,MINISTERIO DA SAUDE
 13000.0,"MINIST.DA AGRICULTURA,PECUARIA E ABAST."
 20000.0,MINISTERIO DA JUSTICA
 17000.0,MINISTERIO DA FAZENDA
 32000.0,MINISTERIO DE MINAS E ENERGIA
 55000.0,MINISTERIO DESENVOLV. SOCIAL E AGRÁRIO
 40105.0,MINISTERIO DA DEFESA
 35000.0,MINISTERIO DAS RELACOES EXTERIORES
 56000.0,MINISTERIO DAS CIDADES
 26000.0,MINISTERIO DO TRABALHO E EMPREGO
 23000.0,MINISTERIO DA PREVIDENCIA SOCIAL
 20113.0,MINISTERIO DO PLANEJ. DESENV. E GESTAO
 49000.0,MINISTERIO DOS TRANSPORTES
 42000.0,MINISTERIO DO DESENVOLVIMENTO AGRARIO
 40112.0,MINISTERIO DA INTEGRACAO NACIONAL
 29000.0,MINISTERIO DO ESPORTE
 40107.0,MINISTERIO DA CULTURA
 40111.0,MINISTERIO DO MEIO AMBIENTE
 41000.0,MINISTERIO DAS COMUNICACOES
 59000.0,MINISTERIO DA TRANSP. FISCAL. E CGU
 28000.0,MINIST.DO DESENV.INDUST.E COMER.EXTERIOR
 54000.0,MINISTERIO DO TURISMO
 90100.0,MINISTERIO PUBLICO FEDERAL
 90400.0,MINIST. PUBLICO DO DF E TERRITORIOS
 90200.0,MINISTERIO PUBLICO DO TRABALHO
 90150.0,MINISTERIO PUBLICO DA UNIAO

TRIBUNAIS:
 101.0,SUPERIOR TRIBUNAL DE JUSTICA
 99048.0,TRIBUNAL DE JUSTICA DO DF E TERRITORIOS
 99047.0,TRIBUNAL SUPERIOR ELEITORAL
 99027.0,TRIBUNAL REGIONAL ELEITORAL - MA
 99052.0,TRIBUNAL REGIONAL DO TRABALHO-MT
 99133.0,TRIBUNAL DE JUSTICA
 99000.0,TRIBUNAL REGIONAL ELEITORAL - MG
 99032.0,TRIBUNAL REGIONAL ELEITORAL - DF
 99026.0,TRIBUNAL REGIONAL ELEITORAL - CE
 166.0,TRIBUNAL REGIONAL ELEITORAL - SP
 90000.0,TRIBUNAL DE CONTAS DA UNIAO
 4000.0,TRIBUNAL SUPERIOR DO TRABALHO
 99044.0,TRIBUNAL DE JUSTICA DO EST DE RONDONIA
 164.0,TRIBUNAL REGIONAL ELEITORAL
 99001.0,TRIBUNAL REGIONAL DO TRABALHO - MG
 99034.0,TRIBUNAL REGIONAL FEDERAL 1A REGIAO / DF
 167.0,TRIBUNAL REGIONAL ELEITORAL - RO
 99021.0,TRIBUNAL REGIONAL ELEITORAL - PB
 99069.0,TRIBUNAL REGIONAL FEDERAL 3A REGIAO/SP
 99905.0,TRIBUNAL REGIONAL DO TRABALHO
 10000.0,SUPREMO TRIBUNAL FEDERAL

BANCOS:
 25201.0,BANCO CENTRAL DO BRASIL
 25281.0,BANCO DO BRASIL S/A
 25220.0,CAIXA ECONOMICA FEDERAL
 45282.0,BANCO NAC. DE DESENV. ECONOMICO E SOCIAL
 25282.0,BANCO DO NORDESTE DO BRASIL
 20502.0,FINANCIADORA DE ESTUDOS E PROJETOS

GOVERNOS ESTADUAIS:
 93520.0,GOVERNO DO ESTADO RONDONIA
 94920.0,GOVERNO DO ESTADO DA BAHIA
 99072.0,GOVERNO DO DISTRITO FEDERAL
 97400.0,GOVERNO DO DISTRITO FEDERAL
 93420.0,GOVERNO DO ESTADO DO PARA
 97120.0,GOVERNO DO ESTADO DO MATO GROSSO DO SUL
 96320.0,GOVERNO DO ESTADO DO RIO GRANDE DO SUL
 95320.0,GOVERNO DO ESTADO DO RIO DE JANEIRO
 95420.0,GOVERNO DO ESTADO DE SAO PAULO
 93720.0,GOVERNO DO ESTADO DE TOCANTINS
 94620.0,GOVERNO DO ESTADO DE PERNAMBUCO
 94520.0,GOVERNO DO ESTADO DA PARAIBA
 93620.0,GOVERNO DO ESTADO DE RORAIMA
 96120.0,GOVERNO DO ESTADO DO PARANA
 94220.0,GOVERNO DO ESTADO DO PIAUI
 94320.0,GOVERNO DO ESTADO DO CEARA
 93120.0,GOVERNO DO ESTADO DO ACRE
 97320.0,GOVERNO DO ESTADO DE GOIAS
 93320.0,GOVERNO DO ESTADO DO AMAZONAS
 95120.0,GOVERNO DO ESTADO DE MINAS GERAIS
 97220.0,GOVERNO DO ESTADO DO MATO GROSSO
 94820.0,GOVERNO DO ESTADO DE SERGIPE
 93220.0,GOVERNO DO ESTADO DO AMAPA
 94720.0,GOVERNO DO ESTADO DE ALAGOAS
 95220.0,GOVERNO DO ESTADO DO ESPIRITO SANTO
 96220.0,GOVERNO DO ESTADO DE SANTA CATARINA
 157.0,GOVERNO DO ESTADO DO MARANHAO
 94420.0,GOVERNO DO ESTADO DO RIO GRANDE DO NORTE

PREFEITURAS:
 99054.0,PREFEITURA MUNICIPAL
 99045.0,PREFEITURA MUNICIPAL DE VITORIA/ES
 99132.0,PREFEITURA DE SAO LUIZ/MA
 99068.0,PREFEITURA MUNICIPAL DE RECIFE/PE
 99040.0,PREFEITURA MUNICIPAL DE FORTALEZA / CE
 99033.0,PREFEITURA MUNICIPAL DE PORTO VELHO-RO

EMPRESAS ESTATAIS:
 25211.0,CASA DA MOEDA DO BRASIL
 99061.0,COMP DOCAS DO ESTADO DO RIO DE JANEIRO
 99058.0,COMPANHIA DOCAS DE SAO PAULO
 99062.0,COMP DOCAS DO ESTADO DA BAHIA
 91084.0,PETROBRAS - HOLDING
 91081.0,ELETROBRAS - HOLDING
 91082.0,TELEBRAS - HOLDING
 99011.0,EMPRESA BRAS. INFR. EST. AEROPORTUARIA
 49280.0,EMPRESA BRAS. DE CORREIOS E TELEGRAFOS
 99014.0,INDUSTRIA DE MATERIAL BELICO DO BRASIL
 73,32201.0,CENTRO DE PESQUISAS DE ENERGIA ELETRICA
 106,80208.0,EMPRESA DE PORTOS DO BRASIL S/A
 50,99063.0,ADMINISTRACAO DO PORTO DE MANAUS
 31,99059.0,CENTRAIS ELETRICAS NORTE BRASIL
 78,99060.0,COMPANHIA ENERGETICA DE BRASILIA

ESTADOS / MUNICIPIOS / EMPRESAS:
 30,99991.0,ESTADOS / MUNICIPIOS / EMPRESAS
 32,99990.0,ESTADOS/MUNICIPIOS/ADM.DIR-AUT/FUNDACOES

OUTROS:
 45,98000.0,ORGAO CENTRAL DO SIPEC
 66,99993.0,EMPRESAS PUBLICAS/CLT
 82,97401.0,CAMARA LEGISLATIVA DO DF
 52,20104.0,SECRETARIA DE ASSUNTOS ESTRATEGICOS / PR

58,99004.0,PROCURADORIA GERAL DA REPUBLICA

8,20101.0,PRESIDENCIA DA REPUBLICA

54,99016.0,SENADO FEDERAL

60,99015.0,CAMARA DOS DEPUTADOS

Categorias:
 MINISTÉRIOS
 TRIBUNAIS
 BANCOS
 GOVERNOS ESTADUAIS
 PREFEITURAS
 EMPRESAS ESTATAIS
 ESTADOS / MUNICIPIOS / EMPRESAS
 PROCURADORIA GERAL DA REPUBLICA
 PRESIDENCIA DA REPUBLICA
 SENADO FEDERAL
 CAMARA DOS DEPUTADOS
 OUTROS
 
 
Item 2152: 
def kanban_webhook(event, context):
    input_body = json.loads(event['body'])
     action = input_body["action"]
    action_type = action["type"]
     if action_type == "createCard":
        list_name, card_name = get_create_card(action["data"])
    elif action_type == "updateCard":
        list_name, card_name = get_update_card(action["data"])
     kanban_list = ["DOING", "BREAK", "DONE"]
    if list_name in kanban_list:
        payload = make_payload(action=list_name, msg=card_name)
        r = send_to_kino({"text": payload})
    ...
 def KANBAN_handle(self, event):
    toggl_manager = TogglManager()
     action = event['action']
    description = event['msg']
    if action.endswith("DOING"):
        toggl_manager.timer(
            description=description,
            doing=True,
            done=False)
    elif action.endswith("BREAK"):
        toggl_manager.timer(doing=False, done=False)
    elif action.endswith("DONE"):
        toggl_manager.timer(doing=False, done=True) 
                                ## connect with Todoist
 
 
Item 2154: 
https://api.clarifai.com/v2/
 C:\go_projects\go\src\github.com\SatishTalim\clarifai
 go run mycf.go
 type TokenResp struct {
        AccessToken string `json:”access_token”`
        ExpiresIn   int    `json:”expires_in”`
        Scope       string `json:”scope”`
        TokenType   string `json:”token_type”`
}
 func requestAccessToken() (string, error) {
 type error interface {
        Error() string
}
 type Values map[string][]string

 func (v Values) Set(key, value string)

 func (v Values) Encode() string

 func NewReader(s string) *Reader

 req.Header.Set(“Content-Type”, “application/x-www-form-urlencoded”)
 AccessToken = WS2bEYTrEeKxSQilf8JqmxWMmlqL7U
 https://api.clarifai.com/v1/tag/?access_token=1P3LNShlwE1HpL2xd0ZLL2rrMKMDzz&url=https://samples.clarifai.com/metro-north.jpg
 {"status_code": "OK", "status_msg": "All images in request have completed successfully. ", "meta": {"tag": {"timestamp": 1463478660.484501, "model": "general-v1.3", "config": "34fb1111b4d5f67cf1b8665ebc603704"}}, "results": [{"docid": 17763255747558799694, "url": "https://samples.clarifai.com/metro-north.jpg", "status_code": "OK", "status_msg": "OK", "local_id": "", "result": {"tag": {"concept_ids": ["ai_HLmqFqBf", "ai_fvlBqXZR", "ai_Xxjc3MhT", "ai_6kTjGfF6", "ai_RRXLczch", "ai_VRmbGVWh", "ai_SHNDcmJ3", "ai_jlb9q33b", "ai_46lGZ4Gm", "ai_tr0MBp64", "ai_l4WckcJN", "ai_2gkfMDsM", "ai_CpFBRWzD", "ai_786Zr311", "ai_6lhccv44", "ai_971KsJkn", "ai_WBQfVV0p", "ai_dSCKh8xv", "ai_TZ3C79C6", "ai_VSVscs9k"], "classes": ["train", "railway", "transportation system", "station", "train", "travel", "tube", "commuter", "railway", "traffic", "blur", "platform", "urban", "no person", "business", "track", "city", "fast", "road", "terminal"], "probs": [0.9989112019538879, 0.9975532293319702, 0.9959157705307007, 0.9925730228424072, 0.9925559759140015, 0.9878921508789062, 0.9816359281539917, 0.9712483286857605, 0.9690325260162354, 0.9687051773071289, 0.9667078256607056, 0.9624242782592773, 0.960752010345459, 0.9586490392684937, 0.9572030305862427, 0.9494642019271851, 0.940894365310669, 0.9399334192276001, 0.9312160611152649, 0.9230834245681763]}}, "docid_str": "76961bb1ddae0e82f683c2fd17a8794e"}]}
 type TagResp struct {
        StatusCode string `json:”status_code”`
        StatusMsg string `json:”status_msg”`
        Meta struct {
                Tag struct {
                        Timestamp float64 `json:”timestamp”`
                        Model string `json:”model”`
                        Config string `json:”config”`
                } `json:”tag”`
        } `json:”meta”`
        Results []struct {
                Docid uint64 `json:”docid”`
                URL string `json:”url”`
                StatusCode string `json:”status_code”`
                StatusMsg string `json:”status_msg”`
                LocalID string `json:”local_id”`
                Result struct {
                       Tag struct {
                           ConceptIds []string `json:”concept_ids”`
                           Classes []string `json:”classes”`
                           Probs []float64 `json:”probs”`
                       } `json:”tag”`
                } `json:”result”`
                DocidStr string `json:”docid_str”`
        } `json:”results”`
}
 Tag0 = train
Tag1 = railway
Tag2 = transportation system
Tag3 = station
Tag4 = train
Tag5 = travel
Tag6 = tube
Tag7 = commuter
Tag8 = railway
Tag9 = traffic
Tag10 = blur
Tag11 = platform
Tag12 = urban
Tag13 = no person
Tag14 = business
Tag15 = track
Tag16 = city
Tag17 = fast
Tag18 = road
Tag19 = terminal
 
 
Item 2155: 
k = 1
Volume of hypercube = 2
Fraction of points inside hypersphere = 1.0
-----------------------------------------
k = 2
Volume of hypercube = 4
Fraction of points inside hypersphere = 0.784976
-----------------------------------------
k = 3
Volume of hypercube = 8
Fraction of points inside hypersphere = 0.52376
-----------------------------------------
 ...
 k = 17
Volume of hypercube = 131072
Fraction of points inside hypersphere = 1e-06
-----------------------------------------
k = 18
Volume of hypercube = 262144
Fraction of points inside hypersphere = 0.0
-----------------------------------------
k = 19
Volume of hypercube = 524288
Fraction of points inside hypersphere = 0.0
-----------------------------------------
 
 
Item 2158: 
x = spline_s_x(s) + d * spline_s_dx(s)
y = spline_s_y(s) + d * spline_s_dy(s)
 typedef function<double (const Vehicle&, const vector<Vehicle>&,  const Trajectory&, const State&, const double&)> CostFunction;
 
 
Item 2159: 
{
  "Authorization": "Bearer PASTE_YOUR_SENGRID_API_SECRET"
}
 {
  "personalizations": [
    {
      "to": [
        {
          "email": "hello@recime.io"
        }
      ],
      "subject": "Requested Support for Weixin Bot"
    }
  ],
  "from": {
    "email": "{{email}}"
  },
  "content": [
    {
      "type": "text/html",
      "value": "{{desc}}"
    }
  ]
}
 
 
Item 2165: 
['beach', 'lagoon', 'coast', 'ocean', 'islet']
 
 
Item 2170: 
Logistic Regression pipeline test accuracy: 0.933
Support Vector Machine pipeline test accuracy: 0.900
Decision Tree pipeline test accuracy: 0.867
Classifier with best accuracy: Logistic Regression
Saved Logistic Regression pipeline to file
 Traducción:
 Precisión de la prueba de la tubería de regresión logística: 0,933 Exactitud de la prueba de tuberías de Vector de soporte: 0,900 Exactitud de la prueba de tuberías del Árbol de decisiones: 0,867 Clasificador con la mejor precisión: Regresión logística Guardado de la tubería de regresión logística para archivar
 
 
Item 2174: 
popt_all_stations = []
row_headers = []

for i in station_ids:
    s = station_availability(i, n_bikes= n_bikes, dow= dow)       
    popt_single = []
    for j in start_times:
        start_time = pd.to_datetime(j)
        end_time = start_time + delta
        df = s.between_time(start_time.time(), end_time.time())
        xdata = df.index.minute
        ydata = np.array(df.bikes_available)
        popt, pcov = optimization.curve_fit(func, xdata, ydata)
        popt_single.append(popt[1])
    popt_all_stations.append(popt_single)
    row_headers.append(i)
    
# reshape model ouput with time as column headers, station as index
df = pd.DataFrame(popt_all_stations)
df.columns = start_times
df.index = row_headers

# Convert model from minute-by-minute change in bike availabilty --> total change during 
# model window (e.g. the 16:00 model applies to status updates between 16:00 and 16:29)
 model = df*29
model['station_id'] = model.index
model = pd.melt(model, id_vars = ['station_id'])
model.head()
 pd.DataFrame(feature_imp, columns=X_train.columns).describe()
 
 
Item 2178: 
k = 1
Volume of hypercube = 2
Fraction of points inside hypersphere = 1.0
-----------------------------------------
k = 2
Volume of hypercube = 4
Fraction of points inside hypersphere = 0.784976
-----------------------------------------
k = 3
Volume of hypercube = 8
Fraction of points inside hypersphere = 0.52376
-----------------------------------------
 ...
 k = 17
Volume of hypercube = 131072
Fraction of points inside hypersphere = 1e-06
-----------------------------------------
k = 18
Volume of hypercube = 262144
Fraction of points inside hypersphere = 0.0
-----------------------------------------
k = 19
Volume of hypercube = 524288
Fraction of points inside hypersphere = 0.0
-----------------------------------------
 
 
Item 2181: 
Rule 1. If (Input I is Negative ) AND (Input II is Negative) then Decrease Throttle
    When Input I is Negative we got the value 0.5
    For Input II Negative was 0.
    So 0.5 AND 0.0 = 0.0
 Rule 2. If (Input I is Zero) AND (Input II is Negative) then Increase Throttle
    Input I is Zero = 0.5
    Input II is Negative = 0.
    So 0.5 AND 0.0 = 0.0
 Rule 3. If (Input I is Positive) AND (Input II is Negative) then Increase Throttle
    0.0 AND 0.0 = 0.0   
 Rule 4. If (Input I is Negative) AND (Input II is Zero) then Decrease Throttle
    0.5 AND 0.5 = 0.5
 Rule 5. If (Input I is Zero) AND (Input II is Zero) then No_Change
    0.5 AND 0.5 = 0.5
 Rule 6. If (Input I is Positive) AND (Input II is Zero) then Increase Throttle
    0.0 AND 0.5 = 0.0
 Rule 7. If (Input I is Negative) AND (Input II is Positive) then Decrease Throttle
    0.5 AND 0.5 = 0.5
 Rule 8. If (Input I is Zero) AND (Input II is Positive) then Decrease Throttle
    0.5 AND 0.5 = 0.5
 Rule 9. If (Input I is Positive) AND (Input II is Positive) then Increase Throttle
    0.0 AND 0.5 = 0.0
 = SQRT(0.00² + 0.50² + 0.50² + 0.50²) = 0.866
 = SQRT(0.50²)= 0.500
 = SQRT(0.00² + 0.00² + 0.00² + 0.00²) = 0.000
 -50 * 0.866 + 0 * 0.500 + 50 * 0.000 = -43.3
 
 
Item 2187: 
Initial state:
 - error: [75870.4884482]
 - parameters: {'w': array([0.]), 'b': 0.0}

Iteration 0:
 - error: [73814.2609893]
 - parameters: {'w': array([0.1]), 'b': 0.1}

Iteration 20:
 - error: [38764.28631114]
 - parameters: {'w': array([2.1]), 'b': 2.1}

Iteration 40:
 - error: [15284.92972772]
 - parameters: {'w': array([4.1]), 'b': 4.1}

Iteration 60:
 - error: [3376.19123904]
 - parameters: {'w': array([6.1]), 'b': 6.1}

Iteration 80:
 - error: [1753.32046443]
 - parameters: {'w': array([7.1]), 'b': 8.1}

Final state:
 - error: [1741.85716443]
 - parameters: {'w': array([7.1]), 'b': 10.0}
 Initial state:
 - error: [75870.4884482]
 - parameters: {'w': array([0.]), 'b': 0.0}

Iteration 0:
 - error: [13536.3070032]
 - parameters: {'w': array([10.17501967]), 'b': array([0.17843399])}

Iteration 4000:
 - error: [1737.28457739]
 - parameters: {'w': array([7.09101188]), 'b': array([10.96966037])}

Iteration 8000:
 - error: [1707.33242182]
 - parameters: {'w': array([6.9583785]), 'b': array([18.67110985])}

Iteration 12000:
 - error: [1692.21685452]
 - parameters: {'w': array([6.86415678]), 'b': array([24.14215949])}

Iteration 16000:
 - error: [1684.5886765]
 - parameters: {'w': array([6.79722241]), 'b': array([28.02875048])}

Final state:
 - error: [1680.73973307]
 - parameters: {'w': array([6.74968272]), 'b': array([30.78917543])}
 
 
Item 2191: 
git clone https://github.com/facebookresearch/Starspace.git
cd Starspace
make
 22256635,"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.",1,0,0,0,0,0
 Nonsense ? kiss off , geek . what I said is true . I 'll have your account terminated . __label__0
 starspace train -ngrams 2 -minCount 10 -thread 4 -trainFile cache/train.txt -model cache/starspace.model
 starspace test -testFile cache/val.txt -model cache/starspace.model -predictionFile cache/starspace.pred
 Example 68:
LHS:
IT 'S HER QUOTE . WHAT CAN'T YOU UNDERSTAND ? Here is the official press release Now , please , can we stop this nonsense ? 
RHS: 
__label__0 
Predictions: 
(--) [0.542934] __label__6 
(--) [0.055087] __label__4 
(--) [0.0467122]        __label__2 
(++) [-0.0127831]       __label__0 
(--) [-0.23463] __label__1
 Example 116:
LHS:
STOP , you had better stop putting all that stupid junk on my IP page . read it . you 'll see what i 'll do if you do it again . 
RHS: 
__label__0 
Predictions: 
(++) [0.529969] __label__0 
(--) [0.416814] __label__2 
(--) [0.388696] __label__4 
(--) [0.34913]  __label__3 
(--) [0.240833] __label__1
 
 
Item 2194: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
2. What do we mean by intelligence, artificial or otherwise? 
3. The how, when, and what of AI
4. AI and the bright future ahead
5. AI and the bleak, darkness that could befall us
6. What should we be doing about AI anyway? (this post)
 
 
Item 2196: 
plot_na <- ggplot(
    bj_aq_full[PM25_NA==1], 
    aes(x=time, y=PM25_NA + 
            runif(sum(bj_aq_full$PM25_NA==1), -0.1, 0.1))) +
    facet_wrap(~station_id, ncol=1) + 
    geom_point(size=0.1, alpha=0.5) +
    labs(y="", title="Missing Data Points for Each Stations") +
    theme_bw() + theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.y=element_blank()
    )
 plot_na <- ggplot(
    bj_aq_full, 
    aes(x=time, y=PM25_NA + 
        runif(nrow(bj_aq_full), -0.3, 0.3))) +
    facet_wrap(~station_id, ncol=1) + 
    geom_point(size=0.1, alpha=0.5) +
    labs(y="", title="Missing Data for Each Stations (0: Present, 1: Absent)") +
    theme_bw() + theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()      
    )
 full_keys <- expand.grid(
    time=seq(ymd_hms("2017-01-01 16:00:00"), 
             ymd_hms("2018-03-30 16:00:00"), 
             by="1 hour"),
    station_id=unique(bj_aq[,station_id])
)
bj_aq_full <- merge(bj_aq, full_keys, by=c("time", "station_id"), all.y=T)
 dygraph(
    quality[station_id=="tiantan_aq", .(date, lower, avg, upper)], 
    main="Tiantan Daily PM2.5 Concentration (Max clipped to 500)"
) %>% 
dyAxis("x", drawGrid=F) %>% dyAxis("y", drawGrid=F) %>%
dySeries(c("lower", "avg", "upper"), label="PM2.5") %>%
dyLimit(0, "good", labelLoc="right", color="grey") %>%
dyLimit(12, "moderate", labelLoc="right", color="grey") %>%
dyLimit(35, "unhealthy for sensitive groups", 
        labelLoc="right", color="grey") %>%
dyLimit(55, "unhealthy", labelLoc="right", color="grey") %>%    
dyLimit(150, "very unhealthy", labelLoc="right", color="grey") %>% 
dyLimit(250, "hazardous", labelLoc="right", color="grey") %>% dyShading(0, 12, color="#ccff99", axis="y") %>%
dyShading(12, 35, color="#ffffcc", axis="y") %>%
dyShading(35, 55, color="#ffebcc", axis="y") %>%
dyShading(55, 150, color="#ffcccc", axis="y") %>%
dyShading(150, 250, color="#e6ccff", axis="y") %>%
dyShading(250, 500, color="#ffddcc", axis="y") %>%
dyOptions(colors = RColorBrewer::brewer.pal(3, "Set1")) %>%
dyRangeSelector(dateWindow=c(as.Date("2017-01-01"), max(quality$date))) %>% 
dyLegend(width = 200, show = "follow")
 data = pd.merge(
    bj_aq[bj_aq["time"]=="2017-01-01 20:00:00"], 
    stations, on="station_id")
 map_hooray = folium.Map(
    location=[39.929, 116.8],
    tiles = "Stamen Terrain",
    zoom_start = 9)

for _, row in data.iterrows():
    color = colourgrad(
        0, 500, min(row["PM25_Concentration"], 500))
    folium.CircleMarker([
        row["Latitude"],row["Longitude"]],
        color=color, radius=9, fill_opacity=1, 
        fill=True, fill_color=color,
        popup=row["station_id"] + ":" + str(row["PM25_Concentration"])
).add_to(map_hooray)
 p <- ggplot(
    bj_aq_full[time<ymd_hms("2017-01-10 00:00:00")], 
    aes(x=Longitude, y=Latitude, 
        fill=pmin(500, PM25_Concentration),
        frame=time, cumulative=FALSE)) +
    geom_point(size=8, pch=21, color="grey50") + 
    scale_fill_gradient(low = "#FFFF99", high = "#990000",
        na.value = "white", guide = "colourbar") 
p <- p + theme_minimal() + labs(fill="PM25") +
    theme(panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank())
animation <- gganimate(p, "animation_tmp.gif", interval = 0.5, ani.width = 1280, ani.height = 720)
# gganimate(p, interval = 0.5, ani.width = 1280,
#   ani.height = 720, filename="pm25_0101_0110.mp4",
#   other.opts = "-pix_fmt yuv420p")
 
 
Item 2197: 
exploratory::scrape_html_table("https://www.cdc.gov/mmwr/volumes/67/wr/mm6731a1.htm", 1, "TRUE" ,encoding="Unicode (UTF-8)") %>%
  exploratory::clean_data_frame() %>%
  rename(Year0 = Year) %>%
  select(-`Average annual rate change¶`) %>%
  rename_all(funs(str_remove(., "Year"))) %>%
  filter(State %nin% c("National", "State")) %>%
  gather(key, value, `0`:`15`, na.rm = TRUE, convert = TRUE) %>%
  mutate(key = key + 1999, value = parse_number(value)) %>%
  rename(Year = key, numbers_per_K = value) %>%
  arrange(`State`, `Year`) %>%
  group_by(State) %>%
  fill(numbers_per_K, .direction = "down") %>%
  fill(numbers_per_K, .direction = "up") %>%
  mutate(numbers_per_K_pct_diff_first = (numbers_per_K - first(numbers_per_K)) / first(numbers_per_K) * 100, numbers_per_K_pct_first = (numbers_per_K / first(numbers_per_K)) * 100, over_1_pct = numbers_per_K > 5) %>%
  filter(Year > 1999)
 
 
Item 2204: 
A Change in Gestation = 1.32 (Coefficient for Plurality) * 2 (difference between a single and triplets)
 5.28 = 1.32 * (5 - 1)
 as.character(plurality)
 Plural > 1
 
 
Item 2208: 
    def find_neighbors(self, new_sample):
        '''List the k neighbors closest to the new sample.
        
        '''
        distances = []      
        for i in range(len(self.X)):
            if self.metric == 'euclidean':  
                distance = self.euclidean(self.X[i], new_sample)
            if self.metric == 'manhattan':
                distance = self.manhattan(self.X[i], new_sample)
            distances.append((self.y[i],distance))
        distances = sorted(distances,key=operator.itemgetter(1))
        
        neighbors = []
        for i in range(self.k):
            neighbors.append(distances[i][0])
        return neighbors
 def euclidean(self, a, b):
    return np.sqrt(((a-b)**2).sum(axis=0))
 def minkowski(self, a, b, p):
    return ((np.abs(a-b)**p).sum(axis=0))**(1/p)
     def majority_vote(self, neighbors):
        '''Determine majority class from the set of neighbors.
        
        '''
        class_votes = {}
        for i in range(len(neighbors)):
            sample_class = neighbors[i]
            if sample_class in class_votes:
                class_votes[sample_class] += 1
            else:
                class_votes[sample_class] = 1
        sorted_votes = sorted(class_votes.items())
        if self.ties:
            sorted_votes = self.tie(sorted_votes)
        return sorted_votes[0][0]
     def predict(self, X_test):
        '''Predict class for each value in array of new samples.
        
        '''
        self.tie_count = 0
        y_pred = []
        for i in range(len(X_test)):
            neighbors = self.find_neighbors(X_test[i])
            pred_class = self.majority_vote(neighbors)
            y_pred.append(pred_class)
        if self.ties:
            print('{} ties'.format(self.tie_count))
        return y_pred
     def tie(self,sorted_votes):
        '''Determine when ties occur in the the neighbors. Of the tied classes,
        choose the class most frequent in the training data.
        
        Print out number of ties.
        '''
        tie = {}
        for pair in sorted_votes:
            count = pair[1]
            if count in tie:
                self.tie_count += 1
                #print('tie')
                tie[count].append(pair[0])
            else:
                tie[count] = [pair[0]]
            #print(tie)
        tie_class_frequency = {}
        if len(tie[count]) > 1:
            #print('tie')
            for tie_class in tie[count]:
                tie_class_frequency[tie_class] = np.count_nonzero(self.y == tie_class)
            max_class = max(tie_class_frequency, key=tie_class_frequency.get)
            #print(max_class)
            sorted_votes = [(max_class,1)]
        return sorted_votes
    def split(self, feat, val, Xy):
        Xi_left = np.array([]).reshape(0,self.Xy.shape[1])
        Xi_right = np.array([]).reshape(0,self.Xy.shape[1])
        for i in Xy:
            #print(i.shape)
            if i[feat] <= val:
                Xi_left = np.vstack((Xi_left,i))
            if i[feat] > val:
                Xi_right = np.vstack((Xi_right,i))
        return Xi_left, Xi_right
 def gini_score(groups,classes):
    n_samples = sum([len(group) for group in groups])
    gini = 0
    for group in groups:
        size = float(len(group))
        if size == 0:
            continue
        score = 0.0
        #print(size)
        for class_val in classes:
            #print(group.shape)
            p = (group[:,-1] == class_val).sum() / size
            score += p * p
        gini += (1.0 - score) * (size / n_samples)
        #print(gini)
    return gini
     def best_split(self, Xy):
        classes = np.unique(Xy[:,-1])
        best_feat = 999
        best_val = 999
        best_score = 999
        best_groups = None
        for feat in range(Xy.shape[1]-1):
            for i in Xy:
                groups = self.split(feat, i[feat], Xy)
                gini = self.gini_score(groups, classes)
                #print('feat {}, valued < {}, scored {}'.format(feat,i[feat], gini))
                if gini < best_score:
                    best_feat = feat
                    best_val = i[feat]
                    best_score = gini
                    best_groups = groups
        output = {}
        output['feat'] = best_feat
        output['val'] = best_val
        output['groups'] = best_groups
        return output
     def terminal_node(self, group):
        classes, counts = np.unique(group[:,-1],return_counts=True)
        return classes[np.argmax(counts)]
            
    def split_branch(self, node, depth):
        left_node, right_node = node['groups']
        del(node['groups'])
        if not isinstance(left_node,np.ndarray) or not isinstance(right_node,np.ndarray):
            node['left'] = node['right'] = self.terminal_node(left_node + right_node)
            return
        if depth >= self.max_depth:
            node['left'] = self.terminal_node(left_node)
            node['right'] = self.terminal_node(right_node)
            return
        if len(left_node) <= self.min_num_sample:
            node['left'] = self.terminal_node(left_node)
        else:
            node['left'] = self.best_split(left_node)
            self.split_branch(node['left'], depth+1)
        if len(right_node) <= self.min_num_sample:
            node['right'] = self.terminal_node(right_node)
        else:
            node['right'] = self.best_split(right_node)
            self.split_branch(node['right'], depth+1)
     def predict_sample(self, node, sample):
        #print(node)
        if sample[node['feat']] < node['val']:
            if isinstance(node['left'],dict):
                return self.predict_sample(node['left'],sample)
            else:
                return node['left']
        else:
            if isinstance(node['right'],dict):
                return self.predict_sample(node['right'],sample)
            else:
                return node['right']
     def predict(self, X_test):
        self.y_pred = np.array([])
        for i in X_test:
            #print(i)
            self.y_pred = np.append(self.y_pred,self.predict_sample(self.root,i))
        return self.y_pred
 
 
Item 2212: 
fc1 = alpha * tf.divide(fc1, tf.norm(fc1, ord='euclidean'))
 
 
Item 2221: 
1.287357370010931 9.908606190326509
 
 
Item 2223: 
library(BIS)
 datasets <- get_datasets()
 library(BIS)
 datasets <- get_datasets()
 get_bis(datasets$url[datasets$name == "Policy rates (monthly)"],quiet = TRUE)
 str_c(date, "-01")
 2018-04-02
 2018/04/02
 2018/4/2
 Reported on 2018, April 2nd.
 
 
Item 2226: 
gs://<project_ID>-vcm/XianHaliHaiguazi/Xian/98.jpg,Xian
gs://<project_ID>-vcm/XianHaliHaiguazi/Xian/99.jpg,Xian
gs://<project_ID>-vcm/XianHaliHaiguazi/Hali/1.jpg,Hali
gs://<project_ID>-vcm/XianHaliHaiguazi/Hali/10.jpg,Hali
 
 
Item 2229: 
def my_callable(x):
    return x + 1
 return my_callable(1)
 class MyClass:
    def __call__(self, x):
        return x + 1

my_callable = MyClass()
 return my_callable(5)
 class MyClass:
    def __init__(self):
        self.x = 5
 my_object = MyClass()
 return my_object.x
 class MyClass:
    def __init__(self):
        pass
     @property
    def x(self):
        return 5
 my_object = MyClass()
 return my_object.x
 cell = LSTM(num_units)
inputs = [...a list of data...]
state = initial_state
result = []
 for input in inputs:
    output, state = cell(input, state)
    result.append(output)
 return result
 output = self._activation(self._linear([inputs, state]))
return output, output
 
 
Item 2232: 
self.add_class("balloon", 1, "balloon")
 self.add_class(“operator”, 1, “operator”) 
 
 
Item 2235: 
<WEBSITE>:
  website,
  http website,
  https website,
  http domain,
  web address,
  online address,
  http address
 <USER>:
  user,
  web user,
  http user,
  https user,
  online user
 
 
Item 2240: 
Your contribution will be highly appreciated on Patreon.
 
 
Item 2243: 
questions: 4688 filtered out of 19558 (missing words)
accuracy = 9.94% (466/4688)
second-guess accuracy = 14.04% (658/4688)
 'man' is to 'woman' as 'husband' is to 'wife'
'boy' is to 'girl' as 'uncle' is to 'aunt'
'falling' is to 'fell' as 'going' is to 'went'
 
 
Item 2245: 
                  sum = x1 + x2 + x3 + ...... xk
 
 
Item 2247: 
[('1993_Clinton_D_1', 0.9777867794036865),
 ('1977_Carter_D_5', 0.9765924215316772),
 ('1969_Nixon_R_5', 0.975844144821167),
 ('1989_Bush_R_5', 0.974454939365387),
 ('2001_Bush_R_1', 0.9733594655990601),
 ('1961_Kennedy_D_5', 0.973156213760376),
 ('1913_Wilson_D_3', 0.9689352512359619),
 ('1981_Reagan_R_5', 0.9668718576431274),
 ('1945_Roosevelt_D_4', 0.9627108573913574),
 ('1953_Eisenhower_R_5', 0.9612894654273987)]
 
 
Item 2249: 
Chop the potato.
Chop your potatoes.
Chop and dice the potato.
 X the Y.
X the Y finely.
X your Y.
X and dice the Y.
 Chop the Y.
Chop the onion finely.
Chop the fish.
Chop the steaks.
 X the potatoes.
Dice the potatoes.
Slice the potatoes.
Wash the potatoes.
Put the potatoes in the oven.
 Cut the onions with a knife
 
 
Item 2251: 
query getSpaces {
  spaces(first: 5) {
    items  {
       title
       id
    }
  }
}
 query getSpace {
  space(id: "enter the space id")  {
    title
    conversation {
      messages {
        items {
          content
          id
        }
      }
    }
  }
}
 query getMessage {
  message (id: "enter message id") {
    id
    content
    contentType
    annotations
  }
}
 {
  "output": {
    "text": {
      "values": [},
      "selection_policy": "sequential"
    },
    "actions": {
      "GET_WEATHER"
    ]
  }
}
 {
  "output": {
    "text": {
      "values": [},
      "selection_policy": "sequential"
    },
    "actions": {
      "GET_FORECAST"
    ]
  }
}
 
 
Item 2255: 
site:medium.com andrii ryzhenko
 site:linkedin.com pulse andrii ryzhenko
 constructor( address[] _crewAddresses) payable public {
        nextCapitan = msg.sender;
        crewList = _crewAddresses;
        voteCount = 1;
        for (uint i = 0; i < crewList.length; i++) {
            crew[crewList[i]] = true;
        }    
}
                      Sorry, no options, it was fixed. !
              Use manual formation inside of Code boxes or 
                           Images with text 
 
 
Item 2266: 
plt.yscale('log')
 
 
Item 2272: 
git init
git add 'Enter file names or folders that have changed'
git commit -m 'enter a message reminder here'
 
 
Item 2273: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
2. What do we mean by intelligence, artificial or otherwise? 
3. The how, when, and what of AI
4. AI and the bright future ahead (this post)
5. AI and the bleak, darkness that could befall us
6. What should we be doing about AI anyway?
 
 
Item 2283: 
!pip install pixiedust_node
import pixiedust_node
 %%node
console.log("Today's date is", new Date());
 npm.install( ('request', 'request-promise') )
 %%node
const request = require('request-promise');
request.get('http://www.google.com').then(print);
 a = 'hello'
b = 2
c = False
d = {'x':1, 'y':2}
e = 3.142
f = [{'a':1}, {'a':2}, {'a':3}]
 %%node
console.log(a, b, c, d, e, f);
// hello 2 false {y:2, x:1} 3.142 [{ a:1 }, { a:2 }, { a:3 }]
 %%node
var i = 'world';
var j = 3;
var k = true;
var l = { z:99 }
var m = 2.7182;
var n = ['apples', 'oranges'];
 print(i,j,k,l,m,n)
# (u'world', 3, True, {u'z': 99}, 2.7182, [u'apples', u'oranges'])
 %%node
var googlehomepage = '';
request.get('http://www.google.com').then(function(data) {
  googlehomepage = data;
  print('Fetched Google homepage');
});
 !pip --upgrade pixiedust_node
 
 
Item 2284: 
1. Caleb Kirksey
   - He gave me some initial help and encouragement
2. kangalow from JetsonHacks
    - He gave me some advice on what ESC to choose
3. Mohamed Alzaki
    - He gave me some advice on how he implemented Ultrasonic Sensors into his robot
4. Tawn Kramer
    - He gave me advice on how to integrate Hardware with Software in his videos
 
 
Item 2287: 
ls -lah shakes.txt
 ### Display:
-rw-r--r--@ 1 sabber  staff   5.6M Jun 15 09:35 shakes.txt
 less -N shakes.txt
 ### Display:
      1 <U+FEFF>
      2 Project Gutenberg’s The Complete Works of William Shakespeare, by William
      3 Shakespeare
      4
      5 This eBook is for the use of anyone anywhere in the United States and
      6 most other parts of the world at no cost and with almost no restrictions
      7 whatsoever.  You may copy it, give it away or re-use it under the terms

 cat shakes.txt | sed -e '149260,149689d' | sed -e '1,141d' > shakes_new.txt
 cat shakes_new.txt | wc | awk '{print "Lines: " $1 "\tWords: " $2 "\tCharacter: " $3 }'
 ### Display
Lines: 149118 Words: 956209 Character: 5827807
 cat shakes_new.txt | tr 'A-Z' 'a-z' | tr -d [:punct:] |  tr -d [:digit:] > shakes_new_cleaned.txt
 ### Display before:
      1 From fairest creatures we desire increase,
      2 That thereby beauty’s rose might never die,
      3 But as the riper should by time decease,
      4 His tender heir might bear his memory:
      5 But thou contracted to thine own bright eyes,
      6 Feed’st thy light’s flame with self-substantial fuel,
      7 Making a famine where abundance lies,
      8 Thy self thy foe, to thy sweet self too cruel:
      9 Thou that art now the world’s fresh ornament,
     10 And only herald to the gaudy spring,
     11 Within thine own bud buriest thy content,
     12 And, tender churl, mak’st waste in niggarding:
     13   Pity the world, or else this glutton be,
     14   To eat the world’s due, by the grave and thee.
 
### Display after:
       1 from fairest creatures we desire increase
      2 that thereby beautys rose might never die
      3 but as the riper should by time decease
      4 his tender heir might bear his memory
      5 but thou contracted to thine own bright eyes
      6 feedst thy lights flame with selfsubstantial fuel
      7 making a famine where abundance lies
      8 thy self thy foe to thy sweet self too cruel
      9 thou that art now the worlds fresh ornament
     10 and only herald to the gaudy spring
     11 within thine own bud buriest thy content
     12 and tender churl makst waste in niggarding
     13   pity the world or else this glutton be
     14   to eat the worlds due by the grave and thee
 cat shakes_new_cleaned.txt | tr -sc ‘a-z’ ‘\12’ > shakes_tokenized.txt
 ### Display (First 10 words)
       1 from
      2 fairest
      3 creatures
      4 we
      5 desire
      6 increase
      7 that
      8 thereby
      9 beautys
     10 rose
 cat shakes_tokenized.txt | sort | uniq -c | sort -nr > shakes_sorted_desc.txt
 ### Display
 29768 the   28276 and  21868 i   20805 to  18650 of  15933 a      14363 you   13191 my   11966 in  11760 that
 cat shakes_tokenized.txt | sort | uniq -c | sort -n > shakes_sorted_asc.txt
 ### Display
 1 aarons       1 abandoner    1 abatements     1 abatfowling          1 abbominable  1 abaissiez    1 abashd         1 abates              1 abbeys       1 abbots
 awk ‘FNR==NR{for(i=1;i<=NF;i++)w[$i];next}(!($1 in w))’ stop_words.txt shakes_tokenized.txt > shakes_stopwords_removed.txt
 cat shakes_stopwords_removed.txt | sort | uniq -c | sort -nr > shakes_sorted_desc.txt
 ### Display most frequent
 3159 lord   2959 good  2924 king  2900 sir
2634 come   2612 well  2479 would 2266 love
2231 let    2188 enter
 cat shakes_stopwords_removed.txt | sort | uniq -c | sort -n > shakes_sorted_asc.txt
 ### Display least frquent
 1 aarons       1 abandoner    1 abatements     1 abatfowling          1 abbominable  1 abaissiez    1 abashd         1 abates              1 abbeys       1 abbots
 Email: sabbers@gmail.com
LinkedIn: https://www.linkedin.com/in/sabber-ahamed/
Github: https://github.com/msahamed
Medium: https://medium.com/@sabber/
 
 
Item 2294: 
CALL netscan.find_communities('User','ANSWERED','userId','weight', 'HIGHER_BETTER', eps, minPts, radius);
 
 
Item 2296: 
<html> 
   <body> 
    <a id="special" class="cool-class" href="http://ceddy.org/article"> Special article <a> 
   </body> 
 </html>
 
 
Item 2298: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
2. What do we mean by intelligence, artificial or otherwise? 
  (this post)
3. The how, when, and what of AI
4. AI and the bright future ahead
5. AI and the bleak, darkness that could befall us
6. What should we be doing about AI anyway?
 
 
Item 2301: 
conda create -n env_name list of packages
 source activate envname
 Windows: actiavte envname
 source deactivate
 source activate envname
conda install packagename
 conda env export > environment.yaml
 conda env create -f environment.yaml
 conda env remove -n env_name
 conda env list
 
 
Item 2303: 
Optimized after 1000 iterations: m = 1.4777440851894448, c = 0.08893651993741346 and error = 112.61481011613473
 
 
Item 2305: 
Running saved model...
[[ 0.00318024]
 [ 0.99879535]
 [ 0.99887906]
 [ 0.9971305 ]
 [ 0.99621754]
 [ 0.0033892 ]]
 
 
Item 2307: 
vgg = vgg19.Vgg19()
vgg.build(images)
 def model(self, input, reuse=False):
    with tf.variable_scope('model', reuse=reuse):
        tf.identity(input, name="input_image")
        coefs = tf.get_variable("my_variable", [1, 400, 600, 3], initializer=tf.random_normal_initializer())
        output = input * coefs
        tf.identity(output, name="output_image")

    return tf.nn.tanh(output)
 vgg = Vgg19()
vgg1 = Vgg19()
vgg.build((target_image + 1)/2)
vgg1.build((network + 1) / 2)
 loss = tl.cost.mean_squared_error(vgg.conv1_1, vgg1.conv1_1, is_mean=True)
 vector_target2 = tf.reshape(vgg.conv5_1, [-1, tf.shape(vgg.conv5_1)[1] * tf.shape(vgg.conv5_1)[2], 512])
vector_output2 = tf.reshape(vgg1.conv5_1, [-1, tf.shape(vgg.conv5_1)[1] * tf.shape(vgg.conv5_1)[2], 512])
 style_loss =  tl.cost.mean_squared_error(tf.matmul(vector_target2, vector_target2, adjoint_a=True)/(50*75*256), tf.matmul(vector_output2, vector_output2, adjoint_a=True)/(50*75*256), is_mean=True)
 
 
Item 2311: 
docker-compose -f neo4j-zeppelin.yml up -d
 crimeDF.printSchema
 z.show(exendedCrimeDF)
 Neo4jDataFrame.mergeEdgeList(sc: SparkContext,
    dataFrame: DataFrame,
    source: (label,Seq[prop]),
    relationship: (type,Seq[prop]),
    target: (label,Seq[prop]))
 %neo4j
 MATCH (crime:Crime)-[is:IS_TYPE]->(type:CrimeType),
 (crime)-[on:ON_DATE]->(date:CrimeDate)
RETURN crime, is, type, on, date
LIMIT 10
 
 
Item 2312: 
m = np.array([[1, 2, 3], [4,5,6], [7,8,9]])
#
# array([[1, 2, 3],
#        [4, 5, 6],
#        [7, 8, 9]])
 c = np.array([10,20,30])
#
# array([10, 20, 30])
 m + c
#
# array([[11, 22, 33],
#        [14, 25, 36],
#        [17, 28, 39]])
 
 
Item 2318: 
python
 print(varName)
print('hello world')
 for i in range(4):
    for j in range(4):
        if j > i:
            break
        print((i, j))
 tup = 4, 5, 6
 a_list = [2, 3, 7, None]
tup = ('foo', 'bar', 'baz')
b_list = list(tup)
b_list
b_list[1] = 'peekaboo'
b_list
 empty_dict = {}
d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4]}
d1
 strings = ['a', 'as', 'bat', 'car', 'dove', 'python']
[x.upper() for x in strings if len(x) > 2]
 
 
Item 2325: 
shape of channel1(retail)data: (30000, 3, 6, 1)
shape of channel2(mortgage)data: (30000, 3, 6, 1)
 shape of input volume(layer)data: (30000, 3, 6, 2)
 X = np.concatenate((retail, mort),axis=3)
 Y = event_id[['default payment next month']].values
 Y = to_categorical(Y,2)
In our data we have payment default rate is: 28.402671 percent
 #Preparing Indermediate model-removing last sigmoid layer
intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer('feature_dense').output)
intermediate_layer_model.summary()
 #predict to get featured data
feauture_engg_data = intermediate_layer_model.predict(X)
feauture_engg_data = pd.DataFrame(feauture_engg_data)
print('feauture_engg_data shape:', feauture_engg_data.shape)
feauture_engg_data shape: (30000, 100)
 #Sorting correlation values in decsending order
new_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)
#Let's see top 10 correlation values
new_corrs[:10] 
[('PAY_1', 0.32479372847862253),
 ('PAY_2', 0.26355120167216467),
 ('PAY_3', 0.2352525137249163),
 ('feat_78', -0.22173805212223915),
 ('PAY_4', 0.21661363684242424),
 ('PAY_5', 0.20414891387616674),
 ('feat_86', -0.20047655459374053),
 ('feat_6', -0.19189993720885604),
 ('PAY_6', 0.1868663616535449),
 ('feat_3', -0.17394080015462873)]
 
 
Item 2326: 
class GlobalData
{
    public string protocol = "http://";
    public string ip = "YOUR SERVER IP";
    public int port = 8080;
    public string endpoint = "/api/TASS/infer";
    public string endpointIDC = "/api/IDC/infer";
    public string dataFolder = "Data\\1";
    //public string dataFolder = "Data\\2";
     public double threshold = 0.80;
    public int expectedCount = 6;
}
 INFO:tensorflow:Global Step 73: Streaming Accuracy: 0.8935 (0.61 sec/step)
INFO:tensorflow:Global Step 74: Streaming Accuracy: 0.8942 (0.67 sec/step)
INFO:tensorflow:Final Streaming Accuracy: 0.8941
 8975_idx5_x301_y801_class0.png
{"Confidence": "1.0", "ResponseMessage": "IDC Not Detected With Confidence 1.0", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 1 8975_idx5_x301_y801_class0.png with 1 confidence.
Processed image 1
 
8975_idx5_x1001_y1451_class1.png
{"Confidence": "0.9526", "ResponseMessage": "IDC Not Detected With Confidence 0.9526", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 4 8975_idx5_x1001_y1451_class1.png with 0.9526 confidence.
Processed image 4
 
8975_idx5_x1051_y1251_class1.png
{"Confidence": "0.807", "ResponseMessage": "IDC Not Detected With Confidence 0.807", "Response": "OK", "Results": 0}
UNSURE: IDC detected in image 5 8975_idx5_x1051_y1251_class1.png with 0.807 confidence.
Processed image 5
 ------------------------------------------------
 8975_idx5_x3501_y1801_class0.png
{"Confidence": "0.984", "ResponseMessage": "IDC Not Detected With Confidence 0.984", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 11 8975_idx5_x3501_y1801_class0.png with 0.984 confidence.
Processed image 11
 
8975_idx5_x3501_y1851_class0.png
{"Confidence": "0.99", "ResponseMessage": "IDC Not Detected With Confidence 0.99", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 12 8975_idx5_x3501_y1851_class0.png with 0.99 confidence.
Processed image 12
 
2 true positives, 0 false positives, 1 false negatives, 3 unsure, 6 true negatives, 1 incorrect examples classified, 0.44 accuracy, 1 precision, 0.67 recall, 0.8 fscore
 
- 2 true positives, 0 false positives, 1 false negatives, 6 true negatives
- 3 unsure
- 1 incorrect examples classified
- 0.44 accuracy
- 1 precision
- 0.67 recall
- 0.8 fscore
 8975_idx5_x1001_y1451_class1.png
{"Confidence": "0.9526", "ResponseMessage": "IDC Not Detected With Confidence 0.9526", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 4 8975_idx5_x1001_y1451_class1.png with 0.9526 confidence.
Processed image 4
 8975_idx5_x1051_y1251_class1.png
{"Confidence": "0.807", "ResponseMessage": "IDC Not Detected With Confidence 0.807", "Response": "OK", "Results": 0}
UNSURE: IDC detected in image 5 8975_idx5_x1051_y1251_class1.png with 0.807 confidence.
Processed image 5
 8975_idx5_x1251_y1251_class1.png
{"Confidence": "0.851", "ResponseMessage": "IDC Not Detected With Confidence 0.851", "Response": "OK", "Results": 0}
UNSURE: IDC detected in image 7 8975_idx5_x1251_y1251_class1.png with 0.851 confidence.
Processed image 7
 
8975_idx5_x1251_y1901_class1.png
{"Confidence": "0.726", "ResponseMessage": "IDC Not Detected With Confidence 0.726", "Response": "OK", "Results": 0}
UNSURE: IDC detected in image 8 8975_idx5_x1251_y1901_class1.png with 0.726 confidence.
Processed image 8
 class GlobalData
{
    public string protocol = "http://";
    public string ip = "YOUR SERVER IP";
    public int port = 8080;
    public string endpoint = "/api/TASS/infer";
    public string endpointIDC = "/api/IDC/infer";
    //public string dataFolder = "Data\\1";
    public string dataFolder = "Data\\2";
     public double threshold = 0.80;
    public int expectedCount = 50;
}
 8975_idx5_x3501_y1801_class0.png
{"Confidence": "0.984", "ResponseMessage": "IDC Not Detected With Confidence 0.984", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 98 8975_idx5_x3501_y1801_class0.png with 0.984 confidence.
Processed image 98
 
8975_idx5_x3501_y1851_class0.png
{"Confidence": "0.99", "ResponseMessage": "IDC Not Detected With Confidence 0.99", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 99 8975_idx5_x3501_y1851_class0.png with 0.99 confidence.
Processed image 99
 
8975_idx5_x3601_y1701_class0.png
{"Confidence": "1.0", "ResponseMessage": "IDC Not Detected With Confidence 1.0", "Response": "OK", "Results": 0}
CORRECT: IDC correctly not detected in image 100 8975_idx5_x3601_y1701_class0.png with 1 confidence.
Processed image 100
 
27 true positives, 0 false positives, 7 false negatives, 16 unsure, 50 true negatives, 7 incorrect examples classified, 0.03 accuracy, 1 precision, 0.79 recall, 0.89 fscore
 
- 27 true positives, 0 false positives, 7 false negatives, 50 true negatives
- 16 unsure
- 7 incorrect examples classified
- 0.03 accuracy
- 1 precision
- 0.79 recall
- 0.89 fscore
 8975_idx5_x1001_y1451_class1.png
{"Confidence": "0.9526", "ResponseMessage": "IDC Not Detected With Confidence 0.9526", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 20 8975_idx5_x1001_y1451_class1.png with 0.9526 confidence.
Processed image 20
 8975_idx5_x1051_y1451_class1.png
{"Confidence": "0.904", "ResponseMessage": "IDC Not Detected With Confidence 0.904", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 25 8975_idx5_x1051_y1451_class1.png with 0.904 confidence.
Processed image 25
 
8975_idx5_x1051_y1601_class1.png
{"Confidence": "0.937", "ResponseMessage": "IDC Not Detected With Confidence 0.937", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 26 8975_idx5_x1051_y1601_class1.png with 0.937 confidence.
Processed image 26
 
8975_idx5_x1051_y1651_class1.png
{"Confidence": "0.9727", "ResponseMessage": "IDC Not Detected With Confidence 0.9727", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 27 8975_idx5_x1051_y1651_class1.png with 0.9727 confidence.
Processed image 27
 8975_idx5_x1151_y1201_class1.png
{"Confidence": "0.973", "ResponseMessage": "IDC Not Detected With Confidence 0.973", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 38 8975_idx5_x1151_y1201_class1.png with 0.973 confidence.
Processed image 38
 
8975_idx5_x1201_y1201_class1.png
{"Confidence": "0.958", "ResponseMessage": "IDC Not Detected With Confidence 0.958", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 51 8975_idx5_x1201_y1201_class1.png with 0.958 confidence.
Processed image 51
 8975_idx5_x1251_y1201_class1.png
{"Confidence": "0.979", "ResponseMessage": "IDC Not Detected With Confidence 0.979", "Response": "OK", "Results": 0}
FALSE NEGATIVE: IDC incorrectly not detected in image 65 8975_idx5_x1251_y1201_class1.png with 0.979 confidence.
Processed image 65
 8975_idx5_x1001_y1301_class1.png
{"Confidence": "0.8223", "ResponseMessage": "IDC Detected With Confidence 0.8223", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 17 8975_idx5_x1001_y1301_class1.png with 0.8223 confidence.
Processed image 17
 8975_idx5_x1051_y1251_class1.png
{"Confidence": "0.807", "ResponseMessage": "IDC Not Detected With Confidence 0.807", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 21 8975_idx5_x1051_y1251_class1.png with 0.807 confidence.
Processed image 21
 
8975_idx5_x1051_y1301_class1.png
{"Confidence": "0.723", "ResponseMessage": "IDC Not Detected With Confidence 0.723", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 22 8975_idx5_x1051_y1301_class1.png with 0.723 confidence.
Processed image 22
 8975_idx5_x1051_y1401_class1.png
{"Confidence": "0.854", "ResponseMessage": "IDC Detected With Confidence 0.854", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 24 8975_idx5_x1051_y1401_class1.png with 0.854 confidence.
Processed image 24
 
8975_idx5_x1101_y1251_class1.png
{"Confidence": "0.8604", "ResponseMessage": "IDC Detected With Confidence 0.8604", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 28 8975_idx5_x1101_y1251_class1.png with 0.8604 confidence.
Processed image 28
 8975_idx5_x1101_y1451_class1.png
{"Confidence": "0.622", "ResponseMessage": "IDC Not Detected With Confidence 0.622", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 32 8975_idx5_x1101_y1451_class1.png with 0.622 confidence.
Processed image 32
 
8975_idx5_x1101_y1501_class1.png
{"Confidence": "0.887", "ResponseMessage": "IDC Not Detected With Confidence 0.887", "Response": "OK", "Results": 0}
UNSURE: IDC detected in image 33 8975_idx5_x1101_y1501_class1.png with 0.887 confidence.
Processed image 33
 
8975_idx5_x1101_y1551_class1.png
{"Confidence": "0.84", "ResponseMessage": "IDC Detected With Confidence 0.84", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 34 8975_idx5_x1101_y1551_class1.png with 0.84 confidence.
Processed image 34
 8975_idx5_x1101_y1701_class1.png
{"Confidence": "0.8193", "ResponseMessage": "IDC Detected With Confidence 0.8193", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 37 8975_idx5_x1101_y1701_class1.png with 0.8193 confidence.
Processed image 37
 
8975_idx5_x1151_y1251_class1.png
{"Confidence": "0.5435", "ResponseMessage": "IDC Not Detected With Confidence 0.5435", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 39 8975_idx5_x1151_y1251_class1.png with 0.5435 confidence.
Processed image 39
 8975_idx5_x1151_y1451_class1.png
{"Confidence": "0.891", "ResponseMessage": "IDC Detected With Confidence 0.891", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 43 8975_idx5_x1151_y1451_class1.png with 0.891 confidence.
Processed image 43
 8975_idx5_x1151_y1601_class1.png
{"Confidence": "0.849", "ResponseMessage": "IDC Detected With Confidence 0.849", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 46 8975_idx5_x1151_y1601_class1.png with 0.849 confidence.
Processed image 46
 8975_idx5_x1151_y1801_class1.png
{"Confidence": "0.7793", "ResponseMessage": "IDC Not Detected With Confidence 0.7793", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 50 8975_idx5_x1151_y1801_class1.png with 0.7793 confidence.
Processed image 50
 
8975_idx5_x1201_y1251_class1.png
{"Confidence": "0.8906", "ResponseMessage": "IDC Not Detected With Confidence 0.8906", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 52 8975_idx5_x1201_y1251_class1.png with 0.8906 confidence.
Processed image 52
 8975_idx5_x1201_y1851_class1.png
{"Confidence": "0.895", "ResponseMessage": "IDC Detected With Confidence 0.895", "Response": "OK", "Results": 1}
UNSURE: IDC detected in image 64 8975_idx5_x1201_y1851_class1.png with 0.895 confidence.
Processed image 64
 
8975_idx5_x1251_y1251_class1.png
{"Confidence": "0.851", "ResponseMessage": "IDC Not Detected With Confidence 0.851", "Response": "OK", "Results": 0}
UNSURE: IDC not detected in image 66 8975_idx5_x1251_y1251_class1.png with 0.851 confidence.
Processed image 66
 
 
Item 2331: 
create issue <project key>
 create issue <issue key>
 
 
Item 2336: 
gun control, gun regulation, gun laws, gun legislation, gun violence, assault weapon, assault rifle, mass shootings, school shootings, AR-15, background checks, bump stocks, National Rifle Association, NRA
 
 
Item 2339: 
[
     [click 1, click 2, click 3, ..., click m],
     [click 1, click 2, click 3, ..., click n],
     ...
 ]
 
 
Item 2342: 
composer require davmixcool/php-sentiment-analyzer
 Use Sentiment\Analyzer;
$analyzer = new Analyzer();
 $sentence = "I just got a call from David - does he realize it's Sunday?";
$result = $analyzer->getSentiment($sentence);
 print_r($result);
 ['neg'=> 0, 'neu'=> 1, 'pos'=> 0, 'compound'=> 0]
 $sentence = "I just got a call from David - i don't like it when he calls on Sundays.";
$result = $analyzer->getSentiment($sentence);
 print_r($result);
 ['neg'=> 0.116, 'neu'=> 0.884, 'pos'=> 0, 'compound'=> -0.1128]
 
 
Item 2346: 
Sequential(
  (0): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1))
  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
  (2): Conv2D(None -> 16, kernel_size=(3, 3), stride=(1, 1))
  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
  (4): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1))
  (5): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
  (6): Conv2D(None -> 32, kernel_size=(3, 3), stride=(1, 1))
  (7): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
  (8): Flatten
  (9): Dense(None -> 64, Activation(relu))
  (10): Dense(None -> 32, Activation(relu))
  (11): Dense(None -> 12, linear)
)
 
 
Item 2347: 
install.packages("ggplot2")
install.packages("dplyr")
 library(ggplot2)
library(dplyr)
 diamonds<- data.frame(diamonds)
 names(diamonds)[8]<-"length"
names(diamonds)[9]<-"width"
names(diamonds)[10]<-"depth"
 names(diamonds)[5]<-"depthperc"
 diamonds <- mutate(diamonds, cubic=length*width*depth)
 diamonds$cubic <- diamonds$length * diamonds$width * diamonds$depth
 colMeans(diamonds[,c(1,5:11)])
 carat    depthperc        table        price       length        
   0.7979397   61.7494049   57.4571839 3932.7997219    5.7311572    
width       depth        cubic 
5.7345260   3.5387338  129.8494033
 diamonds$carat2 <- round(diamonds$carat/.25)*.25
 Summary <- aggregate(cbind(depthperc, table, price, length, width, depth, cubic)~cut+color+clarity+carat2, data=diamonds, mean)
 install.packages("reshape2")
library(reshape2)
 pivot_table <- dcast(diamonds[,c('color','clarity','price')], color~clarity, mean)
 names(Summary)[7]<-"avgprice"
 diamonds <- merge(diamonds, Summary[,c(1:4,7)], by.x=c("cut","color","clarity","carat2"), by.y=c("cut","color","clarity","carat2"))
 diamonds$size[diamonds$carat < 0.5] <- "Small"
diamonds$size[diamonds$carat >=0.5 & diamonds$carat < 1] <- "Medium"
diamonds$size[diamonds$carat >= 1] <- "Large"
 barplot(table(diamonds$size), main="Diamond Size Distribution", xlab="Size Category", ylab="Number of Diamonds", col="blue")
 ggplot(diamonds, aes(clarity)) + geom_freqpoly(aes(group = color, colour = color)) + labs(x="Clarity", y="Number of Diamonds", title="Clarity by Color")
 ggplot(diamonds, aes(carat, price, color=clarity)) + geom_point() + 
  labs(x="Carat Weight", y="Price", title="Price by Carat Weight")
 qplot(carat, price, data=diamonds, color=clarity, xlab="Carat Weight", ylab="Price", main="Price by Carat Weight")
 
 
Item 2349: 
runtime = time_func(model.predict, args, kwargs)
 dense_output = dot(input, kernel) + bias
 
 
Item 2355: 
filter(year(timestamp) == year(today()) - 1
 filter(year(timestamp) == 2017)
 
 
Item 2361: 
static RDD<String> fromFile(Path directoryPath){
    return new PartitionedFileRDD(directoryPath);
}
 public RDD(RDD parent) {
    this.sparkContext = parent.getSparkContext();
    this.dependencies = Arrays.asList(new Dependency() {
        @Override
        public RDD rdd() {
            return parent;
        }
    });
}

public <P> RDD<P> getParent(){
    return (RDD<P>)this.dependencies.get(0).rdd();
}
 private class MapPartitionsRDD<T,O> extends RDD<O>{

    Function<Iterator<T>, Iterator<O>> converter;

    public MapPartitionsRDD(RDD parent, Function<Iterator<T>, Iterator<O>> converter) {
        super(parent);
        this.converter = converter;
    }

    @Override
    protected List<Partition> getPartitions() {
        return getParent().getPartitions();
    }

    @Override
    protected Iterator<O> compute(Partition partitionId) {
        Iterator<T> preComputed = (Iterator<T>)getParent().compute(partitionId);
        return converter
                .apply(preComputed);
    }
}
 public <O> RDD<O> mapPartitions(Function<Iterator<T>, Iterator<O>> converter){
    return new MapPartitionsRDD<T, O>(this, converter);
}
 public <O> RDD<O> flatMap(Function<T, Iterator<O>> splitter){
    return this.mapPartitions(pi->pi.flatMap(splitter));
}
 public <O> RDD<O> map(Function<T, O> mapper){
    return this.mapPartitions(pi->pi.map(mapper));
}
 
 
Item 2363: 
select * from "Samples.'samples.dremio.com'"."SF_incidents2016.json"
 
 
Item 2364: 
{
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetBucketLocation",
                "s3:HeadBucket",
                "s3:ListAllMyBuckets"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Resource": [
                "arn:aws:s3:::auger-acmeorg-g1fxrf",
                "arn:aws:s3:::auger-acmeorg-g1fxrf/*"
            ],
            "Effect": "Allow",
            "Action": [
                "s3:*"
            ]
        }
    ],
    "Version": "2012-10-17"
}
 
 
Item 2366: 
def CNN_LSTM(self):
 frames_input = Input(shape=self.input_shape)
    vision_model = Sequential()
    vision_model.add(Conv2D(64, (1, 2), 
                     activation='relu',
                     padding='same', 
                     input_shape=self.image_dim))
    vision_model.add(BatchNormalization())
    vision_model.add(MaxPooling2D((1, 2)))
    vision_model.add(Flatten())
    vision_model.add(BatchNormalization())
    encoded_frame_sequence = TimeDistributed(vision_model) \
                                            (frames_input)
    encoded_video = LSTM(256, 
                         activation='tanh', 
                         return_sequences=True) \
                         (encoded_frame_sequence)
    fc1 = Dense(64, activation='relu',
                kernel_regularizer=regularizers.l2(0.05)) \
               (encoded_video)
    out = Flatten()(fc1)
    out = Dropout(0.5)(out)
    output = Dense(1, activation='relu')(out)
    CNN_LSTM = Model(inputs=frames_input, outputs=output)
    return CNN_LSTM
 
 
Item 2368: 
Your contribution will be highly appreciated on Patreon.
 Your contribution will be highly appreciated on Patreon.
 
 
Item 2374: 
def isItSpam(document):
  if document.contains(“prince”) OR document.contains(“sex”):
    return True;
  else:
    return False;
 x = [1,6,8] //inputs
w = [0.2,0.7,0.6] //weights
The weighting process could be like: x*w = [0.2, 4.2, 4.8]
 Sum of x*w = 0.2+4.2+4.8 = 9.2, which is positive, and therefore a certain category.
 x = [“this”, ”is”, “text”]
W = [0.2, 0.7, 0.6]
x*W =
 Sentence 1 (S1): “vectorize this text to have numbers!”
Sentence 2 (S2): "what does it mean to vectorize?"
Sentence 3 (S3): "document classification is cool"
 V = ['classification', 'cool', 'document', 'does', 'have', 'is', 'it', 'mean', 'numbers', 'text', 'this', 'to', 'vectorize', 'what']
 S1 = [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0]
S2 = [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]
S3 = [1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
 Sentence 1 (S1): “vectorize this text to have numbers!”
Sentence 2 (S2): "what does it mean to vectorize?"
Sentence 3 (S3): "document classification is cool"
 S1 = [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0]
S2 = [0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]
S3 = [1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
 
 
Item 2382: 
Send an initial notification when score > threshold for N hours.
Send another notification when:
- Score doubles (getting worse)
- Score < threshold for M hours (cleared)
- Is ongoing for 24 hours
 
 
Item 2383: 
Te recomiendo leer este post sobre el valor añadido que ofrecen hoy en día las bases de datos jurídicas, escrito conjuntamente con José María Fernández Comas.
 
 
Item 2384: 
agreements.append(2 * len(intersection) / (len(set1) + len(set2))) 
 Falsifying example: test_average_agreement_properties(list1=[1], list2=[1], max_depth=80624)
 for depth in range(1, max_depth+1):
    set1 = set(list1[:depth])
    set2 = set(list2[:depth])
		
    intersection = set1 & set2
		
    agreements.append(
         2 * len(intersection) / (len(set1) + len(set2))
    )
 
 
Item 2386: 
Ridge(alpha=1, normalize=True)
 RandomForestRegressor(n_estimators=200, min_samples_leaf=5, min_samples_split=6)
 
 
Item 2387: 
to_timestamp(substring("Datetime", 15, 21, 'HH24:MI:SS')::time
 SELECT
 to_date(r1."Datetime", 'DD Mon yyyy') as EVENT_DATE,                             
 r1."MatchID" as "ID",                                  -- bigint
 r1."RoundID" as "RoundID",                             -- bigint
 r1."Home Team Initials" as "HomeTeam",                 -- bigint
 r1."Away Team Initials" as "AwayTeam",                 -- bigint
 null as "VenueID"                     
FROM
S_WORLDCUPMATCHES r1                -- replace by your table name
WHERE r1."Datetime" is not null and r1."MatchID" is not null
 SELECT
 to_date(r1."Year" || '-1-1', 'yyyy-mm-dd') as VALID_FROM,                             
 to_date(r1."Year" || '-12-31', 'yyyy-mm-dd') as VALID_TO,                               
 r1."Year" as "ID",                                   -- bigint
 r1."Country" as "Name"                               -- text
FROM
S_WORLDCUPS r1
 
 
Item 2390: 
rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1
 
 
Item 2395: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading.
 This is a huge topic. That’s why the original post was comprised of more than 7,000 and why I have broken it out into five separate posts. Even so, it didn’t cover nearly every aspect of the importance of AI to our future. My hope is to motivate readers to think about this, learn more, and have many conversations with their families and friends.
 I’ve broken the original post down into these six topics—each in their own post. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
  (this post) 
2. What do we mean by intelligence, artificial or otherwise?
3. The how, when, and what of AI
4. AI and the bright future ahead
5. AI and the bleak, darkness that could befall us
6. What should we be doing about AI anyway?
 
 
Item 2397: 
git clone https://github.com/imisi-akande/Cloud-BigData-machine-learning.git 
 bash ./find_my_ip.sh 
 use grade_spark 
 show tables
 select * from Prison
 select * from Rating 
 select * from Grade 
 select * from Prison where total>capacity 
 /sparkml/train_and_apply.py 
 vi sparkml/train_and_apply.py
 bash authorize_dataproc.sh 
 bash authorize_cloudshell.sh
 use grade_spark
 select * from Grade
 select g.personid, g.prisonid, g.prediction, p.states, p.prisons, p.male, p.female, p.total, p.capacity, p.rating from Grade as g, Prison as p where g.prisonid = p.id and g.personid = 13;
 select g.personid, g.prisonid, g.prediction, p.states, p.prisons, p.male, p.female, p.total, p.capacity, p.rating from Grade as g, Prison as p where g.prisonid = p.id and g.personid = 7;
 
 
Item 2398: 
parallel for (batch of data):
   Acquire lock on current parameters
   Read current parameters
   Calculate the gradient w.r.t. the batch and update parameters
   Release lock
end
 parallel for (batch or sample of data):
   Read current parameters
   Calculate the gradient w.r.t. the batch and update parameters
end
 
 
Item 2402: 
Isn’t this backward?
 It may be surprising to try to schedule trains while already having timing constraints. This is because we are considering cases where we want to make small modifications to a pre-existing schedule. 
 For example if a major football match starts at 20:00, it would make sense to modify the usual schedule to have a train arrive close to the stadium no later than 19:00. Similarly, the departure of the train that brings the spectators back should have an earliest departure of maybe 22:30.
 The JSON files describing the problems get very large. An extension such as JSONView can help you understand the simpler files. Beyond that a good way to visualise them is as tables: the one above was rendered using http://json2table.com/.
 Durations are expressed per ISO_8601 standard. The “P” in "PT3M" is the duration designator, which stands for period. The "T" stands for time, and "M" for minutes.
 Challenge Links
 Challenge page: https://www.crowdai.org/challenges/train-schedule-optimisation-challenge
 Starter kit: https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/
 Gitter channel: https://gitter.im/crowdAI/sbb-challenges
 
 
Item 2403: 
Weights transposed * (Covariance matrix * Weights)
 Expected Return of Investment 1 = 7.5%
Expected Return of Investment 2 = 15.0%
Expected Return of Portfolio = 11.25%
Standard Deviation of Investment 1 = 1.77%
Standard Deviation of Investment 1 = 3.54%
Correlation between Returns of 1 & 2 = -1.0
Risk of Portfilio = 0.88%
 
 
Item 2406: 
Function INTENT(query As String) As String
' IBM WATSON conversation service

    If Len(query) > 0 Then 'If query is not empty
        Dim xmlhttp As New MSXML2.XMLHTTP60, workspaceID As String, workspaceURL As String, authUsername As String, authPassword As String, response As String, body As String
        
        ' WATSON Conversation Credentials
        workspaceID = "{insert workspaceID here}"
        workspaceURL = "https://gateway.watsonplatform.net/conversation/api/v1/workspaces/" + workspaceID + "/message?version=2017-05-26" 'API requests require a version parameter
        authUsername = "{insert username here}"
        authPassword = "{insert password here}"

        
        body = "{""input"":{""text"":"" " + Replace(query, Chr(34), "") + " ""}}" 'The user input in JSON format. Also removed double quotes from input
        
        xmlhttp.Open "POST", workspaceURL, False, authUsername, authPassword
        xmlhttp.setRequestHeader "Content-Type", "application/json"
    
        xmlhttp.Send body
        
        If xmlhttp.Status = 200 Then 'Request success
            response = xmlhttp.responseText
            INTENT = parseINTENT(response)
        Else 'Request fail
            INTENT = "#ERROR: " & xmlhttp.Status & " - " & xmlhttp.responseText
        End If
    End If
End Function

Function parseINTENT(text As String) As String
'Parse intent and confidence from JSON response

    Dim p1 As Integer, p2 As Integer
   
    p1 = InStr(text, "[{" + Chr(34) + "intent")
    p2 = InStr(p1, text, "}]")
    parseINTENT = Mid(text, p1, (p2 - p1) + 2)
    
End Function
 =INTENT(A2)
 
 
Item 2407: 
at_risk = ( age_is_25_30 & white_blood_cell_is_low ) or ( age_is_0_5 ) or ( age_is_65_70 & sex_is_male ) or ...
 expression = lambda x1,x2 : \\
    (( (x1>0) & (x2>0) ) or ( !(x1>0) & !(x2>0) ))*2 -1
 XNOR = lambda A,B : ( A & B) or ( !A & !B )
 (( (x1>0) & (x2>0) ) or ( !(x1>0) & !(x2>0) ))*2 -1
 
 
Item 2409: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
2. What do we mean by intelligence, artificial or otherwise? 
3. The how, when, and what of AI (this post)
4. AI and the bright future ahead
5. AI and the bleak, darkness that could befall us
6. What should we be doing about AI anyway?
 
 
Item 2410: 
{
  "intents": [
    {
      "slots": [
        {
          "name": "food_item",
          "type": "FOOD_TYPE"
        }
      ],
      "intent": "OrderIntent"
    },
    {
      "intent": "ThanksIntent"
    }
  ]
}
 OrderIntent for {food_item}
OrderIntent to get me {food_item}
ThanksIntent Thank you very much
ThanksIntent No thanks. 
ThanksIntent No thank you.
ThanksIntent No that's all. Thank you.
 "Alexa, ask My Butler for fruit"       <-- matched correctly. 
"Alexa, ask My Butler to get me pizza" <-- matched correctly. 
"Alexa, ask My Butler for pizza"       <-- NOT matched correctly. 
 for {food_item}        <-- matches "for pizza" ?
to get me {food_item}
Thank you very much
No thanks. 
No thank you.
No that's all. Thank you.
 UTTERANCE DEFINITION: 
---------------------
for {food_item}
 WHAT THE USER SAYS: 
-------------------
for pizza                    <-- will not match :( 
 UTTERANCE DEFINITION: 
---------------------
buy {food_item}
 WHAT THE USER SAYS: 
-------------------
buy pizza                    <-- WILL match :) 
 UTTERANCE DEFINITION: 
---------------------
for {food_item}
 WHAT THE USER SAYS: 
-------------------
for pizza                    <-- will not match :( 
 UTTERANCE DEFINITION: 
---------------------
OrderIntent for more {food_item}
 WHAT THE USER SAYS: 
-------------------
for more pizza               <-- WILL match :) 
 UTTERANCE DEFINITION: 
---------------------
for more {food_item}
 WHAT THE USER SAYS: 
-------------------
for more pizza               <-- WILL match 
for more bottles of beer     <-- will NOT match 
 OrderIntent for {food_item}
OrderIntent to get me {food_item}
ThanksIntent Thank you very much
ThanksIntent No thanks. 
ThanksIntent No thank you.
ThanksIntent No that's all. Thank you.
 
 
Item 2411: 
[Out]
[8.0, 2.236068, 104.56122]
 
 
Item 2412: 
np.put(np_arr, range(3), py_arr)
 
 
Item 2413: 
> general computer problem
  - utter_confirm_need_help_now
* yes
  - action_create_ticket
  - utter_expect_specialist_soon
 > general computer problem
  - utter_confirm_need_help_now
* yes
  - utter_bring_device_to_helpdesk
 
 
Item 2414: 
library(tidyverse)
 df <- data.frame(
  x1 = runif(1000, 0, 300),
  x2 = runif(1000, 0, 300),
  x3 = runif(1000, 0, 300),
  e  = rnorm(1000, mean = 0, sd = 2)
)
 df$y <- .1*df$x1 +
  20/(1+ exp(-.1*(df$x2 - 100))) +
  sqrt(df$x3) +
  df$e
 test     <- sample(1:nrow(df), 200)
df_train <- df[-test, ] %>% select(-e)
df_test  <- df[test, ] %>% select(-e)
 fit_lm <- lm(y ~ x1 + x2 + x3, data = df_train)
 fit_gam <- mboost::gamboost(
  formula = as.formula(paste0(
    "y ~ bbs(x1, knots=4, degree=3) + ",
    "bbs(x2, knots=4, degree=3) + ", 
    "bbs(x3, knots=4, degree=3)")),
  data = df_train,
  control = boost_control(mstop = 2500)
)
 
 
Item 2415: 
python slitherin.py --human
 python slitherin.py --shortest_path_bfs
 python slitherin.py --shortest_path_dfs
 python slitherin.py --longest_path
 python slitherin.py --hamilton
 python slitherin.py --deep_neural_net
 python slitherin.py --deep_neural_net_monte_carlo
 
 
Item 2419: 
'wiki.hair', 'wiki.weight', 'wiki.height', 'wiki.eyes', 'wiki.place_of_birth', 'wiki.education', 'wiki.citizenship', 'wiki.occupation', 'wiki.bio', 'wiki.bio_text', 'wiki.categories', 'name', 'comics.available'
 
 
Item 2421: 
CMD-SHELL curl -f http://localhost:8000/ || exit 1
 
 
Item 2422: 
## int64
## [ 9 23 33 91 13]
## object
## ['9' '23' '33' '91' '13']
 ## [1] "numeric"
## [1]  9 23 33 91 13
## [1] "character"
## [1] "9"  "23" "33" "91" "13"
 ## [1] FALSE FALSE FALSE FALSE FALSE  TRUE
##   number     player college
## 6      7 Tony Kukoc    <NA>
 ## [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
 ## [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
 ## [1] "character"
 ## [1] "Date"
 
 
Item 2423: 
Author’s note: Because this issue is so crucial to our lives, I’ve broken the original post down into these six topics—each in their own post to allow for easier reading. Feel free to jump around.
 1. An introduction to the opportunities and threats we face as we 
   near the realization of human-level artificial intelligence 
2. What do we mean by intelligence, artificial or otherwise? 
3. The how, when, and what of AI
4. AI and the bright future ahead
5. AI and the bleak, darkness that could befall us (this post)
6. What should we be doing about AI anyway?
 
 
Item 2424: 
def download_images(searchword, form="jpg", lim=100, directory= "data"):
    if not os.path.isdir(directory):
        ! mkdir $directory
    ! googleimagesdownload --keywords $searchword --format $form --limit $lim --output_directory $directory 
    src_path = os.path.join(directory, searchword)
    if not os.path.isdir(src_path):
        ! mkdir $src_path
 for path in file_paths:
    for files in os.listdir(path):
        file_path = os.path.join(path, files)
        if imghdr.what(file_path != 'jpeg':
            os.remove(file_path);
 tfms = tfms_from_model(arch, sz, aug_tfms=augmentation, max_zoom=1.1)
data = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs, num_workers=1)
learn = ConvLearner.pretrained(arch, data, ps=0.4)
learn.fit(1e-3, 1)
 learn.lr_find()
learn.sched.plot()
 learn.unfreeze()
 learn.save("Resnet34_multiclass")
 
 
Item 2431: 
git clone https://github.com/googlecodelabs/tensorflow-for-poets-2
 ls tf_files/flower_photos
 --bottleneck_dir=tf_files/bottlenecks 
 --model_dir=tf_files/models/"${ARCHITECTURE}" \
  --summaries_dir=tf_files/training_summaries/"${ARCHITECTURE}" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
 --image_dir=tf_files/flower_photos
 python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/"${ARCHITECTURE}" \
  --summaries_dir=tf_files/training_summaries/"${ARCHITECTURE}" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --image_dir=tf_files/flower_photos
 python -m scripts.label_image \
    --image=tf_files/flower_photos/daisy/test_image.jpg
 
 
Item 2433: 
summarize( weight_90pct = quantile(weight_pounds, probs=0.9, na.rm=TRUE)
 quantile(weight_pounds, probs=0.9, na.rm=TRUE)
 
 
Item 2436: 
curl -u "${DISCOVERY_USERNAME}:${DISCOVERY_PASSWORD}" "https://gateway.watsonplatform.net/discovery/api/v1/environments/${DISCOVERY_ENVIRONMENT_ID}/configurations/${DISCOVERY_CONFIG_ID}?version=2017-09-01" > collection_config.json
 "entities": {
 "sentiment": true,
 "emotion": false,
 "limit": 50,
 "model": "<watson knowledge studio model ID>"
}
 curl  -X PUT -u "${DISCOVERY_USERNAME}:${DISCOVERY_PASSWORD}" -H "Content-Type: application/json" -d @collection_config.json "https://gateway.watsonplatform.net/discovery/api/v1/environments/e${DISCOVERY_ENVIRONMENT_ID}/configurations/${DISCOVERY_CONFIG_ID}?version=2017-09-01"
 
 
Item 2440: 
class TfIdf
  attr_accessor :tf, :idf, :tf_idf
 def initialize(corpus=[], options={})
    @stopwords = File.read('stopwords.txt')
    @docs = process_corpus(corpus, options[:ngram], options[:min_length])
    @ngram = options[:ngram]
    @min_length = options[:min_length]
    @tf = []
    @idf = {}
    @tf_idf = []
    @docs_size = @docs.size
    compute_tf_and_idf
    compute_tf_idf(options[:limit])
  end
 def ngrams(n, string, min_length) 
    string.downcase.split(/\W+/).reject{|r| (r.length < min_length) || (@stopwords.include?(r)) }.each_cons(n).to_a.map{|c| c.join(" ")}
  end
 def process_corpus(corpus, ngram, min_length)
    corpus.map{|c| ngrams(ngram, c, min_length)}
  end
 def compute_tf_and_idf
    @docs.each do |words|
      terms_freq_in_words = words.inject(Hash.new(0)) { |h, e| h[e] += 1 ; h }
      @tf.push(terms_freq_in_words)
      distinct_words = words.uniq
      distinct_words.each do |w|
        if ( @idf.has_key?(w) )
          y = @docs_size / ( 10**(@idf[w]) )
          y += 1
          @idf[w] = Math.log10(@docs_size / y)
        else
          @idf[w] = Math.log10(@docs_size)
        end
      end
    end
  end
 def compute_tf_idf(limit)
    unless @tf.empty?
      tf_freq = @tf.first #Only calculate TF IDF for the first item
      tfidf = Hash.new(0)
      tf_freq.each do |key,value|
        tfidf[key] = @idf[key] * value
      end
      tfidf = Hash[tfidf.sort_by { |k,v| -v }[0..limit-1]]
      @tf_idf.push(tfidf)
    end
    @tf_idf = @tf_idf.map(&:keys).flatten
  end
end
 require './tf_idf'
 movies = Dir["./subs/*.srt"]
movie_sub = {}
movie_freqs = {}
movies.each do |movie|
  movie_name = movie.gsub("./subs/","").gsub(".srt","")
  movie_sub[movie_name] = File.read(movie, encoding: 'iso-8859-1').split("\r\n").reject{|t| t=="" || t.to_i > 0 || t.include?("-->")}.join(' ').gsub("...", "").gsub("<i>", "").gsub("</i>","").gsub(/["!()\[\]]/,"")
  movie_freqs[movie_name] = {1 => [], 2 => [], 3 => []}
end
 movie_sub.keys.each do |movie_name|
  temp_sub = movie_sub.clone
  corpora = [temp_sub.delete(movie_name)] + temp_sub.values
  (1...4).each do |ngram|
    @tf_idf = TfIdf.new corpora, limit: 8, ngram: ngram, min_length: 3
    movie_freqs[movie_name][ngram] += @tf_idf.tf_idf
  end
end
 movie_freqs.keys.each do |movie_name|
  puts "**********************************"
  puts "Unigrams for #{movie_name}"
  puts "**********************************"
  puts movie_freqs[movie_name][1]
  puts
end
 ariadne
 gunfire
 
 
Item 2442: 
python retrain.py \
 — image_dir ./DL3Dataset \
 — tfhub_module https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/1
 
 
Item 2448: 
Network 1 –  Defines the bounding box, telling network 2 where to look
Network 2 – Classifies the lesion, tells us what we are looking at
 
 
Item 2455: 
Que 1# Which attribute should choose as for being the Root/Parent Node? > After deciding the root node?
 Que 2# When should I Start/Stop Split further Node?
 Entropy(S) = -p(positive)log2 p(positive) — p(negative) log2 p(negative)
 Information Gain (n) =
  Entropy(x) — ([weighted average] * entropy(children for feature))
 outlook = { sunny, overcast, rain }
 temperature = {hot, mild, cool }
 humidity = { high, normal }
 wind = { weak, strong }
 Entropy(S) = — (9/14) Log2 (9/14) — (5/14) Log2 (5/14) = 0.940
 Gain(S,Wind) = Entropy(S) —  { (8/14)* Entropy(Sweak) — (6/14)* Entropy(Sstrong) }
 = 0.940 — { (8/14)*0.811 — (6/14)*1.00 } = 0.048
 Entropy(Sweak) = — (6/8)*log2(6/8) — (2/8)*log2(2/8) = 0.811
 Entropy(Sstrong) = — (3/6)*log2(3/6) — (3/6)*log2(3/6) = 1.00
 S sunny = {D1, D2, D8, D9, D11} = 5 examples from the table with outlook = sunny
 Gain(Ssunny, Humidity) = 0.970
 Gain(Ssunny, Temperature) = 0.570
 Gain(Ssunny, Wind) = 0.019
 IF outlook = sunny AND humidity = high THEN play baseball = no
 IF outlook = rain AND humidity = high THEN play baseball = no
 IF outlook = rain AND wind = strong THEN play baseball = yes
 IF outlook = overcast THEN play baseball = yes
 IF outlook = rain AND wind = weak THEN play baseball = yes
 
 
Item 2458: 
The result is:
h_bushroh
244
david_faniyi
Moustapha_6C
mbao_01
OauNacoss
pyconcharlas
DjangoGirlsPyUK
pyconindia
elonmusk
FrontendMasters
Elishatofunmi
intelaiiot
DurexNG
nextdeegit
PrincesOluebube
gitlab
Djangotarkwa
pykidsghana
pydataghana
instadeepai
forloopoau
 
 
Item 2459: 
.zip file
|-images directory
  |-image files (filename format: objectclass_id.jpg)
|-annotations directory
  |-xmls directory
    |-annotations files (filename format: objectclass_id.xml)
 MODEL = 'faster_rcnn_inception_v2_coco_2018_01_28'
MODEL_FILE = MODEL + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'
DEST_DIR = 'pretrained_model'
 
 
Item 2461: 
!(export sv=2.1 bv=0.3.0 ; cd ~/data/libs/ && wget https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_${sv}/${bv}/bigdl-SPARK_${sv}-${bv}-jar-with-dependencies.jar)
 !pip install bigdl==0.3.0 | cat
 %AddJar https://repo1.maven.org/maven2/com/intel/analytics/bigdl/bigdl-SPARK_2.1/0.3.0/bigdl-SPARK_2.1-0.3.0-jar-with-dependencies.jar
 
 
Item 2468: 
df %>% group_by(Year)
 df %>% 
  group_by(Year) %>%
  mutate(numbers_per_K_percent_rank = percent_rank(numbers_per_K))
 
 
Item 2470: 
PS > dotnet new mvc -lang F#
 PS > dotnet publish
PS > gcloud beta app deploy ` .\bin\Debug\netcoreapp1.1\publish\app.yaml
 
 
Item 2471: 
python google-images-download.py --keywords "peach" --output_directory ..\data --csv=true  --limit 10
 python create_coco_tf_record.py --train_annotations_file=../data/coco_labels.json --val_annotations_file=../data/coco_labels.json --testdev_annotations_file=../data/coco_labels.json   --train_image_dir=../data/images/train --val_image_dir=../data/images/val --test_image_dir=../data/images/test --output_dir=../data
 python train.py --logtostderr --train_dir=../model/train --pipeline_config_path=../model/faster_rcnn_resnet50_coco.config
 python export_inference_graph.py --input_type image_tensor --pipeline_config_path ../model/faster_rcnn_resnet50_coco.config --trained_checkpoint_prefix ../model/train/model.ckpt-0 --output_directory ../model/fine_tuned_model
 python import_pb_to_tensorboard.py --model_dir ../model/fine_tuned_model/frozen_inference_graph.pb --log_dir ../model/fine_tuned_model/logdir
 tensorboard --logdir=../model/fine_tuned_model/logdir
 python -m infer_detections --input_tfrecord_paths=../data/coco_testdev.record --output_tfrecord_path=../data/inference --inference_graph=../model/fine_tuned_model/frozen_inference_graph.pb --discard_image_pixels
 
 
Item 2473: 
(‘previous mean’ * ‘(count -1)’) + ‘new value’) / ‘count’
 
 
Item 2474: 
```{r fig.width=8,fig.height=6}
 library(ggplot2)
library(ggrepel)
 ggplot(ballot_measures_mds, aes(axis1, axis2)) +
geom_point(aes(colour = cluster)) +
 geom_text_repel(aes(axis1, axis2, label = county_name), size = 3)
 ```
 
 
Item 2485: 
Title Goes Here
Bolded Text
Italicized Text
 But this will still be here!
 I run. He ran. She is running. Will they stop running?
 I talked. She was talking. They talked to them about running. Who ran to the talking runner?
 ¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!
 something... is! wrong() with.,; this :: sentence.
 I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?
 My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.
 do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.
 John: "Well, well, well."
James: "There, there. There, there."
 There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.
I have to go get 2 tutus from 2 different stores, too.
 22    45   1067   445
 {{Here is some stuff inside of double curly braces.}}
{Here is more stuff in single curly braces.}
 ['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still',
'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 
'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 
'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡Sebastián', ',', 
'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'tot', 'he', 'store', 'tomorrow', 
'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 
'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 
'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 
'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Star', 'Wars', ';', 'Marvel', 
'Cinematic', 'Universe', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'do', 'not', 
'do', 'it', '...', '.', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 
'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', 
':', '``', 'Well', ',', 'well', ',', 'well', '.', "''", 'James', ':', '``', 'There', ',', 'there', '.', 
'There', ',', 'there', '.', "''", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', 
'.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 
'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 
'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 
'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', 
'.', '}']
 ['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', 
'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', 
'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 
'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', 
'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', 
'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', 
'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', 
'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 
'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', 
'curly', 'braces']
 Stemmed:
 ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', 
'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', 
'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', 
'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', 
'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', 
'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', 
'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred 
and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']
 Lemmatized:
 ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', 
'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 
'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 
'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', 
'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 
'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', 
'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', 
'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 
'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']
 
 
Item 2491: 
#pragma mark - Public
 - (NSUInteger)startAlgorithmWithAITurn:(BOOL)aiTurn; {
     return [self alphabetaAlgorithm:_searchingDeapth    alpha:NSIntegerMin beta:NSIntegerMax maximizing:aiTurn];
 }
 #pragma mark - Private
 - (NSInteger)alphabetaAlgorithm:(NSInteger)depth alpha:(NSInteger)alpha beta:(NSInteger)beta maximizing:(BOOL)maximizing {
     self.currentCheckingDepth = _searchingDeapth - depth;
     if (self.datasource == nil || self.delegate == nil) {
         return 0;
     }
     if (depth == 0 || [self.datasource checkStopConditionForAlgorithm:self onAITurn:maximizing]) {
         return [self.datasource evaluateForAlgorithm:self  onAITurn:maximizing];
     }
     NSArray *nextStates = [self.datasource  possibleNextStatesForAlgorithm:self onAITurn:maximizing];
     for (id state in nextStates) {
         [self.delegate performActionForAlgorithm:self andState:state onAITurn:maximizing];
         if (maximizing) {
             alpha = MAX(alpha, [self alphabetaAlgorithm:depth - 1 alpha:alpha beta:beta maximizing:NO]);
         } else {
             beta = MIN(beta, [self alphabetaAlgorithm:depth - 1  alpha:alpha beta:beta maximizing:YES]);
         }
         [self.delegate undoActionForAlgorithm:self andState:state onAITurn:maximizing];
         if (beta <= alpha) {
             break;
         }
     }
     return (maximizing ? alpha : beta);
 }
 
 
Item 2494: 
ga('send', 'event', [eventCategory], [eventAction], [eventLabel], [eventValue], [fieldsObject]);
 
 
Item 2495: 
lego_green_normal_hsv_low=np.array([62,32,47])
lego_green_normal_hsv_high=np.array([105,78,71])
 lego_black_normal_hsv_low=np.array([0,10.23,3.53])
lego_black_normal_hsv_high=np.array([65,100,20])
 raw_mask = cv2.inRange(hsv_img, lego_black_opencv_hsv_low, lego_black_opencv_hsv_high)
.
.
.
raw_mask = cv2.inRange(hsv_img, lego_green_opencv_hsv_low, lego_green_opencv_hsv_high)
 kernelOp = np.ones((1,1),np.uint8)
kernelCl = np.ones((11,10),np.uint8)
.
.
.
ret,imBin= cv2.threshold(raw_mask,200,255,cv2.THRESH_BINARY)
mask = cv2.morphologyEx(imBin, cv2.MORPH_OPEN, kernelOp)
mask =  cv2.morphologyEx(mask , cv2.MORPH_CLOSE, kernelCl)
.
.
.
ret,imBin= cv2.threshold(raw_mask,200,255,cv2.THRESH_BINARY)
mask = cv2.morphologyEx(imBin, cv2.MORPH_OPEN, kernelOp)
mask =  cv2.morphologyEx(mask , cv2.MORPH_CLOSE, kernelCl)
 _, contours, _  = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
.
.
.
_, contours, _  = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
 
 
Item 2498: 
Simple is better than complex.
Readability counts.
If the implementation is hard to explain, it's a bad idea.
 
 
Item 2510: 
X = c(3,2,1) 
X 
3 2 1 
sort(X)
[1] 1 2 3
rank(X)
[1] 1 2 3
order(X)
[1] 1 2 3
 X = c(2,3,1) 
X 
2 3 1
sort(X)
[1] 1 2 3
rank(X)
[1] 2 3 1
order(X)
[1] 3 1 2
 X = c(5,6,4) 
X 
5 6 4
sort(X)
[1] 4 5 6
rank(X)
[1] 2 3 1
order(X)
[1] 3 1 2
 X = c(1,2,3)
RANK(X) == ORDER(X) == X
RANK(ORDER(X)) == X
ORDER(RANK(X)) == 1:length(X)
 X = c(1,2,3)
all(order(rank(X)) == X)
[1] TRUE
 X = c(2,3,1)
all(order(rank(X)) == X)
[1] FALSE
 X = c(3,1,2)
all(order(rank(X)) == X)
[1] FALSE
 X = c(1,3,2)
all(order(rank(X)) == X)
[1] TRUE
 X = c(2,1,3)
all(order(rank(X)) == X)
[1] TRUE
 X = c(3,2,1)
all(order(rank(X)) == X)
[1] TRUE
 all(order(X)[rank(X)] == rank(x)[order(X)]) == 1:length(X)
TRUE
 X = c(100,200,300)
all(order(X)[rank(X)] == rank(X)[order(X)])
TRUE
 x = c(100,200,300)
all(order(X)[rank(X)] == 1:length(X))
TRUE
all(rank(X)[order(X)] == 1:length(X))
TRUE
1:length(X)
[1] 1 2 3
 size = 5
sex=sample(c("male","female"),size,replace=T)
age = sample(21:100, size, replace=T)
degree = sample(c("BA","BS","MS","MBA"), size, replace=T)
(test.data = data.frame(sex=sex, age=age, degree=degree))
 sex age  degree
1 female  30     BA
2   male  49     BA
3   male  39    MBA
4   male  27     MS
5   male  61     MS
 order(test.data$age)
[1] 4 1 3 2 5
 test.data[order(test.data$age),]
sex age degree
4   male  27     MS
1 female  30     BA
3   male  39    MBA
2   male  49     BA
5   male  61     MS
 order(-test.data$age)
[1] 5 2 3 1 4
 test.data[order(-test.data$age),]
 5   male  61     MS
2   male  49     BA
3   male  39    MBA
1 female  30     BA
4   male  27     MS
 order(test.data$degree,-test.data$age)
[1] 2 1 3 5 4
test.data[order(test.data$degree,-test.data$age),]
 sex age degree
2   male  49     BA
1 female  30     BA
3   male  39    MBA
5   male  61     MS
4   male  27     MS
 (language.ieee =read.csv(file="language-scores.csv"))
    X   language score
1   2          C  99.7
2   5         C#  87.7
3   4        C++  97.1
4   9         Go  75.1
5   3       Java  99.5
6   7 JavaScript  85.6
7   8        PHP  81.2
8   1     Python 100.0
9   6          R  87.7
10 10      Swift  73.1
 rank(language.ieee$score)
9.0  5.5  7.0  2.0  8.0  4.0  3.0 10.0  5.5  1.0
 language.ieee[order(-rank(language.ieee$score)),]
    X   language score
8   1     Python 100.0
1   2          C  99.7
5   3       Java  99.5
3   4        C++  97.1
2   5         C#  87.7
9   6          R  87.7
6   7 JavaScript  85.6
7   8        PHP  81.2
4   9         Go  75.1
10 10      Swift  73.1

 
 
Item 2513: 
getResponseFromAgent(payload) {
 return this._client.textRequest(payload);
 }
 sendMessage(flag) {
    let splitString = [];
    let customMessage = '';
    const tempMessage = this.userMessage;
    let userResponse = {
      url: this.user_url,
      text: this.userMessage
    };
    if (!flag) {
      this.messages.push(userResponse);
    }
    this.userMessage = '';
    this.service.getResponseFromAgent(tempMessage).then(result => {
    let url = result.result.fulfillment.speech;
    if (result.result.action.toString() === 'get-movie-details') {
      splitString = url.split('url');
      customMessage = splitString[0];
      this.poster_path = splitString[1] ? splitString[1].replace(/['"]+/g, '') : '';
    }
      let botResponse = {
        url: this.bot_url,
        text: customMessage ? customMessage : url,
        poster_path: this.poster_path ? 'http://image.tmdb.org/t/p/w154/' + this.poster_path : ''
      };
      this.messages.push(botResponse);
    });
    this.poster_path = '';
  }
 <div *ngFor="let message of messages">
    <span><img src="{{message.url}}" style="height:30px;"></span>
    <span style="white-space: pre-line"><p>{{message.text}}</p>      </span>            
   <span *ngIf="message.poster_path"><img src="{{message.poster_path}}"></span>
</div>
<div class="form-group"></div>
    <div class="input-group">
        <input type="text" class="form-control" [(ngModel)]="userMessage"> &nbsp;
         <span class="input-group-btn">
             <button class="btn btn-success" type="submit"      (click)="sendMessage(0)">Send</button></span>
</div>
 
 
Item 2515: 
{
   "architecture": "lstm-attention",
   "date-range": [["2017–01–01", "2017–04–01"]],
   "parameters": {
     "num_of_attentions": [1, 2, 3],
     "attention_hidden_layer_size": [32, 128, 256],
     "attention_regularization": [0.1, 0.01, 0.001]
   }
 }
 
 
Item 2516: 
lines = spark.textFile(logfile)
errors = lines.filter(_.startsWith("ERROR"))
errors.persist()
errors.filter(_.contains("HDFS")).map(_.split("\t")(3)).collect()
 
 
Item 2519: 
install.packages(c('tidyverse','Hmisc', 'funModeling','reshape2' ,'caret', 'data.table', 'lubridate', 'zoo', 'knitr','infotheo','RColorBrewer','minerva', 'roxygen2','Lock5Data', 'shiny', 'scales','corrplot','feather', 'gridExtra', 'xgboost', 'gbm', 'randomForest', 'devtools'))
 randomForest::randomForest(mtcars, formula = wt ~ qsec)
 funModeling::plot_num(mtcars)
 
 
Item 2520: 
library(shiny)
shiny::runApp()
 input values => R code => output values
 
 
Item 2521: 
tf.train.write_graph(sess.graph_def, directory_to_save_model, model_name, as_text=False)
 mkdir -p ~/graphs
curl -o ~/graphs/inception5h.zip \
https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip && unzip ~/graphs/inception5h.zip -d ~/graphs/inception5h
 gf = tf.GraphDef()
 gf.ParseFromString(open(‘tensorflow_inception_graph.pb’, ‘rb’).read())
 with open(txt_file, ‘w’) as f:
     for n in gf.node:
         f.write(n.name + ‘ => ‘ + n.op + ‘\n’)
 git clone https://github.com/tensorflow/tensorflow.git
 python tensorflow/python/tools/freeze_graph.py \
 — input_graph=path-to-model-graph \
 — output_graph=path-to-frozen-model \
 — input_checkpoint=path-to-checkpoint-file \
 — output_node_names=softmax_1 \
 — input_binary=true
 bazel build tensorflow/tools/graph_transforms:transform_graph
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
 — in_graph=path-to-frozen-model \
 — out_graph=path-to-quantized-model \
 — inputs=input_1 \
 — outputs=softmax_1 \
 — transforms='add_default_attributes strip_unused_nodes(type=float, shape="1,180,180,3") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'
 bazel build tensorflow/contrib/util/convert_graphdef_memmapped_format
bazel-bin/tensorflow/contrib/util/convert_graphdef_memmapped_format \
 — in_graph=path-to-frozen-model \
 — out_graph=path-to-memmaped-model
 xcode-select — install
 pod install
 const bool model_uses_memory_mapping = true;
 const int wanted_input_width = 180;
 const int wanted_input_height = 180;
 const int wanted_input_channels = 3;
 const float input_r_mean = 123.68f;
 const float input_g_mean = 116.78f;
 const float input_b_mean = 103.94f;
 const std::string input_layer_name = "input_1";
 const std::string output_layer_name = "softmax_1";
 for (int c = 0; c < wanted_input_channels; ++c) {
     out_pixel[c] = (in_pixel[c] — input_mean) / input_std;
 }
 out_pixel[2] = in_pixel[2] — input_r_mean;
 out_pixel[1] = in_pixel[1] — input_g_mean;
 out_pixel[0] = in_pixel[0] — input_b_mean;
 open tf_camera_example.xcworkspace
 
 
Item 2524: 
K.any(y_true,axis=1,keepdims=True)
 pred=K.argmax(y_pred,axis=-1)
 K.equal(pred,cls)
 K.not_equal(pred,cls)
 
 
Item 2532: 
(adjusted - first(adjusted)) / first(adjusted) * 100
 
 
Item 2534: 
{
  "type" : "index_hadoop",
  "spec" : {
    "ioConfig" : {
      "type" : "hadoop",
      "inputSpec" : {
        "type" : "static",
        "paths" : "sample.csv"
      }
    },
    "dataSchema" : {
      "dataSource" : "sample",
      "granularitySpec" : {
        "type" : "uniform",
        "segmentGranularity" : "day",
        "queryGranularity" : "none",
        "intervals" : ["2010-09-12/2018-09-13"]
      },
      "parser" : {
        "type" : "hadoopyString",
        "parseSpec": {
    "format" : "csv",
    "timestampSpec" : {
      "column" : "timestamp"
    },
    "columns" : ["timestamp","page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city","added","deleted","delta"],
    "dimensionsSpec" : {
      "dimensions" : ["page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","country","region","city"]
    }
  }
      },
      "metricsSpec" : [
        {
          "name" : "count",
          "type" : "count"
        }
      ]
    },
    "tuningConfig" : {
      "type" : "hadoop",
      "partitionsSpec" : {
        "type" : "hashed",
        "targetPartitionSize" : 5000000
      },
      "jobProperties" : {}
    }
  }
}
 2013-08-31T01:02:33Z,"Gypsy Danger","en","nuclear","true","true","false","false","article","North America","United States","Bay Area","San Francisco",57,200,-143
2013-08-31T03:32:45Z,"Striker Eureka","en","speed","false","true","true","false","wikipedia","Australia","Australia","Cantebury","Syndey",459,129,330
2013-08-31T07:11:21Z,"Cherno Alpha","ru","masterYi","false","true","true","false","article","Asia","Russia","Oblast","Moscow",123,12,111
2013-08-31T11:58:39Z,"Crimson Typhoon","zh","triplets","true","false","true","false","wikipedia","Asia","China","Shanxi","Taiyuan",905,5,900
2013-08-31T12:41:27Z,"Coyote Tango","ja","cancer","true","false","true","false","wikipedia","Asia","Japan","Kanto","Tokyo",1,10,-9
 
 
Item 2538: 
[{CharacterName = "Frodo"; CharacterMentions = 1980;};   {CharacterName = "Sam"; CharacterMentions = 1321;};
 {CharacterName = "Gandalf"; CharacterMentions = 1117;}; ... ]
 val getAllCharacterCounts : (string * int) list =
 [("Aragorn", 61); ("Frodo", 57); ("Sam", 55); ("Gandalf", 51);
 ("Pippin", 47); ("Gimli", 46); ("Merry", 44); ("Legolas", 34);
 ("Gollum", 21); ("Boromir", 15); ("Faramir", 15); ("Saruman", 10)]
 val mostDominantRace : string * int = ("Men", 240) 
 [("Gimli", "Legolas", 0.1329787234); 
 ("Merry", "Pippin", 0.2234042553);
 ("Frodo", "Sam", 0.25)]
 [("Gimli", "Legolas", 0.1595744681); 
 ("Merry", "Pippin", 0.170212766); 
 ("Frodo", "Sam", 0.2446808511)]
 val relationshipScores : RelationshipScore list =
 [("Frodo", "Sam", 49.46808511); 
 ("Merry", "Pippin", 39.36170213);
 ("Gimli", "Legolas", 29.25531915)]
 val bestRelationship : RelationshipScore = ("Frodo", "Sam", 49.46808511)
 < The Fellowship Of The Ring ~ A Long-expected Party >
 
 
Item 2541: 
|-- core/
    |-- tests/
        |-- test_pull_data.py
        |-- test_prepare_data.py
        |-- test_model.py
        |-- test_deploy.py
        |-- test_utils.py
    |-- pull_data.py
    |-- prepare_data.py
    |-- model.py
    |-- deploy.py
    |-- utils.py
|-- experiment_1/
    |-- data/
        |-- training.csv
        |-- validation.csv
        |-- test.csv
    |-- output/
        |-- results.json
        |-- models/
            |-- model1
            |-- model2
    |-- job_config.py
    |-- build_data.py
    |-- train.py
    |-- evaluate.py
    |-- prod.py
|-- experiment_2/
    |-- data/
        |-- training.csv
        |-- validation.csv
        |-- test.csv
    |-- output/
        |-- results.json
        |-- models/
            |-- model1
            |-- model2
    |-- job_config.py
    |-- build_data.py
    |-- train.py
    |-- evaluate.py
    |-- prod.py
 
 
Item 2542: 
mutate(order_date_year = year(order_date))
 mutate(order_date_year = year(order_date), order_date_month = month(order_date))
 order_date_month <=2
 mutate(order_date_year = year(order_date), order_date_month = month(order_date), adjustment = ifelse(order_date_month <=2, -1, 0))
 ifelse(order_date_month <=2, - 1, 0)
 mutate(order_date_year = year(order_date), order_date_month = month(order_date), adjustment = ifelse(order_date_month <=2, - 1, 0), year_adjusted = order_date_year + adjustment)
 
 
Item 2548: 
ssh -L 8888:localhost:8888 193.71.x.x
 ssh -R 8888:localhost:8888 193.72.x.x
 ./ngrok http 8888
 
 
Item 2550: 
product1_action = product1-favourite-add
 action_type = add
 action_name = favourite
 product_name = product1
 
 
Item 2551: 
face_score_threshold = 3
 face_score_mask = face_score > face_score_threshold
second_face_score_mask = np.isnan(second_face_score)
unknown_gender_mask = np.logical_not(np.isnan(gender_classes))
 age_classes = np.array([calc_age(photo_taken[i], dob[i]) 
                        for i in range(len(dob))
              ])
valid_age_range = np.isin(age_classes, [x for x in range(101)])
 mask = np.logical_and(face_score_mask, second_face_score_mask)
mask = np.logical_and(mask, unknown_gender_mask)
mask = np.logical_and(mask, valid_age_range)
 detector = dlib.get_frontal_face_detector()
 
 
Item 2552: 
python ../scripts/validate_docstrings.py pandas.DataFrame.all
 Docstring text (summary) should start in the line immediately after the opening quotes (not in the same line, or leaving a blank line in between)
 Use only one blank line to separate sections or paragraphs
 git diff upstream/master -u -- "*.py" | flake8 --diff
 
 
Item 2553: 

#create annotators for words and sentences
 word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()

 #creates annotators of kind person, location and organization
 person_ann <- Maxent_Entity_Annotator(kind = "person")
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")

 
 
Item 2557: 
Predicting the age of abalone from physical measurements. 
 
Name		Data Type	Meas.	Description
	----		---------	-----	-----------
	Sex		nominal			M, F, and I (infant)
	Length		continuous	mm	Longest shell measurement
	Diameter	continuous	mm	perpendicular to length
	Height		continuous	mm	with meat in shell
	Whole weight	continuous	grams	whole abalone
	Shucked weight	continuous	grams	weight of meat
	Viscera weight	continuous	grams	gut weight (after bleeding)
	Shell weight	continuous	grams	after being dried
	Rings		integer			+1.5 gives the age in years
 
 
Item 2558: 
Abdollahpouri, H., Burke, R., & Mobasher, B. (2017). Controlling Popularity Bias in Learning-to-Rank Recommendation (pp. 42–46). Presented at the the Eleventh ACM Conference, New York, New York, USA: ACM Press. http://doi.org/10.1145/3109859.3109912
 Jannach, D., & Ludewig, M. (2017). When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation (pp. 306–310). Presented at the the Eleventh ACM Conference, New York, New York, USA: ACM Press. http://doi.org/10.1145/3109859.3109872
 Freno, A. (2017). Practical Lessons from Developing a Large-Scale Recommender System at Zalando (pp. 251–259). Presented at the the Eleventh ACM Conference, New York, New York, USA: ACM Press. http://doi.org/10.1145/3109859.3109897
 Ning, Y., Shi, Y., Hong, L., Rangwala, H., & Ramakrishnan, N. (2017). A Gradient-based Adaptive Learning Framework for Efficient Personal Recommendation (pp. 23–31). Presented at the the Eleventh ACM Conference, New York, New York, USA: ACM Press. http://doi.org/10.1145/3109859.3109909
 
 
Item 2564: 
y_output = {'output1': y1, 'output2': y2} #https://stackoverflow.com/questions/38972380/keras-how-to-use-fit-generator-with-multiple-outputs-of-different-type
 model.compile(optimizer='rmsprop', metrics=['accuracy'], loss={'category_output': 'categorical_crossentropy', 'sub_category_output' : 'categorical_crossentropy', 'attribute_output' : 'binary_crossentropy'})
 train_generator = generator_from_df(df_train, df_overall, headings_dict, batch_size, target_size, features=None, parametrization_dict = parametrization_dict)
 validation_generator = generator_from_df(df_validation, df_overall, headings_dict, batch_size, target_size, features=None, parametrization_dict= parametrization_dict)
 nbatches_train, mod = divmod(ntrain, batch_size)
nbatches_valid, mod = divmod(nvalid, batch_size)
 nworkers = 10
 model.fit_generator(
    train_generator,
    steps_per_epoch=nbatches_train,
    epochs=epochs,
    verbose=2,
    validation_data=validation_generator,
    validation_steps=nbatches_valid,
    workers=nworkers)
 #https://stackoverflow.com/questions/38972380/keras-how-to-use-fit-generator-with-multiple-outputs-of-different-type
 
 
Item 2567: 
{
 "name": "tensorflowjs",
 "version": "1.0.0",
 "dependencies": {
 "express": "latest"
 }}
 npm install
 node server.js
 
 
Item 2579: 
@load_and_save(load_fn, save_fn)
def my_fn(input_data, option_1=True):
    return results
 obj = wrap_obj(obj, ‘Clear data source description’)
 class GreatModel(Processor):
    def __init__(self, great_param, mysterious_param=None):
        
        super(GreatModel, self).__init__(great_param,     
                mysterious_param=mysterious_param)
         …
        self.great_param = great_param
     @change_state
    def ch_great_param(new_val):
        self.great_param = new_val
 with db_client(‘config.json’).as_default():
    input_data = wrap_obj(obj, ‘data source’)
    res = my_fn(input_data)
 @identity_wrapper()
def get_dataset(n_class=10):
    return load_digits(n_class=n_class)
 def np_save(results, file_path):
    np.save(file_path, results)
 def np_load(file_path):
    full_file_name = file_path + ‘.npy’
    return np.load(full_file_name)
 @load_and_save(np_load, np_save, unpack=4)
def split(digits, seed=42, test_size=.3):
    …
 with db_client().as_default():
    digits = get_dataset()
    X_train, X_test, y_train, y_test = split(digits)
    print(X_train.data.shape)
 class SVMClassifier(Processor):
    def __init__(self, *args, **kwargs):
        super(SVMClassifier, self).__init__(*args, **kwargs)
         self.svm_model = svm.SVC(*args, **kwargs)
     @change_state
    def fit(self, data, target): 
        self.svm_model.fit(flatten_sample(data), target)
     @identity_wrapper()
    def predict(self, data):
         return self.svm_model.predict(flatten_sample(data))
 with db_client().as_default():
    classifier = SVMClassifier()
    classifier.fit(X_train, y_train)
    pred = classifier.predict(X_test)
    print(confusion_matrix(y_test, pred).data)
    print(‘Accuracy’, accuracy(y_test, pred).data)
 classifier = SVMClassifier(kernel=’poly’)
 db = db_client()
cursor = list(db.list_results(‘accuracy’)[0])
rec_list = [db.recursive_get(res) for res in cursor]
pprint(rec_list)
 
 
Item 2580: 
name,email,credit_card_number,amount
Kan O.,kan@prontocool.io,3530111333300000,30300.49
Mils S.,mils@prontocool.io,4111111111111111,27500.12
 name,email,credit_card_number,amount
Patricia Meyer,robinsonjillian@curtis.com,3088948286183933,30300.49
Margaret Watson,williamrosales@humphrey.info,869954160247894,27500.12
 
 
Item 2581: 
“features”:{ 
 “adults”:2,
 “cost_per_star_stddev”:0.0799,
 “days_to_checkin”:31,
 “leisure”:1,
 “nights”:1,
 “ppd”:2.7701,
 “priority”:1.0324,
 “refundable”:false,
 “to_airport_km”:4.6,
 “to_city_km”:17.6,
 “total_cost_stddev”:-0.3921,
 “travelers”:2
 }
 "target": ["0 - not booked", "1 - booked"]
 decision_tree = Scoruby.load_model 'decision_tree.pmml'
features = { f1 : v1, ... } 
decision_tree.decide(features)

=> #<Decision:0x007fc232384180 @score="0", @score_distribution={"0"=>"0.999615579933873", "1"=>"0.000384420066126561"}>
 
 
Item 2583: 
learn.fit(0.01, 3)
 learn.fit(0.01, 3)
 
 
Item 2584: 
def zscore(col):
  mean = 3.04
  std = 1.2
  return (col — mean)/std
 feature_name = ‘total_bedrooms’
normalized_feature = tf.feature_column.numeric_column(
  feature_name,
  normalizer_fn=zscore)
 def zscore(col):
  mean = 3.04
  std = 1.2
  return (col — mean)/std
 feature_name = ‘total_bedrooms’
normalized_feature = tf.feature_column.numeric_column(
  feature_name,
  normalizer_fn=zscore)
 {‘households’: {‘mean’: 501.34073416222617, ‘std’: 382.81658748493305},
‘housing_median_age’: {‘mean’: 28.5441089402013, ‘std’: 12.610144469735635},
‘median_income’: {‘mean’: 3.8814239564831365, ‘std’: 1.9061255708184284},
‘population’: {‘mean’: 1428.941163410302, ‘std’: 1150.5106244960523},
‘total_bedrooms’: {‘mean’: 539.6057578448787, ‘std’: 418.76075045523334},
‘total_rooms’: {‘mean’: 2642.2929988158676, ‘std’: 2162.649970020439}}
 NUMERIC_FEATURES = [‘housing_median_age’, ‘total_rooms’, ‘total_bedrooms’, ‘population’, ‘households’, ‘median_income’]
 feature_columns = create_feature_cols(NUMERIC_FEATURES, use_normalization=True)
 model = tf.estimator.DNNRegressor(hidden_units=[10,4], 
                                  model_dir = outdir,
                                  feature_columns = feature_columns)
 OUTPUT_DIR=’trained_model’
export PYTHONPATH=${PYTHONPATH}:${PWD}/model_code
python -m trainer.task — outdir $OUTPUT_DIR — normalize_input 1
 JOBNAME=my_ml_job_$(date -u +%y%m%d_%H%M%S)
REGION=’us-central1'
BUCKET=’gs://crawles-sandbox’
OUTPUT_DIR=$BUCKET/’housing_trained_model’
PACKAGE_PATH=$PWD/model_code/trainer
 gcloud ml-engine jobs submit training $JOBNAME \
  -- package-path=$PACKAGE_PATH \
  -- module-name=trainer.task \
  -- region=$REGION \
  --staging-bucket=$BUCKET\
  -- scale-tier=BASIC \
  -- runtime-version=1.8 \
  -- \
  -- outdir=$OUTPUT_DIR\
  -- normalize_input=0
 
 
Item 2586: 
python generate_tfrecord.py — csv_input=data/train_labels.csv — output_path=data/train.record
 python generate_tfrecord.py — csv_input=data/test_labels.csv — output_path=data/test.record
 python3 train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco_2017_11_17.config
 python3 export_inference_graph.py \
    --input_type image_tensor \
    --pipeline_config_path training/ssd_inception_v2_coco_2017_11_17.config \
    --trained_checkpoint_prefix training/model.ckpt-7051 \
    --output_directory logos_inference_graph
 
 
Item 2594: 
{
  "type": "FeatureCollection",
  "features": [
    {
      "type": "Feature",
      "geometry": {
        "type": "Point",
        "coordinates": [102.0, 0.5]
      },
      "properties": {
        "prop0": "value0"
      }
    }
 DateTime.FromText(Text.Start( [REPORTDATE], 19))
 
 
Item 2595: 
##  Box-Ljung test
## 
## data:  gdp_ts
## X-squared = 319.62, df = 4, p-value < 2.2e-16
 ##  Ljung-Box test
## 
## data:  Residuals from ARIMA(2,1,1)(0,1,1)[4]
## Q* = 6.1916, df = 4, p-value = 0.1853
## 
## Model df: 4.   Total lags used: 8
 ##                     ME     RMSE     MAE         MPE      MAPE      MASE
## Training set -1763.393 18524.64 13332.4 -0.08197817 0.6440916 0.1939318
## Test set     47152.498 49475.29 47152.5  1.68398118 1.6839812 0.6858757
##                    ACF1 Theil's U
## Training set 0.06526246        NA
## Test set     0.09563655 0.6768184
  Quarter       Forecast   Lo 80   Hi 80   Lo 95   Hi 95
 2018 Q2        2887406 2853687 2921524 2835996 2939748
 2018 Q3        2892970 2841213 2945670 2814191 2973955
 2018 Q4        2990005 2917818 3063978 2880312 3103876
 2019 Q1        2888631 2805996 2973700 2763213 3019742
 2019 Q2        2973066 2870171 3079650 2817151 3137610
 2019 Q3        2976688 2859234 3098966 2798945 3165717
 2019 Q4        3076720 2939974 3219827 2870063 3298258
 
 
Item 2597: 
docker pull cyberreboot/poseidonml
 docker run –v <path_to_pcap>:/pcaps/eval.pcap cyberreboot/poseidonml
 docker run –v <path_to_pcap>:/pcaps/eval.pcap cyberreboot/poseidonml:onelayer
 Message: {“98:01:xx:xx:xx:xx”: {“classification”: {“confidences”: [0.6856065894902689, 0.2727088338248893, 0.022470232107183397], “labels”: [“Developer workstation”, “Unknown”, “Active Directory controller”]}, “timestamp”: 1498669414.355485, “valid”: false, “decisions”: {“behavior”: “normal”, “investigate”: false}}}
 DeviceName-deviceID-time-duration-flags.pcap
 DevWorkstation-User1-Tues1520–60mins-n00.pcap
 Fileserver-a-unk-unk-n00.pcap
 GPULaptop-user1-Fri0409–60mins-n00.pcap
 Iphone-b-Wed0945–5days-n00.pcap
 BusinessWorkstation-user2-Mon-3days-00.pcap
 {
“DevWorkstation”: “Developer workstation”,
 “Iphone”: “Smartphone”,
 “GPULaptop”: “GPU laptop”,
 “Fileserver”: “File server”,
 “BusinessWorkstation”: “Business workstation”
}
 docker run -v <path-to-dataset>:/pcaps -v <path-to-save-model>:/models/cyberreboot/poseidonml train
 docker run -v <path-to-dataset>:/pcaps -v <path-to-save-model>:/models/cyberreboot/poseidonml testing
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Results with unknowns
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 F1 of 0.XX for Developer workstation
 F1 of 0.XX for Smartphone
 F1 of 0.XX for Unknown
 …
 Mean F1: 0.XX
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Results forcing decisions
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 F1 of 0.XX for Developer workstation
 F1 of 0.XX for Smartphone
 …
 Mean F1: 0.XX
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Analysis statistics
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 Evaluated X pcaps in X seconds
 Total data: X Mb
 Total capture time: X hours
 Data processing rate: X Mb per second
 time per 15 minute capture X seconds
 — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —
 
 
Item 2599: 
jupyter nbconvert slides-in-a-notebook.ipynb --to slides --post serve
 
 
Item 2601: 
Agora, o valor de Y pode ser qualquer coisa entre -inf e + inf. O neurônio realmente não conhece os limites do valor. Então, como decidimos se o neurônio deve ou não ser acionado?

Decidimos adicionar “funções de ativação” para esse propósito. Para verificar o valor Y produzido por um neurônio e decidir se as conexões externas devem considerar esse neurônio como Sim ou Não. Ou melhor, digamos - "ativado" ou não.
 
 
Item 2605: 
Function GetURL(rng As Range) As String
    On Error Resume Next
    GetURL = rng.Hyperlinks(1).Address
End Function
 
 
Item 2606: 
sentence #0
where do I go to file a claim ?
Tokens: (0,where,WRB), (1,do,VBP), (2,I,PRP), (3,go,VB), (4,to,TO), (5,file,VB), (6,a,DT), (7,claim,NN), (8,?,.)
roots: 3
outgoing:
	0:
	1:
	2:
	3: (0,advmod) (1,aux) (2,nsubj) (5,xcomp)
	4:
	5: (4,aux) (7,dobj)
	6:
	7: (6,det)
incoming:
	0: (3,advmod)
	1: (3,aux)
	2: (3,nsubj)
	3:
	4: (5,aux)
	5: (3,xcomp)
	6: (7,det)
	7: (5,dobj)


entities:
List(Object) => claim
	------------------------------
	Rule => obj
	Type => TextBoundMention
	------------------------------
	Object => claim
	------------------------------


events:
==================================================
 sentence #0
he sued Fred last june in Mexico .
Tokens: (0,he,PRP), (1,sued,VBD), (2,Fred,NNP), (3,last,JJ), (4,june,NNP), (5,in,IN), (6,Mexico,NNP), (7,.,.)
roots: 1
outgoing:
	0:
	1: (0,nsubj) (2,dobj) (4,tmod) (6,prep_in)
	2:
	3:
	4: (3,amod)
	5:
	6:
incoming:
	0: (1,nsubj)
	1:
	2: (1,dobj)
	3: (4,amod)
	4: (1,tmod)
	5:
	6: (1,prep_in)


entities:
List(Date) => last june
	------------------------------
	Rule => ner-date
	Type => TextBoundMention
	------------------------------
	Date => last june
	------------------------------

List(Entity, Subject) => he
	------------------------------
	Rule => subj
	Type => TextBoundMention
	------------------------------
	Entity, Subject => he
	------------------------------

List(Entity, Object) => Fred
	------------------------------
	Rule => obj
	Type => TextBoundMention
	------------------------------
	Entity, Object => Fred
	------------------------------

List(Location) => Mexico
	------------------------------
	Rule => ner-location
	Type => TextBoundMention
	------------------------------
	Location => Mexico
	------------------------------

List(liability) => he sued Fred last june in Mexico
	------------------------------
	Rule => liability-syntax-1
	Type => EventMention
	------------------------------
	trigger => sued
	claimaint (Entity, Subject) => he
	defendant (Entity, Object) => Fred
	location (Location) => Mexico
	------------------------------
 
 
Item 2607: 
nlp = spacy.load('en')
 corpus = st.CorpusFromPandas(df_parsed.sample(10000), 
                              category_col='FromCat', 
                              text_col='Body',
                              nlp=nlp).build()
 scaled_f_scores_vs_background = corpus.get_scaled_f_scores_vs_background()
 term_freq_df = corpus.get_term_freq_df()
 for c in term_freq_df:
    f = c[:c.find(' freq')]
    term_freq_df[f+' fscore'] = corpus.get_scaled_f_scores(f)
 columns = pd.Series([x[:x.find(' ')] for x in term_freq_df.columns]).drop_duplicates()
 term_freq_df.columns = pd.MultiIndex.from_product([['freq','fscore'],columns])
 term_freq_df.loc[:,'fscore'].sort_values('other',ascending=False).iloc[:200].to_excel('fscores.xlsx')
 [{"id":0,"label":"chris.germany@enron.com","group":"person"},{"id":1,"label":"debra.perlingiere@enron.com","group":"person"},{"id":2,"label":"drew.fossum@enron.com","group":"person"},{"id":3,"label":"enron.announcements@enron.com","group":"person"},{"id":4,"label":"eric.bass@enron.com","group":"person"},{"id":5,"label":"gerald.nemec@enron.com","group":"person"},...]
 [{"from":0,"to":21},{"from":1,"to":21},{"from":2,"to":21},{"from":4,"to":21},{"from":5,"to":21},{"from":6,"to":21},{"from":7,"to":21},{"from":8,"to":21},{"from":9,"to":21},{"from":10,"to":21},{"from":11,"to":21},{"from":13,"to":21},{"from":15,"to":21},{"from":16,"to":21},{"from":17,"to":21},{"from":18,"to":21},{"from":19,"to":21},{"from":20,"to":21},...]
 
 
Item 2609: 
loudml create-model behavior.json
 loudml train behavior-model --from "2018–01–01T00:00:00.000Z" --to "2018–02–01T00:00:00.000Z" -l 1000
 loudml predict behavior-model --from "2017–03–01T00:00:00.000Z" --to "2017–03–08T00:00:00.000Z"
 
 
Item 2611: 
MB_KEY="my_key_here"
docker run -p 8080:8080 -e "MB_KEY=$MB_KEY" machinebox/tagbox
 docker pull machinebox/tagbox:latest
 
 
Item 2613: 
val bitmap = MediaStore.Images.Media.getBitmap(contentResolver, imageUri)
 val image = FirebaseVisionImage.fromMediaImage(mediaImage, rotation)
 
 
Item 2615: 
Return of Portfolio = Sum(Wa * Ra + Wb * Rb … Wn * Rn)
 where:
Wa = Weight of Asset a in the portfolio
Wb = Weight of Asset b in the portfolio
Wn = Weight of Asset n in the portfolio
Ra = Rate of Return of Asset a
Rb = Rate of Return of Asset b
Rn = Rate of Return of Asset n
 Summation of weights = 1
 
 
Item 2621: 
[Parameter(Mandatory = $true, HelpMessage = '{
    "Title": "Exam Name",
    "Description": "Exam Name",
    "Placeholder": "i.e. MATH 01",
    "ErrorMessage": {
    "Pattern": "Exam name does not match the correct format. i.e.   MATH 01."
    }
 }')]
 [ValidatePattern('[A-Za-z]{4}\ [0-9]{2}')]
 [string]$ExamName
 
 
Item 2623: 
git clone https://github.com/fkabinoff/playground-walkingdead.git
 ./playground-walkingdead/resources/script.js
 app.visualization.create('kpi', //viz type
  [{qDef: {qDef: "=Sum([Kills])",
    qLabel: "Total walker kills",
    qNumFormat: {
      qType: "I",
      qFmt: "##,###",
      qUseThou: true,
      qThou: ","
  }}}], //columns
  {} //options
).then(function(viz){
  viz.show('QV01');
});
 
 
Item 2626: 
for classifier in ["XGBoost", "Logistic Regression", "SVM"]:
    if classifier == "XGBoost":
        print(f"{classifier} is the best! Maybe.")
    else:
        print(f"{classifier} is fun. Somehow.")
 class KFold(...):
    def __init__(...):
        ...
    def _iter_test_indices(...):
        ...
        current = 0
        for fold_size in fold_sizes:
            start, stop = current, current + fold_size
            yield indices[start:stop]
            current = stop
 def staged_predict_proba(...):
    ...
    for score in self._staged_decision_function(X):
          yield self.loss_._score_to_proba(score)
 
 
Item 2631: 
git clone https://github.com/pjreddie/darknet
cd darknet
make
 wget https://pjreddie.com/media/files/yolo.weights
 ./darknet detect cfg/yolo.cfg yolo.weights data/dog.jpg
 
 
Item 2632: 
We’re studying Distance-Based Charges: charging per kilometre driven within a given boundary, varying by time and location. We’re exploring ways this type of charge could work in Metro Vancouver to figure out whether this approach would reduce congestion, improve fairness, and support transportation investment in our growing region. We want to hear your thoughts when online engagement opens on February 26!
 We’re studying Distance-Based Charges: charging per kilometre driven within a given boundary, varying by time and location. We’re exploring ways this type of charge could work in Metro Vancouver to figure out whether this approach would reduce congestion, improve fairness, and support transportation investment in our growing region. We want to hear your thoughts when online engagement opens on February 26!
 
 
Item 2634: 
docker run -d -it --name <CONTAINER_NAME> -v <VOLUME_NAME>:/tf_files gcr.io/tensorflow/tensorflow:latest-devel
 docker exec -it <CONTAINER_NAME> bash
 (while inside your Docker container)
 mkdir /tf && cd /tf
 curl -O http://download.tensorflow.org/example_images/flower_photos.tgz
 tar xzf flower_photos.tgz
 -----------------------------------------------------------------
 (to copy images from your own drive to Docker)
 docker cp <FOLDER_NAME> <CONTAINER_NAME>:/tf/<FOLDER_NAME>
 cd /tensorflow && git pull
 python /tensorflow/tensorflow/examples/image_retraining/retrain.py \
-- bottleneck_dir=/tf_files/bottlenecks \
-- model_dir=/tf_files/inception \
-- output_graph=/tf_files/retrained_graph.pb \
-- output_labels=/tf_files/retrained_labels.txt \
-- image_dir /tf/flower_photos/
 python /tf/label.py /tf/<SOME_IMAGE>.jpg
 
 
Item 2644: 
database = setup_database()
run_face_recognition(database)
 
 
Item 2649: 
!pip install git+https://github.com/gfilla/dsxtools.git
 !pip install git+https://<user_name>:<personal_access_token>@github.<your_company>.com/<your_org>/<your_repo>.git --ignore-installed
 install.packages('devtools')
library(devtools)
install_github('<username>/<repo>') #installs the package
library('<repo_name>')  #loads the package for use
 
 
Item 2651: 
name: tensorport
channels:
  - kundaje
dependencies:
  - dragonn
  - h5py
 
 
Item 2653: 
https://www.facebook.com/Jumia.ma/reviews/
 merge(x = reviews, y = users, by = “username”, all = TRUE)
 {
    "review": "Im trying all day to make a contact with you but all the numbers are busy  I ve tried many times  bad service  And those blackfriday products they disappear in just a second   Not a minute",
    "date": "Friday, November 24, 2017 at 11:47am",
    "rating": "1 star",
    "gender": "Male",
    "location": "Casablanca",
    "username": "profile.php?id=100013226341737",
    "reactions": [
        "14 Like",
        "1 Love"
     ]
}
 {
    "review": "Im trying all day to make a contact with you but all the numbers are busy I ve tried many times  bad service And those blackfriday products they disappear in just a second Not a minute",
    "date": "24-11-2017 11:47",
    "rating": "1",
    "gender" : "Male",
    "location" : "Casablanca",
    "username": "profile.php?id=100013226341737",
    "Like" : "14",
    "Love" : "1",
    "Haha" : "0",
    "Angry" : "0",
    "Sad" : "0",
    "Wow" : "0",
    "longitude" : "33.984518",
    "latitude" : "-6.8657004"
}
 
 
Item 2656: 
<tf.Variable 'variable:0' shape=(1,) dtype=int32_ref>
 INFO:tensorflow:Restoring parameters from /tmp/model.ckpt
 name: "variable" op: "VariableV2" attr { key: "container" value { s: "" } } attr { key: "dtype" value { type: DT_INT32 } } attr { key: "shape" value { shape { dim { size: 1 } } } } attr { key: "shared_name" value { s: "" } }
 None
 INFO:tensorflow:Restoring parameters from /tmp/model.ckpt <tf.Variable 'variable:0' shape=(1,) dtype=int32_ref>
 name: "variable" op: "VariableV2" attr { key: "container" value { s: "" } } attr { key: "dtype" value { type: DT_INT32 } } attr { key: "shape" value { shape { dim { size: 1 } } } } attr { key: "shared_name" value { s: "" } }
 {'variable': 'variable_2:0'}
 None <tf.Tensor 'variable_2:0' shape=(1,) dtype=int32_ref>
 Using TensorFlow backend.
 0.125
0.46875
0.8125
0.953125
0.828125
0.890625
0.796875
0.9375
0.953125
0.921875
 {'x': 'ExpandDims:0', 
'y': 'one_hot:0', 
'forward_raw': 'dense_1/BiasAdd:0', 
'accuracy': 'Mean:0', 
'loss': 'Mean_1:0', 
'train_step': 'Adam'}
 INFO:tensorflow:Restoring parameters from /tmp/model.ckpt
 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) <ipython-input-21-9def5e0d8f6c> in <module>() ----> 1 mnist_model.accuracy AttributeError: 'MNISTModel' object has no attribute 'accuracy'
 <tf.Tensor 'Mean:0' shape=() dtype=float32>
 INFO:tensorflow:Restoring parameters from /tmp/model.ckpt 
1.0
 
 
Item 2661: 
library(boot)
 set.seed(187)
 n <- 10000
mu <- 1
sigma <- 1
control <- rnorm(n, mu, sigma)
variant <- rnorm(n, mu * 1.05, sigma)
 print("Without Outlier")
t.test(control, variant)
 print("With Outlier")
control[1] <- mu + sigma * 200
t.test(control, variant)
 print("Outlier With Bootstrapping")
 bootpothesis <- function(x, y, bs_n) {
  sample_tstat <- abs(t.test(x, y)$statistic)
 pooled_mean <- mean(c(x,y))
  x1 <- x-mean(x) + pooled_mean
  y1 <- y-mean(y) + pooled_mean
 bootstrapped_tstats <- vector(length = bs_n)
  for (i in 1:bs_n) {
    boot_control <- sample(x1, size=length(x), replace=T)
    boot_variant <- sample(y1, size=length(y), replace=T)
    bootstrapped_tstats[i] <- t.test(boot_control, boot_variant, var.equal=TRUE)$statistic
  }
 return(mean(bootstrapped_tstats > sample_tstat))
}
 bs_n <- 10000
 bootpothesis(control, variant, bs_n = bs_n)
 
 
Item 2663: 
mkdir images-extractor
 cd $_
 npm init
 npm install request request-promise puppeteer async md5
 node images-extractor.js 'Monkeys'
 
 
Item 2665: 
plot(fit_mle)
 fevd(x = as.vector(month_bm$month_max), type = "GEV", method = "Lmoments", 
    period.basis = "month")
[1] "GEV  Fitted to  as.vector(month_bm$month_max)  using L-moments estimation."
   location       scale       shape 
114.0563798  19.6206000   0.2034706
 fevd(x = as.vector(month_bm$month_max), type = "GEV", method = "Lmoments", 
    period.basis = "month")
get(paste("return.level.fevd.", newcl, sep = ""))(x = x, return.period = return.period, 
    conf = 0.05)
 GEV model fitted to  as.vector(month_bm$month_max)  
Data are assumed to be  stationary 
[1] "Return Levels for period units in months"
  3-month level   6-month level  12-month level  24-month level  48-month level 120-month level 
       133.4993        153.9620        176.1072        200.9296        229.1534        272.8342
 return.level(fit_lmom, conf = 0.05, return.period= c(3,6,12,24,48,120*20))
 3-month level    6-month level   12-month level   24-month level   48-month level 2400-month level 
        133.4993         153.9620         176.1072         200.9296         229.1534         487.4839
 
 
Item 2669: 
scrapy runspider tripadvisorscraper.py -o reviews.csv
 
 
Item 2680: 
AND:[0,0] = 0
AND:[0,1] = 0
AND:[1,0] = 0
AND:[1,1] = 1
 NAND:[0,0] = 1
NAND:[0,1] = 1
NAND:[1,0] = 1
NAND:[1,1] = 0
 0 + 0 => Sum: 0 Carry: 0
1 + 0 => Sum: 1 Carry: 0
0 + 1 => Sum: 1 Carry: 0
1 + 1 => Sum: 0 Carry: 1
 
 
Item 2681: 
size=[3,2,3,3]
self.DummyState = tf.Variable([[0.1]])

output = slim.fully_connected(self.DummyState,a_size,\
    biases_initializer=None, weights_initializer=tf.ones_initializer())

self.res = [tf.squeeze(slim.fully_connected(output, x, \
                              biases_initializer=None, \
                              activation_fn=tf.nn.sigmoid,
                              weights_initializer=tf.ones_initializer(), 
                              scope='results'+str(i))) 
           for i, x in enumerate(size)]
 self.bandits = [np.random.random(*size)]
 self.bandits = [np.random.random(x)/50+0.95 for x in size]
 
 
Item 2682: 
street,city,zip,state,beds,baths,sq_ft,type,sale_date,price,latitude,longitude
3526 HIGH ST,SACRAMENTO,95838,CA,2,1,836,Residential,Wed May 21 00:00:00 EDT 2008,59222,38.631913,-121.434879
51 OMAHA CT,SACRAMENTO,95823,CA,3,1,1167,Residential,Wed May 21 00:00:00 EDT 2008,68212,38.478902,-121.431028
2796 BRANCH ST,SACRAMENTO,95815,CA,2,1,796,Residential,Wed May 21 00:00:00 EDT 2008,68880,38.618305,-121.443839
2805 JANETTE WAY,SACRAMENTO,95815,CA,2,1,852,Residential,Wed May 21 00:00:00 EDT 2008,69307,38.616835,-121.439146
6001 MCMAHON DR,SACRAMENTO,95824,CA,2,1,797,Residential,Wed May 21 00:00:00 EDT 2008,81900,38.51947,-121.435768
5828 PEPPERMILL CT,SACRAMENTO,95841,CA,3,1,1122,Condo,Wed May 21 00:00:00 EDT 2008,89921,38.662595,-121.327813
6048 OGDEN NASH WAY,SACRAMENTO,95842,CA,3,2,1104,Residential,Wed May 21 00:00:00 EDT 2008,90895,38.681659,-121.351705
2561 19TH AVE,SACRAMENTO,95820,CA,3,1,1177,Residential,Wed May 21 00:00:00 EDT 2008,91002,38.535092,-121.481367
11150 TRINITY RIVER DR Unit 114,RANCHO CORDOVA,95670,CA,2,2,941,Condo,Wed May 21 00:00:00 EDT 2008,94905,38.621188,-121.270555
 column_name,data_type
street,VARCHAR (1024)
city,VARCHAR (1024)
zip,BIGINT
state,VARCHAR (1024)
beds,BIGINT
baths,BIGINT
sq_ft,BIGINT
type,VARCHAR (1024)
sale_date,VARCHAR (1024)
price,BIGINT
latitude,DOUBLE PRECISION
longitude,DOUBLE PRECISION
ob_transaction_id,varchar(256)
ob_file_name,varchar(2048)
ob_processed_at,varchar(256)
ob_modified_date,datetime
 ################
DATA PIPELINE - BATCH DETAILS
################
 Welcome! You have setup a connection to batch data for processing, routing and loading. Below are your connection details, credentials and some prerequisites to connecting to your system.
 ----------
CONNECTION
----------
The following are your connection details.
host: pipeline-01.openbridge.io
port: 22 or 443
database destination: athena-customer-database
table destination: loyalty_purchases
 Note: port 443 is often used in the event a corporate firewall blocks outbound connections on port 22.
 ----------
CREDENTIALS
----------
The following are your Data Pipeline Batch credentials. Keep them safe and secure. They provide access to READ and WRITE access to data resident for your pipeline.
 username: yourusername
password: yourpassword
 ----------
TIPS & RESOURCES
----------
General overview, requirements and best practices:
https://docs.openbridge.com/data-pipelines/data-pipeline-overview
 Set Up Prerequisites
You will need software a software client to transfer data to Openbridge. You have a number of options available. Pick one that is right for your team:
https://docs.openbridge.com/data-pipelines/data-pipeline-batch-client-software
 We have also an open source solution that leverages rclone for advanced file transfers from various cloud and local storage systems:
https://github.com/openbridge/ob_bulkstash
 
 
Item 2683: 
get_wiki_rikishi <- function(name) {
  url <- paste0("https://ja.wikipedia.org/wiki/",name)
  html <- read_html(url)
  tb1 <- html %>% html_node("table") %>% html_table(header = FALSE, fill=TRUE)
  tb2 <- tb1 %>% spread(X1, X2)
  tb2
}

yokozuna_df <- data.frame(X1=character(0))
for (name in names_df$name[c(1:17, 19:39, 42, 44:54, 56:63,65,68:72)]) {
  
  tryCatch({
    df0 <- get_wiki_rikishi(name)
    yokozuna_df <- yokozuna_df %>% bind_rows(df0)
  }, error = {
    cat(name)
  })
}
 
 
Item 2684: 
data$Month <- factor(data$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
data$Region <- factor(data$Region)
data$Year <- factor(data$Year)
 data %>%
    group_by(Year, Month) %>%
    summarise(Total_Cases = sum(Dengue_Cases)) %>%
    ggplot(aes(Year,Month,fill=Total_Cases)) + 
    geom_tile() +
    scale_fill_gradient(low = "white", high = "red", name = "Dengue  cases/ \n100,000 population") +
    xlab(label="Year") + 
    ylab(label="Month")
 data %>%
    group_by(Year, Region) %>%
    summarise(Total_Cases = sum(Dengue_Cases)) %>%
    ggplot(aes(Year,Region,fill=Total_Cases)) + 
    geom_tile() +
    scale_fill_gradient(low = "white", high = "red", name = "Dengue cases/ \n100,000 population") +
    xlab(label="Year") + 
    ylab(label="Month")
 total_cases <- data %>%
    group_by(Year, Month) %>%
    summarise(Total_Cases = sum(Dengue_Cases))
 total_cases_ts <- ts(total_cases$Total_Cases, start=c(2008,1), end=c(2016,12), frequency=12)
 plot(stl(total_cases_ts, s.window = "periodic"))
 adf.test(total_cases_ts, alternative = "stationary")
 dengue_arima <- auto.arima(total_cases_ts)
 dengue_future <- forecast(dengue_arima, 24)
plot(dengue_future, ylab = "Dengue cases per 100,000 population")
 
 
Item 2685: 
X = left_value+(Q-left_count) / (right_count-left_count)*bin_width 
 X-left_value = (Q-left_count) / (right_count-left_count)*bin_width 
 (X-left_value)/bin_width = (Q-left_count)/(right_count-left_count) 
 (X-left_value)/bin_width*(right_count-left_count) = Q-left_count
 Q = (X-left_value)/bin_width*(right_count-left_count)+left_count
 
 
Item 2686: 
{
  "landmarkAnnotations": [
    {
      "mid": "/m/035p3",
      "description": "Golden Gate Bridge",
      "score": 0.35074955,
      "boundingPoly": {
        "vertices": [
          {
            "x": 287,
            "y": 169
          },
          {
            "x": 607,
            "y": 169
          },
          {
            "x": 607,
            "y": 318
          },
          {
            "x": 287,
            "y": 318
          }
        ]
      },
      "locations": [
        {
          "latLng": {
            "latitude": 37.810868,
            "longitude": -122.476272
          }
        }
      ]
    },
    {
      "mid": "/m/035p3",
      "description": "Golden Gate Bridge",
      "score": 0.3461872,
      "boundingPoly": {
        "vertices": [
          {
            "x": 79,
            "y": 157
          },
          {
            "x": 299,
            "y": 157
          },
          {
            "x": 299,
            "y": 287
          },
          {
            "x": 79,
            "y": 287
          }
        ]
      },
      "locations": [
        {
          "latLng": {
            "latitude": 37.811013,
            "longitude": -122.477801
          }
        }
      ]
    }
  ],
  "labelAnnotations": [
    {
      "mid": "/m/015kr",
      "description": "bridge",
      "score": 0.9573081,
      "topicality": 0.9573081
    },
    {
      "mid": "/m/01bqvp",
      "description": "sky",
      "score": 0.9408597,
      "topicality": 0.9408597
    },
    {
      "mid": "/m/0csby",
      "description": "cloud",
      "score": 0.88219845,
      "topicality": 0.88219845
    },
    {
      "mid": "/m/01b2w5",
      "description": "sunset",
      "score": 0.87522304,
      "topicality": 0.87522304
    },
    {
      "mid": "/m/0d1n2",
      "description": "horizon",
      "score": 0.8456816,
      "topicality": 0.8456816
    },
    {
      "mid": "/m/06npx",
      "description": "sea",
      "score": 0.84551364,
      "topicality": 0.84551364
    },
    {
      "mid": "/m/051gs3",
      "description": "fixed link",
      "score": 0.813437,
      "topicality": 0.813437
    },
    {
      "mid": "/m/0csy8",
      "description": "suspension bridge",
      "score": 0.7733133,
      "topicality": 0.7733133
    },
    {
      "mid": "/m/01b3l7",
      "description": "dusk",
      "score": 0.7567896,
      "topicality": 0.7567896
    },
    {
      "mid": "/m/06cnp",
      "description": "river",
      "score": 0.75497615,
      "topicality": 0.75497615
    },
    {
      "mid": "/m/01b3kg",
      "description": "dawn",
      "score": 0.7400379,
      "topicality": 0.7400379
    },
    {
      "mid": "/m/0838f",
      "description": "water",
      "score": 0.7339397,
      "topicality": 0.7339397
    },
    {
      "mid": "/m/02_ktj",
      "description": "morning",
      "score": 0.7337289,
      "topicality": 0.7337289
    },
    {
      "mid": "/m/01b2q6",
      "description": "sunrise",
      "score": 0.73126,
      "topicality": 0.73126
    },
    {
      "mid": "/m/02l215",
      "description": "reflection",
      "score": 0.713901,
      "topicality": 0.713901
    },
    {
      "mid": "/m/026fm63",
      "description": "calm",
      "score": 0.70837736,
      "topicality": 0.70837736
    },
    {
      "mid": "/m/04mx32",
      "description": "evening",
      "score": 0.7023143,
      "topicality": 0.7023143
    },
    {
      "mid": "/m/013vs",
      "description": "afterglow",
      "score": 0.6465814,
      "topicality": 0.6465814
    },
    {
      "mid": "/m/05kq4",
      "description": "ocean",
      "score": 0.6145469,
      "topicality": 0.6145469
    },
    {
      "mid": "/m/0157l1",
      "description": "cable stayed bridge",
      "score": 0.5945261,
      "topicality": 0.5945261
    },
    {
      "mid": "/g/11k2xz7mr",
      "description": "meteorological phenomenon",
      "score": 0.5603266,
      "topicality": 0.5603266
    },
    {
      "mid": "/m/04p25",
      "description": "loch",
      "score": 0.54516745,
      "topicality": 0.54516745
    },
    {
      "mid": "/m/09wdrl",
      "description": "extradosed bridge",
      "score": 0.5216861,
      "topicality": 0.5216861
    }
  ],
  "safeSearchAnnotation": {
    "adult": "VERY_UNLIKELY",
    "spoof": "VERY_UNLIKELY",
    "medical": "VERY_UNLIKELY",
    "violence": "VERY_UNLIKELY",
    "racy": "VERY_UNLIKELY"
  },
  "imagePropertiesAnnotation": {
    "dominantColors": {
      "colors": [
        {
          "color": {
            "red": 55,
            "green": 44,
            "blue": 40
          },
          "score": 0.1882846,
          "pixelFraction": 0.116575204
        },
        {
          "color": {
            "red": 131,
            "green": 106,
            "blue": 103
          },
          "score": 0.016361877,
          "pixelFraction": 0.010496031
        },
        {
          "color": {
            "red": 218,
            "green": 195,
            "blue": 172
          },
          "score": 0.009196449,
          "pixelFraction": 0.016569875
        },
        {
          "color": {
            "red": 253,
            "green": 204,
            "blue": 114
          },
          "score": 0.004897723,
          "pixelFraction": 0.0041025095
        },
        {
          "color": {
            "red": 35,
            "green": 27,
            "blue": 23
          },
          "score": 0.15742785,
          "pixelFraction": 0.10160371
        },
        {
          "color": {
            "red": 68,
            "green": 49,
            "blue": 36
          },
          "score": 0.1535307,
          "pixelFraction": 0.0701689
        },
        {
          "color": {
            "red": 99,
            "green": 86,
            "blue": 90
          },
          "score": 0.1457985,
          "pixelFraction": 0.08300922
        },
        {
          "color": {
            "red": 75,
            "green": 46,
            "blue": 32
          },
          "score": 0.06324315,
          "pixelFraction": 0.032020885
        },
        {
          "color": {
            "red": 102,
            "green": 72,
            "blue": 55
          },
          "score": 0.049987912,
          "pixelFraction": 0.09126752
        },
        {
          "color": {
            "red": 97,
            "green": 75,
            "blue": 59
          },
          "score": 0.036990125,
          "pixelFraction": 0.023975704
        }
      ]
    }
  },
  "cropHintsAnnotation": {
    "cropHints": [
      {
        "boundingPoly": {
          "vertices": [
            {
              "x": 50
            },
            {
              "x": 562
            },
            {
              "x": 562,
              "y": 639
            },
            {
              "x": 50,
              "y": 639
            }
          ]
        },
        "confidence": 0.79999995,
        "importanceFraction": 0.83
      },
      {
        "boundingPoly": {
          "vertices": [
            {},
            {
              "x": 639
            },
            {
              "x": 639,
              "y": 639
            },
            {
              "y": 639
            }
          ]
        },
        "confidence": 0.79999995,
        "importanceFraction": 1
      },
      {
        "boundingPoly": {
          "vertices": [
            {},
            {
              "x": 639
            },
            {
              "x": 639,
              "y": 536
            },
            {
              "y": 536
            }
          ]
        },
        "confidence": 0.79999995,
        "importanceFraction": 0.83
      }
    ]
  },
  "webDetection": {
    "webEntities": [
      {
        "entityId": "/m/051gs3",
        "score": 0.8717412,
        "description": "Bridge–tunnel"
      },
      {
        "entityId": "/m/035p3",
        "score": 0.83145,
        "description": "Golden Gate Bridge"
      },
      {
        "entityId": "/m/0157l1",
        "score": 0.59367347,
        "description": "Cable-stayed bridge"
      },
      {
        "entityId": "/m/015kr",
        "score": 0.5786,
        "description": "Bridge"
      },
      {
        "entityId": "/m/0csy8",
        "score": 0.5603842,
        "description": "Suspension bridge"
      },
      {
        "entityId": "/m/06npx",
        "score": 0.5181158,
        "description": "Sea"
      },
      {
        "entityId": "/m/09wdrl",
        "score": 0.44899777,
        "description": "Extradosed bridge"
      },
      {
        "entityId": "/m/0bmz2",
        "score": 0.2979,
        "description": "Electrical cable"
      },
      {
        "entityId": "/m/03p1p8r",
        "score": 0.2936,
        "description": "Sky plc"
      }
    ],
    "visuallySimilarImages": [
      {
        "url": "https://www.videvo.net/videvo_files/images/preview_161022_077_SanFrancisco_4K.jpg"
      },
      {
        "url": "https://thumbs.dreamstime.com/b/sailboat-golden-gate-bridge-sunset-clouds-sailboat-golden-gate-bridge-sunset-colorful-wispy-clouds-107643862.jpg"
      },
      {
        "url": "https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/S_-0n06Vipfunqrp/san-francisco-bay-golden-gate-golden-gate-bridge-at-sunset-golden-gate-bridge-over-san-francisco-bay_rpeotwkcl__S0000.jpg"
      },
      {
        "url": "https://i.pinimg.com/236x/41/26/89/412689698db67e9d6c970ff9afe2f7eb--golden-gate-bridge-vin.jpg"
      },
      {
        "url": "http://goldengate.org/exhibits/images/GGB-exhibit2-4_5.jpg"
      },
      {
        "url": "http://c1.peakpx.com/wallpaper/721/408/35/golden-gate-bridge-sunset-wallpaper.jpg"
      },
      {
        "url": "https://media-cdn.tripadvisor.com/media/photo-s/05/5f/22/e0/blazing-saddles.jpg"
      },
      {
        "url": "https://thumbs.dreamstime.com/b/golden-gate-bridge-sunrise-san-francisco-beach-california-60924538.jpg"
      },
      {
        "url": "https://d2v9y0dukr6mq2.cloudfront.net/video/thumbnail/pbJRSBz/the-golden-gate-bridge-sunset_4jiio98lg__S0000.jpg"
      },
      {
        "url": "http://footage.framepool.com/shotimg/qf/909179899-san-francisco-bay-golden-gate-strait-golden-gate-bridge-pacific-coast.jpg"
      },
      {
        "url": "https://us.123rf.com/450wm/kitleong/kitleong1509/kitleong150900270/45116349-golden-gate-bridge-sfo-sun-raise.jpg?ver=6"
      },
      {
        "url": "https://i.dailymail.co.uk/i/pix/2014/03/16/article-0-1C585E9A00000578-390_634x422.jpg"
      },
      {
        "url": "http://media6.trover.com/T/58c4309010b26810ab02f909/fixedw_large_4x.jpg"
      },
      {
        "url": "https://n7tpw52zdt-flywheel.netdna-ssl.com/wp-content/uploads/2017/05/san-francisco-golden-gate-bridge-8.jpg"
      },
      {
        "url": "http://rubinweddings.com/wp-content/uploads/2016/04/San-Francisco-and-Golden-Gate-Bridge-Engagement-Session-0016.jpg"
      },
      {
        "url": "https://static1.squarespace.com/static/5935836e5016e13ecccd943e/5986791837c5815a82171acc/598679184c0dbf1460634378/1501985450661/IMG_1712-1.jpg"
      }
    ],
    "bestGuessLabels": [
      {
        "label": "golden gate bridge",
        "languageCode": "en"
      }
    ]
  }
}
 
 
Item 2689: 
CLV = past profits + future profits
 CLV = past profits +
      expected profit per transaction * expected no. of transactions
 
 
Item 2693: 
+--------+-----------------+
| Input | Desired output |
+--------+-----------------+
| 0 | 0 |
| 1 | 2 |
| 2 | 4 |
| 3 | 6 |
| 4 | 8 |
+--------+-----------------+
 +--------+------------------------------------+
| Input | Actual output of model 1 (y= 3.x) |
+--------+------------------------------------+
| 0 | 0 |
| 1 | 3 |
| 2 | 6 |
| 3 | 9 |
| 4 | 12 |
+--------+------------------------------------+
 +--------+-----------------+-----------------+
| Input | Actual output | Desired output |
+--------+-----------------+-----------------+
| 0 | 0 | 0 |
| 1 | 3 | 2 |
| 2 | 6 | 4 |
| 3 | 9 | 6 |
| 4 | 12 | 8 |
+--------+-----------------+-----------------+
 +--------+----------+-----------+------------------+---------------+
| Input | actual | Desired | Absolute Error | Square Error |
+--------+----------+-----------+------------------+---------------+
| 0 | 0 | 0 | 0 | 0 |
| 1 | 3 | 2 | 1 | 1 |
| 2 | 6 | 4 | 2 | 4 |
| 3 | 9 | 6 | 3 | 9 |
| 4 | 12 | 8 | 4 | 16 |
| Total: | - | - | 10 | 30 |
+--------+----------+-----------+------------------+---------------+
 +--------+----------+-------+-----------+------------+---------+
| Input | Output | W=3 | rmse(3) | W=3.0001 | rmse |
+--------+----------+-------+-----------+------------+---------+
| 0 | 0 | 0 | 0 | 0 | 0 |
| 1 | 2 | 3 | 1 | 3.0001 | 1.0002 |
| 2 | 4 | 6 | 4 | 6.0002 | 4.0008 |
| 3 | 6 | 9 | 9 | 9.0003 | 9.0018 |
| 4 | 8 | 12 | 16 | 12.0004 | 16.0032 |
| Total: | - | - | 30 | - | 30.006 |
+--------+----------+-------+-----------+------------+---------+
 
 
Item 2695: 
A little help from a friend, called Kelp
 University of Washington researchers used a malamute dog, Kelp, to gather canine data.
 They mounted a GoPro camera on his head and attached motion sensors to his body.
 With this kit, they’ve recorded Kelp going on about his daily routine and matched the motion data with the video.
 Having fed it into the neural network, the AI was able to predict the dog’s behaviour from visual clues.
 For example, if someone threw a stick, the virtual dog would run to fetch it.
 
 
Item 2696: 
<dependency>
    <groupId>com.google.cloud.dataflow</groupId>
    <artifactId>google-cloud-dataflow-java-sdk-all</artifactId>
    <version>2.5.0</version>
</dependency>
 DRG Definition: The code and description identifying the MS-DRG. MS-DRGs are a classification system that groups similar clinical conditions (diagnoses) and procedures furnished by the hospital during their stay.
 Provider Id: The CMS Certification Number (CCN) assigned to the Medicare-certified hospital facility.
 Provider Name: The name of the provider.
 Provider Street Address: The provider’s street address.
 Provider City: The city where the provider is located.
 Provider State: The state where the provider is located.
 Provider Zip Code: The provider’s zip code.
 Provider HRR: The Hospital Referral Region (HRR) where the provider is located.
 Total Discharges: The number of discharges billed by the provider for inpatient hospital services.
 Average Covered Charges: The provider’s average charge for services covered by Medicare for all discharges in the MS-DRG. These will vary from hospital to hospital because of the differences in hospital charge structures.
 Average Total Payments: The average total payments to all providers for the MS-DRG including the MSDRG amount, teaching, disproportionate share, capital, and outlier payments for all cases. Also included in the average total payments are co-payment and deductible amounts that the patient is responsible for and any additional payments by third parties for coordination of benefits.
 Average Medicare Payments: The average amount that Medicare pays to the provider for Medicare’s share of the MS-DRG. Average Medicare payment amounts include the MS-DRG amount, teaching, disproportionate share, capital, and outlier payments for all cases. Medicare payments DO NOT include beneficiary co-payments and deductible amounts nor any additional payments from third parties for coordination of benefits.
 package my.proj;
import java.io.Reader;
import java.io.Serializable;
import java.sql.Types;
import java.text.DateFormat;
import java.text.ParseException;
import java.util.ArrayList;
import java.util.Arrays;
import java.lang.Integer;
import java.nio.charset.StandardCharsets;
import java.lang.Double;
import java.util.List;
import org.apache.beam.runners.dataflow.DataflowRunner;
import org.apache.beam.runners.dataflow.options.DataflowPipelineOptions;
import org.apache.beam.sdk.Pipeline;
import org.apache.beam.sdk.extensions.gcp.auth.GcpCredentialFactory;
import org.apache.beam.sdk.extensions.sql.BeamRecordSqlType;
import org.apache.beam.sdk.extensions.sql.BeamSql;
import org.apache.beam.sdk.extensions.sql.BeamSql.SimpleQueryTransform;
import org.apache.beam.sdk.io.FileIO;
import org.apache.beam.sdk.io.Read;
import org.apache.beam.sdk.io.TextIO;
import org.apache.beam.sdk.options.PipelineOptionsFactory;
import org.apache.beam.sdk.transforms.DoFn;
import org.apache.beam.sdk.transforms.ParDo;
import org.apache.beam.sdk.transforms.SerializableFunction;
import org.apache.beam.sdk.transforms.DoFn.ProcessContext;
import org.apache.beam.sdk.transforms.DoFn.ProcessElement;
import static java.nio.charset.StandardCharsets.UTF_8;
import org.apache.beam.sdk.values.BeamRecord;
import org.apache.beam.sdk.values.KV;
import org.apache.beam.sdk.values.PCollection;
import com.google.api.services.storage.Storage;
import com.google.appengine.tools.cloudstorage.ListItem;
import com.google.appengine.tools.cloudstorage.ListOptions;
import com.google.appengine.tools.cloudstorage.ListResult;
import com.google.appengine.tools.cloudstorage.RetryParams;
import com.google.appengine.tools.util.Logging;
import com.google.auth.Credentials;
import com.google.auth.oauth2.GoogleCredentials;
import com.google.bigtable.admin.v2.StorageType;
import com.google.cloud.storage.StorageOptions;
import com.pojo.ClassHospital;
 DataflowPipelineOptions options = PipelineOptionsFactory.as(DataflowPipelineOptions.class);
  options.setProject("Hospital_Charges_Analysis"); 
  options.setStagingLocation("gs://GoogleStorageBucket/staging");
  options.setRunner(DataflowRunner.class);
  DataflowRunner.fromOptions(options);
 Pipeline p = Pipeline.create(options);
 PCollection<String> hosp_obj =p.apply(TextIO.read().from("gs://GoogleStorageBucket/input/hosptital_charges.csv"));
 PCollection<ClassHospital> pojo = hosp_obj.apply(ParDo.of(new DoFn<String, ClassHospital>() { 
 private static final long serialVersionUID = 1L;
   @ProcessElement
   public void processElement(ProcessContext c) {
    String[] strArr = c.element().split(",");

    ClassHospital obj = new ClassHospital();
    obj.setDRGDefinition(strArr[0]);
    obj.setProviderId(strArr[1]);
    obj.setProviderName(Integer.parseInteger(strArr[2]));
    obj.setProviderStreetAddress(strArr[3]);
    obj.setProviderCity((strArr[4]);
    obj.setProviderState(strArr[5]);
    obj.setProviderZipCode(Integer.parseInteger(strArr[6]));
    obj.setHospitalReferralRegionDescription(strArr[7]);
    obj.setTotalDischarges(Integer.parseInteger(strArr[8]));
    obj.setAverageCoveredCharges(Double.parseDouble(strArr[9]));
    obj.setAverageTotalPayments(Double.parseDouble(strArr[10]));
    obj.setAverageMedicarePayments(Double.parseDouble(strArr[11]));
    c.output(obj);
     }
     }
  }));
 List<String> Col_Names = Arrays.asList("DRGDefinition", "ProviderId", "ProviderName", "ProviderStreetAddress","ProviderCity", "ProviderState", "ProviderZipCode","HospitalReferralRegionDescription", "TotalDischarges", "AverageCoveredCharges", "AverageTotalPayments", "AverageMedicarePayments");
   List<Integer> Col_Types = Arrays.asList(Types.VARCHAR, Types.VARCHAR, Types.INTEGER, Types.DOUBLE, Types.INTEGER, Types.VARCHAR, Types.INTEGER,Types.VARCHAR,Types.VARCHAR,Types.DOUBLE,Types.DOUBLE,Types.DOUBLE);
 final BeamRecordSqlType record_Type = BeamRecordSqlType.create(Col_Names, Col_Types);
 PCollection<BeamRecord> record = pojos.apply(ParDo.of(new DoFn<ClassHospital, BeamRecord>() {
   private static final long serialVersionUID = 1L;
   @ProcessElement
   public void processElement(ProcessContext c) {
    BeamRecord br = new BeamRecord(record_Type, c.element().DRGDefinition, c.element().ProviderId, c.element().ProviderName,
      c.element().ProviderStreetAddress, c.element().ProviderCity, c.element().ProviderState, c.element().ProviderZipCode, c.element().HospitalReferralRegionDescription, c.element().TotalDischarges, c.element().AverageCoveredCharges, c.element().AverageTotalPayments, c.element.AverageMedicarePayments);
    c.output(br); 
    }
  })).setCoder(record_Type.getRecordCoder());
 PCollection<BeamRecord> record_trig= record.apply(Window.<BeamRecord>into(new GlobalWindows()).triggering(Repeatedly.forever(AfterWatermark.pastEndOfWindow())).withAllowedLateness(org.joda.time.Duration.standardMinutes(1)).discardingFiredPanes());
 PCollection<BeamRecord> result_1 = record_trig.apply(BeamSql.query(
  "SELECT ProviderState, AVG(AverageCoveredCharges) from PCOLLECTION GROUP BY ProviderState"));
 PCollection<BeamRecord> result_2 = record_trig.apply(BeamSql.query(
  "SELECT ProviderState, AVG(AverageMedicarePayments) from PCOLLECTION GROUP BY ProviderState"));
 PCollection<BeamRecord> result_3 = record_trig.apply(BeamSql.query(
  "SELECT DRGDefinition, ProviderState, SUM(TotalDischarges) as Discharge from PCOLLECTION GROUP BY ProviderState, DRGDefinition"));
 PCollection<BeamRecord> result_4 = result_3.apply(BeamSql.query(
  "SELECT DRGDefinition, ProviderState, Discharge from PCOLLECTION ORDER BY Discharge"));
 PCollection<String> gs_output = result_4.apply(ParDo.of(new DoFn<BeamRecord, String>() {
     private static final long serialVersionUID = 1L;
     @ProcessElement
     public void processElement(ProcessContext c) {
      c.output(c.element().toString());
      System.out.println(c.element().toString());
     }
    }));
 gs_output.apply(TextIO.write().to("gs://GoogleStorageBucket/output/result_4"));
 
 
Item 2698: 
what is the address of the company
please tell me the headquarter address
is the samsung A8 available in pink
 I want to book a flight please
 I need a home insurance
I need an insurance for my car
 
 
Item 2708: 
library(readr)
timetable <- read_csv("C:/Users/my pc/Downloads/isl_wise_train_detail_03082015_v1.csv")
 timetable.srs.dest <- subset(timetable, timetable$`station Code` == timetable$`Destination station Code`)
 library(dplyr)
traingp<-group_by(timetable.srs.dest,`source Station Name`)
trainsfromStation<-summarise(traingp,count=n())
trainsfromStation
arrange(trainsfromStation,desc(count))
write.csv(trainsfromStation,"NumbOfTrainsoriginating.csv")
 trainflux<-group_by(timetable,`Station Name`)
trainflux
trainfluxCount<-summarise(trainflux,count=n())
trainfluxCount
arrange(trainfluxCount,desc(count))
arrange(trainfluxCount,count)
write.csv(trainfluxCount,"trainsstoppingperstation.csv")
 trainNoSource=subset(timetable,dataset$Distance!=0)
trainNoSource$`Arrival time`<- gsub("'",'',trainNoSource$`Arrival time`)
trainNoSource$`Departure time`<- gsub("'",'',trainNoSource$`Departure time`)
trainNoSource$`Arrival time`<-strptime(trainNoSource$`Arrival time`,format = ("%H:%M:%S"))
trainNoSource$`Departure time`<-strptime(trainNoSource$`Departure time`,format = ("%H:%M:%S"))
trainNoSource$`Halt Time`<-as.difftime(trainNoSource$`Departure time`-trainNoSource$`Arrival time`,format = "%H:%M:%S",units="mins")
 trainNoSource=subset(trainNoSource,trainNoSource$`Halt Time`>=0)
trainNoSource$`Arrival time`<-NULL
trainNoSource$`Departure time`<-NULL
trainNoSource_grpstation<-group_by(trainNoSource,`Station Name`)
trainNoSource_haltTimes<-summarise(trainNoSource_grpstation,count=n(),totalhalt=sum(trainNoSource_grpstation$`Halt Time`))
arrange(trainNoSource_haltTimes,desc(count))
trainNoSource_haltTimes<-summarise(trainNoSource_grpstation,count=n(),totalhalt=sum(`Halt Time`))
arrange(trainNoSource_haltTimes,desc(count))
arrange(trainNoSource_haltTimes,desc(totalhalt))
trainNoSource_haltTimes$`avgstop`<-trainNoSource_haltTimes$totalhalt/trainNoSource_haltTimes$'count'
arrange(trainNoSource_haltTimes,desc(avgstop))
write.csv(trainNoSource_haltTimes,"avgHaltTimeperstation.csv")
 
 
Item 2710: 
public static void main(String[] args){
 Pipeline p = Pipeline.create(useCloudDataflow());

 p.apply(FileIO.match()
 .filepattern("gs://naismith-dev/sessions/*/*.txt"))
 .apply(new ForceShuffle<>(25))
 .apply(FileIO.readMatches())
 .apply(ParDo.of(new ExtractFileFn()))
 .apply(new SessionWriter());

 p.run();
 }
 Shot count: 1077
 Overall make percentage: 0.420
 Average shot distance: 13.148
 Average shot angle (offset cft): 91.976
 Average shot release angle: 47.833
 Average shot release speed: 14.478
 Average shot release height: 2490.105
 [‘distance’,’angle_to_rim’, ‘prior_1_distance’,’prior_1_angle_to_rim’,’prior_1_launch_speed’, ‘prior_1_launch_angle’,’prior_1_release_height’,’prior_1_release_dist_to_rim’,
 ‘Prior_1_outcome’, ‘prior_2_distance’,’prior_2_angle_to_rim’,’prior_2_launch_speed’, ‘prior_2_launch_angle’,’prior_2_release_height’,’prior_2_release_dist_to_rim’,’prior_2_outcome’,
 ‘priors_distance_avg’,’priors_angle_to_rim_avg’,’priors_launch_speed_avg’, ‘priors_launch_angle_avg’,’priors_release_height_avg’,
 ‘priors_deviation_avg’,’priors_release_dist_to_rim_avg’]
 
 
Item 2715: 
“Job Security: What happens when AI takes over web design?” by Josh Aarons (@joshaarons). Learn More on Noteworthy >
 “How Clarifai Buils Accurate and Unbiased AI Technology” submitted by Avi Eisenberger (@aeisenberger). Learn More on Clarifai >
 “Business questions engineers should ask when interviewing at ML/AI companies” submitted by Samiur Rahman (@samiur1204). Learn More on Medium >
 “Importance Of Bloomberg’s Article On Apple’s AI Headset Project” submitted by Carl DeBrule (@carldebrule). Learn More on Seeking Alpha >
 
 
Item 2716: 
sagify cloud train -d local-src-dir/ -i s3://my-bucket/training-data/ -o s3://my-bucket/model-output/ -e ml.m4.xlarge
 try:
    my_train_logic(
         input_path=input_data_path,
         output_path=model_save_path
    )
    print('Training complete.')
except Exception as e:
 
 
Item 2718: 
m = length(y);
h = sigmoid(X * theta);
theta_reg = [0; theta(2:length(theta))];
 reg_loss = (lambda / (2 * m)) * theta_reg’ * theta_reg;
data_loss = (1 / m) * (-y’ * log(h) — (1 — y)’ * log(1 — h));
 J = data_loss + reg_loss;
grad = (1 / m) * (X’ * (h — y) + lambda * theta_reg);
 
 
Item 2719: 
table(DSInt_df$AttendedBootcamp)
 0     1
622   13
 DSInt_df$AttendedBootcamp <- factor(DSInt_df$AttendedBootcamp, levels = c(1, 0), labels = c('Yes','No'))
 table(FCC_df$JobRelocateYesNo)
Yes  No 
 13 622
 DSInt_df %>%
    ggplot(aes(AttendedBootcamp)) +
    geom_bar(fill = "green")
 DSInt_df$IsSoftwareDev <- factor(DSInt_df$IsSoftwareDev, levels = c(1, 0), labels = c('Yes','No'))
 table(DSInt_df$IsSoftwareDev)
Yes  No
  0   0
 summary(DSInt_df$IsSoftwareDev)
 Yes   No NA's 
   0    0  646
 DSInt_df %>%
ggplot(aes(CityPopulation)) +
  geom_bar(fill = "magenta")
 DSInt_df %>%
  ggplot(aes(CityPopulation, fill = EmploymentStatus)) +
  geom_bar()
 DSInt_df %>%
  ggplot(aes(CityPopulation, fill = EmploymentStatus)) +
  geom_bar(position = "dodge"
 DSInt_df %>%
  ggplot(aes(SchoolDegree)) +
  geom_bar(fill = "Turquoise") +
  coord_flip()
 ggplot(DSInt_df, aes(x = ExpectedEarning)) +
  geom_histogram(fill = "dark green")
 DSInt_df %>%
  mutate(SchoolDegree = fct_infreq(SchoolDegree)) %>%
  ggplot(aes(SchoolDegree, ExpectedEarning)) +
  geom_boxplot() +
    coord_flip()
 
 
Item 2720: 
cd "C:\Program Files\MongoDB\Server\3.4\bin"
 mongod --dbpath "D:\data"
 use pokemon
 db.pokemon.insert({
        _id: ObjectId(),
        title: 'Pokedex',
        description: 'Pokedex is the pokemon database',
        pkid: '001',
        pkname: 'Chamander',
        pktype: 'Fire',
        pkevol: 16
})
 db.pokemon.find()
 
 
Item 2722: 
def explore(values):
    '''
    Returns the index of maximum average value 
    for an action
    '''
    return values.index(max(values))
 def exploit(values):
    '''
    Returns random average value for an action
    '''
    return random.randrange(len(values))
 def select_action(epsilon, values):
    '''
    If the random number is greater than epsilon
    then we exploit else we explore.
    '''
    if random.random() > epsilon:
        return exploit(values)
    else: 
        return explore(values)
 def update_counts(action):
    '''
    Update the value of count corresponding 
    to the action that is taken.
    '''
    counts[action] = counts[action] + 1
    return counts
 def update_values(action, reward, counts):
    '''
    Calculates the estimated value for the chosen
    action. If this is the first experience ever with 
    the chosen arm, we set the estimated value  directly 
    to the reward we just received from playing that arm.
    If we had played the arm in the past, i.e n != 0, 
    we update the estimated value of the chosen arm to 
    be a weighted average of the previously estimated  
    value and the reward we just received.
    '''
    current_value = values[action]
    n = counts[action]
    values[action] = ((n-1)/float(n))*value + (1/float(n))*reward
    return values
 def update_all(action, reward):
    counts = update_counts(action)
    values = update_values(action, reward, counts)
    return counts, values
 
 
Item 2726: 
select * from "Samples.'samples.dremio.com'"."SF_incidents2016.json"
 
 
Item 2730: 
<button>
<text>Tell Me More</text>
<postback>xmoreabout foo</postback>
</button>
 <button>
<text>Visit Us</text>
<url>https://home.pandorabots.com</url>
</button>
 <reply>
<text>yes</text>
<postback>xsayyesto foo</postback>
</reply>
 <image>https://url.for.image</image>
 <video>https://url.for.video</video>
 <card>
 <title>Card Menu</title>
 <subtitle>Describe a card</subtitle>
 <image>https://url.for.image</image>
 <button>
  <text>Learn More</text>
  <postback>xlearnmoreabout cards</postback>
 </button>
</card>
 <carousel>
 <card>
  <title>Robots</title>
  <subtitle>Made of metal</subtitle>
  <image>https://url.for.image</image>
  <button>Tell Me More</button>
 </card>
 <card>
  <title>Humans</title>
  <subtitle>Made of carbon</subtitle>
  <image>https://url.for.image</image>
  <button>Tell Me More</button>
 </card>
</carousel>
 <delay>3</delay>
 Some text
<split/>
More text
 <category><pattern>MY NAME IS *</pattern>
<template>
<think><set name=”clientname”><star/></set></think>
 <msg lang=”en”>Your name is <star/>.</msg>
 <msg lang=”fr”>Votre nom est <star/>.</msg>
</template>
</category>
 <category><pattern>JE M APPELLE *</pattern>
<template><srai>MY NAME IS <star/></srai></template>
</category>
 Input to Chatbot: My name is Rosie.
 Output from Chatbot: <msg lang=“en”>Your name is Rosie.</msg>
                     <msg lang=“fr”>Votre nom est Rosie.</msg>
 <category><pattern>XGETPREDICATES</pattern>
 <template>
 <json>
 {
   “firstname”: “<get name=”firstname” />”,
   “lastname”: “<get name=”lastname” />”,
   “birthday”: “<get name=”birthday” />”
 }
 </json>
 </template>
</category>
 <category><pattern>XSEEDCHATBOT * *</pattern>
 <template>
  xignore
 <star/> = <set><name><star/></name><star index=”2"/></set>
 </template>
</category>
 Input to Chatbot: XSEEDCHATBOT firstname Rosie
 Output from Chatbot: xignore firstname = Rosie
 <category><pattern>XBUILDPRODUCTCAROUSEL</pattern>
 <template>
  xsendproductlist
   xcategory catIdXXXXXXXX
   xaimlbody
 <carousel>
  <card>
  <image>xgetattribute productImageUrl</image>
  <title>xgetattribute productDisplayName</title>
  <subtitle>xgetattribute productDescription</subtitle>
  <button>
  <title>SHOP</title><url>xgetattribute productPageUrl</subtitle>
  </button>
  </card>
 </carousel> 
 xaimlbodyend
 </template>
</category>
 
 
Item 2735: 
ufos <- read.csv("scrubbed.csv")
splitDf <- split_k(ufos,4)
 ufo1 <- splitDf$`1`
ufo2 <- splitDf$`2`
ufo3 <- splitDf$`3`
ufo4 <- splitDf$`4`
 ufo1And2 <- rbind(ufo1,ufo2)
 > NROW(ufo1)
[1] 20083
> NROW(ufo2)
[1] 20083
> NROW(ufo1And2)
[1] 40166
 do.call("rbind", list(dataframes to merge))
 ufoMerged <- do.call("rbind", list(ufo1, ufo2, ufo3, ufo4))
 > NROW(ufoMerged)
[1] 80332
> NROW(ufos)
[1] 80332
 library(dplyr)
ufoChecker <- anti_join(ufoMerged, ufos)
ufoChecker
 [1] datetime             city                 state                country             
 [5] shape                duration..seconds.   duration..hours.min. comments            
 [9] date.posted          latitude             longitude           
<0 rows> (or 0-length row.names)
 
 
Item 2738: 
crontab -e
 0 */2 * * * root /var/projects/my_scrape_and_feed_code.py
 
 
Item 2741: 
DO
 Creative, useful, enjoy, best, pet, price, esteem, launching, priceless,essential,economic, cosy, ready, exciting, just, available, impressive, benefit, accessible, eternal, auspicious, topnotch, needs, beautiful, nature
 DON'T
 Absence, famous, thrilling, cheap, prominent, original, artificial, loss, gone, unlimited, separate, finishing, zero, less, amount, professional, ultramodern.
 Pronoun      Us   Your  You   The   We   Yourself   That  This   Our
 Avg CTR      12%   8%   7%    8%    7%     11%       10%   7%    11%
 Call-Action   Get    Buy   Enjoy  Visit  Build    Learn    Discover
 Avg. CTR       9%     5%     10%    7%      8%        6%       7%
 (!)  Exclamation marks       18% of  Ads        with avg.  7% CTR
 (%)  Percentage              10% of  Ads        with avg.  9% CTR
 ($)  Dollar Sign             1%  of  Ads        with avg.  1% CTR
 (?)  Question mark           1%  of  Ads        with avg.  4% CTR
 
 
Item 2742: 
              W = 0.01 * np.random.randn(fan_in, fan_out)
               W = 1.0 * np.random.randn(fan_in, fan_out)
 W = 1.0 * np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
 J(theta) - Overall objective function to minimize.
n        - Number of training samples.
y(i)     - Actual label for ith training sample.
y_hat(i) - Predicted label for ith training sample.
Loss     - Cross entropy loss.
Theta    - Weights of neural network.
Lambda   - Regularization parameter.
 
 
Item 2753: 
select image, geocode(latlng) from mapbox.satellite where imagery_prediction(image) contains ‘lake’
 
 
Item 2759: 
User: Alexa, what are my appointments today? 
 Alexa: You have three dates today. At half past nine, you have the meeting with Carlos. At half past twelve, you have lunch with Ana and at half past five you have the hairdresser's appointment.
 
 
Item 2760: 
eval_list = [train_optimizer, merged_summary, model_loss,              train_accuracy, valid_accuracy]        
for i in range(NUM_EPOCHS):            
    _, summary, loss, accuracy_t, accuracy_v = sess.run(eval_list)                       
    train_writer.add_summary(summary)
 
 
Item 2761: 
OUTPUT:
{'bali': 1, 'island': 3, 'country': 2, 'peru': 5, 'south': 6, 'america': 0, 'japan': 4}
 OUTPUT:
matrix([[0, 1, 1, 1, 0, 0, 0], 
        [1, 0, 1, 0, 0, 1, 1],
        [0, 0, 2, 0, 0, 0, 0],
        [0, 0, 1, 1, 1, 0, 0]])
 IDF: 
[ 1.916  1.916  1. 1.510  1.916 1.916 1.916]
 OUTPUT:
[[ 0.   0.726   0.379   0.572   0.   0.   0.   ]
 [ 0.55 0.      0.288   0.      0.   0.55 0.552]
 [ 0.   0.      1.      0.      0.   0.   0.   ]
 [ 0.   0.      0.379   0.572   0.72 0.   0.   ]]
 
 
Item 2762: 
df = spark.read.format(“csv”).load(“wasbs://blob_container@account_name.blob.core.windows.net/example.csv”, inferSchema = True)
 
 
Item 2763: 
Min 87.0
Max 90.0
Avg 88.5
Std 0.8062257748296293
 Min 81.0
Max 90.0
Avg 88.92
Std 0.8084553172560881
 Min 89.0
 Max 90.0
 Avg 89.1
 Std 0.30000000000175836
 Min 87.0
Max 90.0
Avg 88.45
Std 0.5361902647376715
 
 
Item 2764: 
Skcript has spent over 4 years crafting a powerful Artificial Intelligence engine called the UnderstandEngine. This enables us to build customised AI based solutions faster for businesses, with accuracies like never seen before.
 
 
Item 2769: 
W_one = tf.get_variable('weights_1', [85 * 85 * 3, 1000], initializer=xavier())
b_one = tf.get_variable('bias_1', [1000], initializer=zeros())
logits_one = tf.add(tf.matmul(inputs, W1), b1)
layer_one = tf.nn.relu(logits1)
 layer_one = slim.fully_connected(inputs, 1000)
 
 
Item 2770: 
def expectedPercentileRanking(model: MatrixFactorizationModel, ratings: RDD[Rating]) = {
    val itemFactors = model.productFeatures.collect()
    val itemMatrix = new DoubleMatrix(itemFactors.map(_._2))
    val imBroadCast = sc.broadcast(itemMatrix)
    val itemListPerUser = ratings.groupBy(_.user).map {
      case (user, ratingList) => (user, ratingList.map(rt => (rt.product, rt.rating)).toArray)
    }
    val rankRDD = model.userFeatures.join(itemListPerUser).map {
      case (userId, (userFeatures, itemRatingList)) =>
        val userVector = new DoubleMatrix(userFeatures)
        val scores = imBroadCast.value.mmul(userVector)
        val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)
        val itemsOrderedByPref = sortedWithId.map(_._2).toSeq
        val rankWeightedByRating = itemRatingList.map {
          case (itemId, rating) =>
            rating * itemsOrderedByPref.indexOf(itemId).toDouble / (itemsOrderedByPref.size - 1)
        }.sum
        rankWeightedByRating
    }
    val weightedRankOverAll = rankRDD.sum
    val sumWeight = ratings.map(_.rating).sum()
    weightedRankOverAll / sumWeight
  }
 def expectedPercentileRank(model: ALSModel, ratings: RDD[Rating]) = {
    val itemFactors = model.itemFactors.rdd.collect()
    val itemMatrix = new FloatMatrix(
      itemFactors.map(r => r.getAs[Seq[Float]](1).toArray))
      
    val items = itemFactors.map(r => r.getAs[Int](0))
    
    val userFactors = model.userFactors.rdd.map { r =>
      (r.getAs[Int](0), r.getAs[Seq[Float]](1).toArray)
    }
    
    val itemListPerUser = ratings.groupBy(_.customer_id).map {
      case (user, ratingList) =>
        (user, ratingList.map(rt => (rt.product_id, rt.count)).toArray)
    }
    
    val rankRDD = userFactors.join(itemListPerUser).map {
      case (userId, (userFeatures, itemRatingList)) =>
        val userVector = new FloatMatrix(userFeatures)
        val scores = itemMatrix.mmul(userVector)
        val sortedWithId = scores.data.zip(items).zipWithIndex.sortBy(-_._1._1).toSeq
        val itemsOrderedByPref = sortedWithId.map(_._1._2).toSeq
        val rankWeightedByRating = itemRatingList.map {
          case (itemId, rating) =>
            rating * (itemsOrderedByPref
              .indexOf(itemId)
              .toDouble) / (itemsOrderedByPref.size)
        }.sum
        rankWeightedByRating
    }
    val weightedRankOverAll = rankRDD.sum
    val sumWeight = ratings.map(_.count).sum()
    val rank = weightedRankOverAll / sumWeight
 rank
  }
 
 
Item 2771: 
curl -X POST -H "Content-Type: multipart/form-data" -F "url=https://wt-12345-0.run.webtask.io/sentigram-webtask" 'https://api.telegram.org/bot1234:1234567890/setWebhook'
 {"ok":true,"result":true,"description":"Webhook was set"}
 
 
Item 2774: 
MatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratings), rank, numIterations, 0.01);
 JavaPairRDD<Tuple2<Integer, Integer>, Double> predictions = JavaPairRDD.fromJavaRDD(
  model.predict(JavaRDD.toRDD(userProducts)).toJavaRDD()
      .map(r -> new Tuple2<>(new Tuple2<>(r.user(), r.product()), r.rating()))
);
 
 
Item 2775: 
select
  count(
    case
      when ab_test_entrants.user_id is not null
      and ab_test_conversions.user_id is null
        then 1
      else null
    end
  )
  as users_entered_but_not_converted
  , count(
    case
      when ab_test_entrants.user_id is not null
      and ab_test_conversions.user_id is not null
        then 1
      else null
    end
  )
  as users_entered_and_converted
  , count(
    case
      when ab_test_entrants.user_id is null
      and ab_test_conversions.user_id is not null
        then 1
      else null
    end
  )
  as users_converted_and_not_entered
from
  [ab_test_entrants]
  full outer join [ab_test_conversions] using (user_id)
 select
  datediff('day', date(ab_test_entrant_date), date(conversion_date)) as days_from_entry_to_conversion
  , count(*) as num_entrants
from
  [ab_test_entrants]
  left join [ab_test_conversions] using (user_id)
group by
  1
 select
  ab_test_version
  , count(ab_test_entrants.user_id) as ab_test_entrants
  , count(ab_test_conversions.user_id) as ab_test_conversions
from
  [ab_test_entrants]
  left join [ab_test_conversions] using (user_id)
group by
  1
 select
  ab_test_version
  , count(ab_test_conversions.user_id)::float /     
    count(ab_test_entrants.user_id) as conversion_rate
from
  [ab_test_entrants]
  left join [ab_test_conversions] using (user_id)
group by
  1
 with
  ab_test_conversions as (
    select
      ab_test_version
      , count(ab_test_entrants.user_id) as ab_test_entrants
      , count(ab_test_conversions.user_id) as ab_test_conversions
    from
      [ab_test_entrants]
      left join [ab_test_conversions] using (user_id)
    group by
      1
  )
  , ab_test_conversion_rates as (
    select
      ab_test_version
      , ab_test_entrants
      , ab_test_conversions
      , (ab_test_conversions + 1.92) / (ab_test_entrants + 3.84 )::float as conversion_rate
    from
      ab_test_conversions
  )
  , conversion_rate_standard_error as (
    select
      ab_test_version
      , ab_test_entrants
      , ab_test_conversions
      , conversion_rate
      , sqrt(conversion_rate * (1 - conversion_rate) / ab_test_entrants) as standard_error
    from
      ab_test_conversion_rates
  )
select
  ab_test_version
  , ab_test_entrants
  , ab_test_conversions
  , conversion_rate - standard_error * 1.96 as conversion_rate_low
  , conversion_rate
  , conversion_rate + standard_error * 1.96 as conversion_rate_high
from
  conversion_rate_standard_error
 with
  ab_test_conversions_over_time as (
    select
      date(ab_test_entrant_date) as ab_test_entrant_date
      , ab_test_version
      , count(ab_test_entrants.user_id) as ab_test_entrants
      , count(ab_test_conversions.user_id) as ab_test_conversions
    from
      [ab_test_entrants]
      left join [ab_test_conversions] using (user_id)
    group by
      1
      , 2
  )
  , ab_test_conversions_cumulative as (
    select
      ab_test_entrant_date
      , ab_test_version
      , sum(ab_test_entrants) over(partition by ab_test_version order by ab_test_entrant_date rows unbounded preceding) as cumulative_entrants
      , sum(ab_test_conversions) over(partition by ab_test_version order by ab_test_entrant_date rows unbounded preceding) as cumulative_conversions
    from
      ab_test_conversions_over_time
  )
select
  ab_test_entrant_date
  , ab_test_version
  , cumulative_conversions::float / cumulative_entrants as cumulative_conversion_rate
from
  ab_test_conversions_cumulative
 Parameterized SQL Snippet:
ab_test_confidence_intervals(ab_test_name)
 with
  ab_test_conversions as (
    select
      ab_test_version
      , count(ab_test_entrants.user_id) as ab_test_entrants
      , count(ab_test_conversions.user_id) as ab_test_conversions
    from
      [ab_test_entrants]
      left join [ab_test_conversions] using (user_id)
    where
      ab_test_name=[ab_test_name] -- dynamic SQL Snippet parameter
    group by
      1
  )
  , ab_test_conversion_rates as (
    select
      ab_test_version
      , ab_test_entrants
      , ab_test_conversions
      , (ab_test_conversions + 1.92) / (ab_test_entrants + 3.84 )::float as conversion_rate
    from
      ab_test_conversions
  )
  , conversion_rate_standard_error as (
    select
      ab_test_version
      , ab_test_entrants
      , ab_test_conversions
      , conversion_rate
      , sqrt(conversion_rate * (1 - conversion_rate) / ab_test_entrants) as standard_error
    from
      ab_test_conversion_rates
  )
select
  ab_test_version
  , ab_test_entrants
  , ab_test_conversions
  , conversion_rate - standard_error * 1.96 as conversion_rate_low
  , conversion_rate
  , conversion_rate + standard_error * 1.96 as conversion_rate_high
from
  conversion_rate_standard_error
 [ab_test_confidence_intervals('new_funnel_test_2017-12-01')]
 
 
Item 2777: 
scipy
h5py
numpy
tensorflow
tflearn
 $pip install tensorflow tflearn
...
Successfully installed bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 numpy-1.13.1 protobuf-3.4.0 six-1.10.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.6 werkzeug-0.12.2 Pillow-4.2.1 olefile-0.44 tflearn-0.3.2
 $python
Python 3.6.2 (v3.6.2:5fd33b5926, Jul 16 2017, 20:11:06)
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
import tflearn
hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)
Scipy not supported!
 bleach==1.5.0
h5py==2.7.1
html5lib==0.9999999
Markdown==2.6.9
numpy==1.13.1
olefile==0.44
Pillow==4.2.1
protobuf==3.4.0
scipy==0.19.1
six==1.10.0
tensorflow==1.3.0
tensorflow-tensorboard==0.1.6
tflearn==0.3.2
Werkzeug==0.12.2
 scipy==0.19.1
h5py==2.7.1
numpy==1.13.1
tensorflow==1.0.0
tflearn==0.3.2
 
 
Item 2780: 
<category>
<pattern>*</pattern>
<template>I have no answer for that.</template>
</category>
 <pattern>HELLO *</pattern>
 <pattern>HELLO ^</pattern>
 <pattern>1000</pattern>
:
<pattern><set>number</set></pattern>
 <pattern>$WHO IS MIKE</pattern>
:
<pattern>_ MIKE</pattern>
 <category>
<pattern>MY NAME IS * AND I AM * YEARS OLD</pattern>
<template>Hi <star/>. I am also <star index="2"/> years old!</template>
</category>
 <category>
<pattern>WHY</pattern>
<that>I LIKE *</that>
<template><srai>Why do you like <thatstar index="1"/>
</template>
</category>
 
 
Item 2781: 
dataset = read.csv("OnlineRetail.csv", na.strings = c("","NA"))
 attach(dataset)
 #checking if there any missing values, where are they missing, and how many of them are missing
 any(is.na(dataset))
 [1] TRUE
 apply(dataset, 2, function(x) any(is.na(x)))
 InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country
 FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE
 sum(is.na(CustomerID))
 [1] 135080
 sum(is.na(Description))
 [1] 1454
 dataset$InvoiceNo = as.character(InvoiceNo)
trim = function (x) gsub("^\\s+|\\s+$", "", x)
dataset$InvoiceNo = trim(InvoiceNo)
dataset$Description = trim(as.character(Description))
 is_C = function (x) startsWith(x,"C")
dataset2 = dataset[which(!is_C(dataset$InvoiceNo)),] #subsetting
dataset3 = subset(dataset2,!is.na(dataset2$Description)) #subsetting
 buzzwords = c("WRONG","LOST", "CRUSHED", "SMASHED", "DAMAGED", "FOUND", "THROWN", "MISSING", "AWAY", "\\?", "CHECK", "POSTAGE", "MANUAL", "CHARGES", "AMAZON", "FEE", "FAULT", "SALES", "ADJUST", "COUNTED", "LABEL", "INCORRECT", "SOLD", "BROKEN", "BARCODE", "CRACKED", "RETURNED", "MAILOUT", "DELIVERY", "MIX UP", "MOULDY", "PUT ASIDE", "ERROR", "DESTROYED", "RUSTY")
 library(stringr)   #function str_detect
 isUndesirable = function(x){
 c = FALSE   #assume that the string is undesirable (FALSE), and perhaps switch to desirable (TRUE)
 for (i in 1:(length(buzzwords))){
  c = c || ifelse(str_detect(toupper(x),buzzwords[i]),TRUE,FALSE)
}
 #now we know whether or not the string is undesirable
 return(c)
 }
 dataset4 = subset(dataset3, dataset3$Quantity > 0)
 dataset5 = dataset4[which(!isUndesirable2(as.character(dataset4$Description))),]
 Time = format(as.POSIXct(strptime(dataset5$InvoiceDate,"%Y-%m-%d %H:%M",tz="")) ,format = "%H:%M:%S")
 dataset5$InvoiceDate = as.Date(dataset5$InvoiceDate)
dataset5$Description = as.factor(dataset5$Description)
 
 
Item 2784: 
def transform_roc(X1, X2, X3):    
    n_batch = len(X1)    
    xmb = np.zeros((n_batch, 2, n_ctx, 2), dtype=np.int32)    
    mmb = np.zeros((n_batch, 2, n_ctx), dtype=np.float32)    
    start = encoder['_start_']    
    delimiter = encoder['_delimiter_']    
    for i, (x1, x2, x3), in enumerate(zip(X1, X2, X3)):        
        x12 = [start]+x1[:max_len]+[delimiter]+\
              x2[:max_len]+[clf_token]        
        x13 = [start]+x1[:max_len]+[delimiter]+\
              x3[:max_len]+[clf_token]        
        l12 = len(x12)        
        l13 = len(x13)
        xmb[i, 0, :l12, 0] = x12
        xmb[i, 1, :l13, 0] = x13
        mmb[i, 0, :l12] = 1
        mmb[i, 1, :l13] = 1
    xmb[:, :, :, 1] = np.arange(
        n_vocab+n_special, n_vocab+n_special+n_ctx)
    return xmb, mmb
 def transform_texts(list_of_texts):    
    tokens = TEXT_ENCODER.encode(list_of_texts, verbose=False)
    n_batch = len(tokens)
    xmb = np.zeros((n_batch, N_CTX, 2), dtype=np.int32)
    mmb = np.zeros((n_batch, N_CTX), dtype=np.float32)
    for i, x in enumerate(tokens):
        x1 = x[:N_CTX]
        l1 = len(x1)
        print(f"length: {l1}")
        xmb[i, :l1, 0] = x1
        mmb[i, :l1] = 1
    xmb[:, :, 1] = np.arange(N_VOCAB, N_VOCAB+N_CTX)
    return xmb, mmb
 we = tf.get_variable(
    "we", 
    [n_vocab+n_special+n_ctx, n_embd],                              
    initializer=tf.random_normal_initializer(stddev=0.02))
we = dropout(we, embd_pdrop, train)
 X = tf.reshape(X, [-1, n_ctx, 2])
M = tf.reshape(M, [-1, n_ctx])
 lm_h = tf.reshape(h[:, :-1], [-1, n_embd])
lm_logits = tf.matmul(lm_h, we, transpose_b=True)       
lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(  
    logits=lm_logits, labels=tf.reshape(X[:, 1:, 0], [-1]))  lm_losses = tf.reshape(
    lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])        lm_losses = tf.reduce_sum(
    lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)
 lm_h = tf.reshape(h, [-1, N_EMBD])        
lm_logits = tf.reshape(
    tf.matmul(lm_h, we[:N_VOCAB, :], transpose_b=True),
    [-1, N_CTX, N_VOCAB])        
lm_logits_truncated = tf.reshape(        
    lm_logits[:, :-1],        
    [-1, N_VOCAB])
lm_losses = tf.nn.sparse_softmax_cross_entropy_with_logits(
    logits=lm_logits_truncated, 
    labels=tf.reshape(X[:, 1:, 0], [-1]))
lm_losses = tf.reshape(       
    lm_losses, [shape_list(X)[0], shape_list(X)[1]-1])        lm_losses = tf.reduce_sum(        
    lm_losses*M[:, 1:], 1)/tf.reduce_sum(M[:, 1:], 1)
 
 
Item 2786: 
INFO:root:Epoch[0] Batch [20] Speed: 410198.92 samples/sec mae=252.519361
INFO:root:Epoch[0] Batch [20] Speed: 485757.12 samples/sec mae=249.524583
INFO:root:Epoch[0] Batch [40] Speed: 604001.95 samples/sec mae=252.177676
INFO:root:Epoch[0] Batch [40] Speed: 580103.10 samples/sec mae=248.072072
INFO:root:Epoch[0] Batch [60] Speed: 582447.42 samples/sec mae=251.373124
INFO:root:Epoch[0] Batch [60] Speed: 584691.36 samples/sec mae=247.879646
INFO:root:Epoch[0] Train-mae=252.523378
INFO:root:Epoch[0] Train-mae=247.340798
INFO:root:Epoch[0] Time cost=0.148
INFO:root:Epoch[0] Time cost=0.148
INFO:root:Epoch[0] Validation-mae=247.291481
INFO:root:Epoch[0] Validation-mae=251.256459
INFO:root:Epoch[1] Batch [20] Speed: 597801.87 samples/sec mae=252.482471
INFO:root:Epoch[1] Batch [20] Speed: 595002.67 samples/sec mae=249.561601
INFO:root:Epoch[1] Batch [40] Speed: 593674.42 samples/sec mae=252.141089
INFO:root:Epoch[1] Batch [40] Speed: 595555.46 samples/sec mae=248.108620
INFO:root:Epoch[1] Batch [60] Speed: 544159.24 samples/sec mae=251.336894
INFO:root:Epoch[1] Batch [60] Speed: 528026.47 samples/sec mae=247.915919
INFO:root:Epoch[1] Train-mae=252.487181
INFO:root:Epoch[1] Time cost=0.134
INFO:root:Epoch[1] Train-mae=247.376799
INFO:root:Epoch[1] Time cost=0.135
INFO:root:Epoch[1] Validation-mae=251.220531
INFO:root:Epoch[1] Validation-mae=247.327390
INFO:root:Epoch[2] Batch [20] Speed: 567127.15 samples/sec mae=252.446482
INFO:root:Epoch[2] Batch [20] Speed: 587089.04 samples/sec mae=249.597713
INFO:root:Epoch[2] Batch [40] Speed: 597311.35 samples/sec mae=252.105400
INFO:root:Epoch[2] Batch [40] Speed: 578497.42 samples/sec mae=248.144278
INFO:root:Epoch[2] Batch [60] Speed: 598497.45 samples/sec mae=247.951315
INFO:root:Epoch[2] Batch [60] Speed: 568493.35 samples/sec mae=251.301550
INFO:root:Epoch[2] Train-mae=247.411917
INFO:root:Epoch[2] Train-mae=252.451868
INFO:root:Epoch[2] Time cost=0.148
INFO:root:Epoch[2] Time cost=0.149
INFO:root:Epoch[2] Validation-mae=247.362424
INFO:root:Epoch[2] Validation-mae=251.185472
INFO:root:Epoch[3] Batch [20] Speed: 729345.08 samples/sec mae=249.632942
INFO:root:Epoch[3] Batch [20] Speed: 715732.45 samples/sec mae=252.411375
INFO:root:Epoch[3] Batch [40] Speed: 796079.31 samples/sec mae=252.070580
INFO:root:Epoch[3] Batch [40] Speed: 733837.48 samples/sec mae=248.179065
INFO:root:Epoch[3] Batch [60] Speed: 708185.38 samples/sec mae=247.985841
INFO:root:Epoch[3] Batch [60] Speed: 667858.92 samples/sec mae=251.267068
INFO:root:Epoch[3] Train-mae=247.446186
INFO:root:Epoch[3] Time cost=0.106
INFO:root:Epoch[3] Train-mae=252.417416
 
 
Item 2788: 
subject_headline = news_headlines[0]
subject_token = headline_tokens[0]
 print('Headline: ', subject_headline)
print('=' * 50)
print()
 for token, headline in zip(headline_tokens, news_headlines):
    print('-' * 50)
    print('Comparing to:', headline)
    distance = glove_model.wmdistance(subject_token, token)
    print('distance = %.4f' % distance)
 Headline:  Elon Musk's Boring Co to build high-speed airport link in Chicago
==================================================

--------------------------------------------------
Comparing to: Elon Musk's Boring Co to build high-speed airport link in Chicago
distance = 0.0000
--------------------------------------------------
Comparing to: Elon Musk's Boring Company to build high-speed Chicago airport link
distance = 0.3589
--------------------------------------------------
Comparing to: Elon Musk’s Boring Company approved to build high-speed transit between downtown Chicago and O’Hare Airport
distance = 1.9456
--------------------------------------------------
Comparing to: Both apple and orange are fruit
distance = 5.4350
 
 
Item 2795: 
I like apple.
{"I": 1, "like": 1, "apple": 1}
 I like mango.
{"I": 1, "like": 1, "mango": 1}
 I like apple, but I don't like mango.
I like mango, but I don't like apple.
 ["I", "like", "apple", "mango"]
 I     => [1, 0, 0, 0]
like  => [0, 1, 0, 0]
apple => [0, 0, 1, 0]
mango => [0, 0, 0, 1]
 "I like apple" =>
[[1, 0, 0, 0],
 [0, 1, 0, 0],
 [0, 0, 1, 0]]
 "I like mango" =>
[[1, 0, 0, 0],
 [0, 1, 0, 0],
 [0, 0, 0, 1]]
 
 
Item 2796: 
y = mx + b
 
 
Item 2800: 
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::mycompanyname-openbridge-athena/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetBucketLocation",
                "s3:CreateBucket",
                "s3:ListBucket",
                "s3:ListBucketMultipartUploads"
            ],
            "Resource": [
                "arn:aws:s3:::mycompanyname-openbridge-athena"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "athena:*"
            ],
            "Resource": [
                "*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "glue:CreateDatabase",
                "glue:DeleteDatabase",
                "glue:GetDatabase",
                "glue:GetDatabases",
                "glue:UpdateDatabase",
                "glue:CreateTable",
                "glue:DeleteTable",
                "glue:BatchDeleteTable",
                "glue:UpdateTable",
                "glue:GetTable",
                "glue:GetTables",
                "glue:BatchCreatePartition",
                "glue:CreatePartition",
                "glue:DeletePartition",
                "glue:BatchDeletePartition",
                "glue:UpdatePartition",
                "glue:GetPartition",
                "glue:GetPartitions",
                "glue:BatchGetPartition"
            ],
            "Resource": [
                "*"
            ]
        }
    ]
}
 
 
Item 2804: 
movie_series['Budget']=details_soup.find(text='Budget').find_parent().find_parent().text
 Series.str.get_dummies(sep='|')
 
 
Item 2805: 
df.withColumn('new', F.lit('A')) 
 df = spark.createDataframe([(None,None),(1,None),(None,2)],(a,b)) 
 df.select(coalesce(df['a'], df['b']))
 df = sqlContext.createDataFrame([('a', None, None),('a', 'code1', None) ('a', 'code2', 'name2'),], ['id', 'code', 'name']) 
 df2 = df.groupby('id').agg(collect_set('code'), collect_list('name')) 
 df = sqlContext.createDataFrame([('a', None, None),('a', 'code1', None),
 ('a', 'code2', 'name2'),], ['id', 'code', 'name']) 
 df2 = df.select(concat(df['code'], df['name']).alias('concadenado')) 
 df = sqlContext.createDataFrame([('2'),('3')], ['numero']) 
 df2 = df.select(bin(df['numero']).alias('binario')) 
 df = spark.createDataFrame([(1, 'A', [1,2,3]), (2, 'B', [3,5])],['col1', 'col2', 'col3']) 
 df2 = df.withColumn('col3', explode(df.col3)) 
 df = spark.createDataFrame([{'id': 123, 'dt': 0}, {'id': 456, 'dt':0}, {'id':456, 'dt':2}]) 
 w = Window.partitionBy('id').orderBy('dt') 
df2 = df.select('id','dt', count('dt').over(w).alias('count')) 
 df = spark.createDataFrame([{'id': 1, 'name': 'jonathan'}, {'id': 2, 'name':'jonathan1'}, {'id':3, 'name':'jonathan2'}]) 
 df2 = df.withColumn('category', udfScoreToCategory(df['score'])) 
 df_group = df.groupby(“passengerID”)\
 .agg(F.sum(F.when(F.col(“hour”).isin(types_manana1),1).otherwise(0)).alias(“manana1”), 
 F.sum(F.when(F.col(“hour”).isin(types_manana2),1).otherwise(0)).alias(“manana2”),
 F.sum(F.when(F.col(“hour”).isin(types_tarde),1).otherwise(0)).alias(“tarde”),
 F.sum(F.when(F.col(“hour”).isin(types_noche1),1).otherwise(0)).alias(“noche1”),
 F.sum(F.when(F.col(“hour”).isin(types_noche2),1).otherwise(0)).alias(“noche2”), 
 F.sum(F.when(F.col(“hour”).isin(types_juerga),1).otherwise(0)).alias(“juerga”),
 F.sum(F.when(~F.col(“weekday”).isin(type_weekday),1).otherwise(0)).alias(“semana”),
 F.sum(F.when(F.col(“weekday”).isin(type_weekday),1).otherwise(0)).alias(“fin_de_semana”),
 F.sum(“payment_value”).alias(“monto_total”),
 F.avg(“taximeter_distance”).alias(“distance”),
 F.round(F.datediff(F.max(“requested_at”), F.min(“requested_at”))/F.count(“*”),2).alias(“tiempo_entre_viajes”)
 )
 df_group2 = df_group.withColumn(“v_total”, F.col(“manana1”)+ F.col(“manana2”)+ F.col(“tarde”)+ F.col(“noche1”)+ F.col(“noche2”)+ F.col(“juerga”))
 df_group2 = df_group2.withColumn(“v_total_semana”, F.col(“semana”)+ F.col(“fin_de_semana”))
 df_group3 = df_group2.withColumn(“v_manana1”, df_group2[“manana1”])
 df_group3 = df_group3.withColumn(“v_manana2”, df_group2[“manana2”])
 df_group3 = df_group3.withColumn(“v_tarde”, df_group2[“tarde”])
 df_group3 = df_group3.withColumn(“v_noche1”, df_group2[“noche1”])
 df_group3 = df_group3.withColumn(“v_noche2”, df_group2[“noche2”])
 df_group3 = df_group3.withColumn(“v_juerga”, df_group2[“juerga”])
 df_group3 = df_group3.withColumn(“v_semana”, df_group2[“semana”])
 df_group3 = df_group3.withColumn(“v_fin_de_semana”, df_group2[“fin_de_semana”])
 df_group3 = df_group3.withColumn(“v_avg_distance”, F.round(df_group2[“distance”],2))
 df_group3 = df_group3.withColumn(“v_avg_payment”, F.round(df_group2[“monto_total”],2)) 
 
 
Item 2806: 
d43 = d42.append(d41).groupby(‘Tam’).get_group(‘Liverpool’)
display(d43)
 
 
Item 2809: 
“duct”      -->     [ 1 0 0 0 0 0 0 0 0 0 ]
 “tape”      -->     [ 0 1 0 0 0 0 0 0 0 0 ]
 …
 “magic”     -->     [ 0 0 0 0 0 1 0 0 0 0 ]
 …
 “worshiped” -->     [ 0 0 0 0 0 0 0 0 0 1 ]
 def one_hot_encode_my_word(my_word, the_vocabulary):
    one_hot_encoded_vector = []
    for word in the_vocabulary:
        if word == my_word:
            one_hot_encoded_vector.append(1)
        else:
            one_hot_encoded_vector.append(0)
 v = ["duct", "tape", "works", "anywhere", "is", "magic", "and", "should", "be", "worshiped"]
 one_hot_encode_my_word("tape", v)
 >>> [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
 
 
Item 2814: 
dataset_file = h5py.File(“data” + os.sep + “filename.hdf5”, “r”)
 train_dataset, train_labels = dataset_file[‘/train/images’], dataset_file[‘/train/labels’]
 test_dataset, test_labels = dataset_file[‘/test/images’], dataset_file[‘/test/labels’]
 
 
Item 2822: 
cd pubg-match-deaths && ls -la
 unzip aggregate.zip && unzip deaths.zip
 head -n 5 agg_match_stats_0.csv
 head -n 1 agg_match_stats_0.csv | tr ‘\,’ ‘\n’
 head -n 1 agg_match_stats_0.csv | tr ‘\,’ ‘\n’ | wc -l
 head -n 1 agg*.csv
 wc -l agg*.csv
 13849288 agg_match_stats_0.csv
 13844276 agg_match_stats_1.csv
 13841505 agg_match_stats_2.csv
 13840681 agg_match_stats_3.csv
 11993486 agg_match_stats_4.csv
 67369236 total
 cat agg_match_stats_0.csv >> aggregate_match_stats_final.csv
 cat agg_match_stats_1.csv > aggregate_match_stats_final.csv
 cat agg_match_stats_1.csv >> aggregate_match_stats_final.csv
 rm aggregate_match_stats_final.csv
 cat agg*.csv >> aggregate_match_stats_final.csv
 wc -l aggregate_match_stats_final.csv
 67369236 aggregate_match_stats_final.csv
 head -n 1 agg_match_stats_0.csv > aggregate_match_stats_final.csv && tail -n +2 -q agg_match_stats_*.csv >> aggregate_match_stats_final.csv
 wc -l aggregate_match_stats_final.csv
 67369233 aggregate_match_stats_final.csv
 head -n 1 kill_match_stats_final_0.csv > kills_match_stats_final.csv && tail -n +2 -q kill_match_stats_final_*.csv >> kills_match_stats_final.csv
 cut -d, -f 1 aggregate_match_stats_final.csv | sort -nr | head -1
 2018–01–10T12:02:16+0000
 cut -d, -f 1 aggregate_match_stats_final.csv | sort | head -1
 cut -d, -f11,12 aggregate_match_stats_final.csv | grep 62,
 62,gogolnyg
 62,KJ-Qun_326373092
 62,l56624
 
 
Item 2827: 
[(1, 'a') (1, 'b') (1, 'c') (2, 'a') (2, 'b') (2, 'c') (3, 'a')
(3, 'b') (3, 'c')]
    customer order_date  amount
0         1 2018-01-03      25
1         2 2018-01-05      42
2         3 2018-01-07     116
3         2 2018-01-08      21
4         3 2018-01-09      83
5         1 2018-01-10       4
6         1 2018-01-10      67
 customer order_date   amount     
1        2018-01-03      25
         2018-01-10      71
2        2018-01-05      42
         2018-01-08      21
3        2018-01-07     116
         2018-01-09      83
 [(1, Timestamp('2018-01-01 00:00:00', freq='D'))
 (1, Timestamp('2018-01-02 00:00:00', freq='D'))
 (1, Timestamp('2018-01-03 00:00:00', freq='D'))
 (1, Timestamp('2018-01-04 00:00:00', freq='D'))
 (1, Timestamp('2018-01-05 00:00:00', freq='D'))]
 amount  running_amount
customer order_date                        
1        2018-01-01       0               0
         2018-01-02       0               0
         2018-01-03      25              25
         2018-01-04       0              25
         2018-01-05       0              25
         2018-01-06       0              25
         2018-01-07       0              25
         2018-01-08       0              25
         2018-01-09       0              25
         2018-01-10      71              96
2        2018-01-01       0               0
         2018-01-02       0               0
         2018-01-03       0               0
         2018-01-04       0               0
         2018-01-05      42              42
         2018-01-06       0              42
         2018-01-07       0              42
         2018-01-08      21              63
         2018-01-09       0              63
         2018-01-10       0              63
3        2018-01-01       0               0
         2018-01-02       0               0
         2018-01-03       0               0
         2018-01-04       0               0
         2018-01-05       0               0
         2018-01-06       0               0
         2018-01-07     116             116
         2018-01-08       0             116
         2018-01-09      83             199
         2018-01-10       0             199
 
 
Item 2828: 
“runmenu:restart-and-run-all”: {
 “command”: “runmenu:restart-and-run-all”,
 “keys”: [
 “”
 ],
 “selector”: “[data-jp-code-runner]”,
 “title”: “Restart Kernel and Run All”,
 “category”: “Run Menu”
 },
 {
 “runmenu:restart-and-run-all”: {
 “command”: “runmenu:restart-and-run-all”,
 “keys”: [
 “Shift A Z”
 ],
 “selector”: “[data-jp-code-runner]”,
 “title”: “Restart Kernel and Run All”,
 “category”: “Run Menu”
 }
}
 "notebook:run-all-below": {
 "command": "notebook:run-all-below",
 "keys": [
 "Shift D"
 ],
 "selector": ".jp-Notebook:focus",
 "title": "Run All Below",
 "category": "Notebook Cell Operations"
 }
 
 
Item 2830: 
<eligibility>
  <criteria>
    <textblock> Inclusion Criteria: - Patients with a clinical indication for EVAR/FEVAR of AAA and meeting anatomic inclusion criteria on preoperative enhanced CT-scan compatible with an endovascular repair...
    </textblock>
  </criteria>
  <gender>All</gender>
  <minimum_age>N/A</minimum_age>
  <maximum_age>N/A</maximum_age
  <healthy_volunteers>No</healthy_volunteers>
</eligibility>
 
 
Item 2831: 
devtools::use_testthat()
 usethis::edit_r_environ("project")
 testthat::test_that("mpg_by_cyl has expected number of rows", {
    
    mpg_by_cyl %>%
      nrow() %>% 
      testthat::expect_equal(3)
 })
 testthat::test_that("date contains a recent date", {
    
  testthat::expect_true(grepl(" 20", date))
 })
 
 
Item 2832: 
[(n+2p-f/s)+1] *[(n+2p-f/s)+1]
 n -> size of the image
p -> padding
s -> stride
 
 
Item 2838: 
Structural + Visual + Behavioral = Optimization Outcome
 Structural + Visual + Behavioral = Optimization Outcome
 
 
Item 2839: 
learner.models.model[0].encoder.weight = nn.Parameter(T(vectors))
learner.models.model[1].decoder.weight = (
    learner.models.model[0].encoder.weight
)
 lrs = 1e-4
learner.fit(lrs, 1, wds=1e-7, use_clr=(32, 5),
            cycle_len=6, use_wd_sched=True)
 self.transform_pad = transforms.Compose([
    ResizeAndPad(299),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
    ])
 self.transform_center = transforms.Compose([
    transforms.Resize(299),
    transforms.CenterCrop(299),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225])
    ])
 self.region_emb = nn.Embedding(28, 3)
self.city_emb = nn.Embedding(290, 5)
self.p_cate_emb = nn.Embedding(9, 3)
self.cate_emb = nn.Embedding(47, 5)
self.image_top1_emb = nn.Embedding(888, 5)
self.user_type_emb = nn.Embedding(3, 2)
self.weekday_emb = nn.Embedding(7, 3)
self.param1_emb = nn.Embedding(204, 5)
self.param2_emb = nn.Embedding(131, 3)
self.param3_emb = nn.Embedding(113, 3)
 
 
Item 2844: 
>>> go run dblp.go "chi"
2017/09/12 20:01:34 Reading in DBLP gzipped file
2017/09/12 20:01:34 Decoding all proceedings from file
2017/09/12 20:03:23 Done decoding
2017/09/12 20:03:23 Number of CHI titles: 13885 Average title length: 67.01498019445445
2017/09/12 20:03:23 Output data to chi.txt
 python scripts/preprocess.py \
--input_txt data/chi.txt \
--output_h5 data/chi.h5 \
--output_json data/chi.json
 th train.lua -input_h5 data/chi.h5 -input_json data/chi.json -seq_length 60 -batch_size 1 -gpu -1
 th sample.lua -checkpoint cv/checkpoint_581150.t7 -length 2000 -gpu -1 -temperature 0.1 > data/chi_sample.txt
 
 
Item 2846: 
docker build -t container_name .
 docker build -t mydatascience_env .
 docker run -p 8887:8888 -v /path_your_machine/notebook_folder/:/Documents -it datascience_env
 http://127.0.0.1:8007
 docker  stop datascience_env
 
 
Item 2850: 
curl -X POST -H “Content-Type: application/json” -d ‘{
    “recipient”:{
        “id”:”<PSID>”
    },
    “message”:{
        “text”: “Which subjects can you help others with?”,
        “quick_replies”:[
            {
                “content_type”:”text”,
                “title”:”Math”,
                “payload”:”<POSTBACK_PAYLOAD>”,
            },
            {
                “content_type”:”text”
                “title”:”History”;
                “payload”:”<POSTBACK_PAYLOAD>”,
            },
            {
                “content_type”:”text”,
                “title”:”Science”,
                “payload”:”<POSTBACK_PAYLOAD>”
            }
        ]
   }
}’ “https://graph.facebook.com/v2.6/me/messages?access_token=<PAGE_ACCESS_TOKEN>"
 
 
Item 2851: 
for i in range(nb_interacao):
  params_grad = evaluate_gradient(fun_custo, data, params)
  params = params - params_grad * learning_rate
 for i in range(nb_interacao):
   np.random.shuffle(data)
   for example in data:
     params_grad = evaluate_gradient(fun_custo, example, params)
     params = params - params_grad * learning_rate
 for i in range(nb_interacao):
   np.random.shuffle(data)
   for batch in get_batches(data, batch_size=50):
     params_grad = evaluate_gradient(fun_custo, batch, params)
     params = params - params_grad * learning_rate
 
 
Item 2852: 
feature, label = make_classification(n_samples=200, n_features=10, n_redundant=0, n_informative=5, random_state=0)
 
 
Item 2853: 
struct node {
  int val;
  node* left;
  node* right;
  node(int v_) : val(v_), left(NULL), right(NULL) {};
};
 class BinaryTree {
  private:
    struct node {
      ...
    };
    node* root;
    void print(node* temp) {
      ...
    } 
  public:
    void addNode(int v_) {
      ...
    }
    int findNodeCount(int v_) {
      ...
    }
    void print() {
      ...
    } 
    BinaryTree() { ... }
};
 void addNode(int v_) {
      if(!root)
        root = new node(v_);
      else {
        node* walker = root;
        while(walker) {
          if(v_ < walker->val) {
            if(walker->left)
              walker = walker->left;
            else {
              walker->left = new node(v_);
              walker = NULL;
            }
          }
          else {
            if(walker->right)
              walker = walker->right;
            else {
              walker->right = new node(v_);
              walker = NULL;
            }
          }  
        }
      }
}
 int findNodeCount(int v_) {
      int count = 0;
      node* walker = root;
      while(walker) {
        if(v_ == walker->val)
          return count;
          walker = (v_ < walker->val) ? walker->left : walker->right;
        count++;
      }
      return -1;
    }
 void print() {
      cout<<"Printing binary tree:"<<endl<<"[ ";
      print(root);
      cout<<"]"<<endl;
    }
 void print(node* temp) {
      if(temp->left)
        print(temp->left);
      cout<<temp->val<<" ";
      if(temp->right)
        print(temp->right);
    }
 BinaryTree() { root = NULL; }
 #include <iostream>
#include <stdlib.h>
#include <time.h>
 using namespace std;
 class BinaryTree {
  private:
    struct node {
      int val;
      node* left;
      node* right;
      node(int v_) : val(v_), left(NULL), right(NULL) {};
    };
    node* root;
    void print(node* temp) {
      if(temp->left)
        print(temp->left);
      cout<<temp->val<<" ";
      if(temp->right)
        print(temp->right);
    }
  public:
    void addNode(int v_) {
      if(!root)
        root = new node(v_);
      else {
        node* walker = root;
        while(walker) {
          if(v_ < walker->val) {
            if(walker->left)
              walker = walker->left;
            else {
              walker->left = new node(v_);
              walker = NULL;
            }
          }
          else {
            if(walker->right)
              walker = walker->right;
            else {
              walker->right = new node(v_);
              walker = NULL;
            }
          }
        }
      }
    }
    int findNodeCount(int v_) {
      int count = 0;
      node* walker = root;
      while(walker) {
        if(v_ == walker->val)
          return count;
          walker = (v_ < walker->val) ? walker->left : walker->right;
        count++;
      }
      return -1;
    }
    void print() {
      cout<<"Printing binary tree:"<<endl<<"[ ";
      print(root);
      cout<<"]"<<endl;
    }
    BinaryTree() { root = NULL; }
};
 int main() {
  srand(time(NULL));
  int randomNumberToFind;
  int randomPlacement = rand()%100000;
  BinaryTree bt;
  for(int i=0;i<100000;i++) {
    int randomNumber = 202*10000000+(rand()%900 + 100)*1000+rand()%9000+1000;
    if(i == randomPlacement)
      randomNumberToFind = randomNumber;
    bt.addNode(randomNumber);
  }
  bt.print();
  cout<<endl<<"Looking for "<<randomNumberToFind<<"..."<<endl;
  cout<<"Found after "<<bt.findNodeCount(randomNumberToFind)<<" traversals."<<endl;
  return 0;
}
 
 
Item 2861: 
df.dropna(axis=0, how=’any’, thresh=None, subset=None, inplace=False)
 
 
Item 2863: 
Install-Package MathAnalytics
 
 
Item 2869: 
Medication annotations:  9318
Dosage annotations:  4666
Mode annotations:  3513
Frequency annotations:  4229
Duration annotations:  571
Reason annotations:  1694
 
 
Item 2880: 
print( "hello, world!" )
 public class Main {
 public static void main(String[] args) {
 System.out.println(“hello, world!”);
 }
 }
 
 
Item 2881: 
https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query/user?apiKey=API_KEY
 {{ first name }}, {{ last name }}, {{ gender }}, {{ locale }}, {{ profile pic url }}, {{ timezone }}
 https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query?apiKey=API_KEY
 n: the number of recommendations to present between 1 and 20, default 5
label: the label of the action button within the recommendation widget (must be URL encoded), default 'Check it out'
widget: the type of Facebook widget displayed to the user ('carousel', 'list', 'card'), default 'carousel'
Note: the Facebook 'list' widget displays a maximum of 4 recommendations and the Facebook 'card' widget displays a maximum of 1 recommendations, regardless of n
 https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query?apiKey=API_KEY&n=3&label=Click+Here&widget=list
 https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query?apiKey=API_KEY
 {{ last user freeform input }}
 recommend: a true or false variable to optionally return recommendations in the response, default false
 https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query?apiKey=API_KEY&recommend=true
 n: the number of recommendations to present between 1 and 20, default 5
label: the label of the action button within the recommendation widget (must be URL encoded), default 'Check it out'
widget: the type of Facebook widget displayed to the user ('carousel', 'list', 'card'), default 'carousel'
Note: the Facebook 'list' widget displays a maximum of 4 recommendations and the Facebook 'card' widget displays a maximum of 1 recommendations, regardless of n
 https://api.cashbot.ai/recommend/{{ chatfuel user id }}/query?apiKey=API_KEY&recommend=true&n=3&label=Click+Here&widget=list
 {{ last clicked button name }}
 GetRecommendations: Get recommendations for user
PostUser: Update user information
PostInputText: Submit user text input (no response)
PostInputButton: Submit user button click (no response)
PostTextGetRecs: Submit user text input, get recommendations
PostButtonGetRecs: Submit user button click, get recommendations
 
 
Item 2888: 
df.shape
(457728, 93)
 df.iloc[0:5, 0: 10]
 ### Feature: loan_amnt
-------------------------------
 len(df.loan_amnt.value_counts())
1415
 plot_feature('loan_amnt', True)
 ### Feature: term
-------------------------------
 df.term = df.term.str.replace('months', '').astype(np.int)
df.term.value_counts()
 36    341042
60    116686
Name: term, dtype: int64
 plot_feature('term', False)
 ### Feature: int_rate
-------------------------------
 df.int_rate = df.int_rate.str.replace('%', '').astype(np.float32)
len(df.int_rate.value_counts())
200
plot_feature('int_rate', True)
 ### Feature: emp_title
-------------------------------
len(df.emp_title.value_counts())
128310
 It looks like emp_title has lots of unique value, which may not strongly be associated with predicted loan_status. Therefore, I delete the feature.
 features_to_be_removed.extend(['emp_title', 'id'])
 df.iloc[0:5, 6: 20]

 ### Feature: issue_d
### ------------------
 df.issue_d.value_counts()
Oct-2014    33699
Jul-2014    25974
Jul-2015    23990
Jan-2015    22750
Oct-2015    22431
Nov-2014    21186
Apr-2015    20097
Dec-2015    19106
May-2015    17964
Aug-2015    17492
Apr-2014    17140
May-2014    17133
Aug-2014    16780
Nov-2015    16540
Jun-2015    15304
Jun-2014    15207
Mar-2015    14997
Mar-2014    14988
Feb-2015    14464
Jan-2014    14464
Feb-2014    13945
Sep-2015    13838
Sep-2014     9344
Dec-2014     8283
Jan-2017     5912
Mar-2017     5008
Feb-2017     4241
May-2017     3644
Apr-2017     3431
Jun-2017     2849
Jul-2017     2366
Aug-2017     1863
Sep-2017     1298
Name: issue_d, dtype: int64
 df['issue_month'] = pd.Series(df.issue_d).str.replace(r'-\d+', '')
plot_feature('issue_month', False)
 df.issue_month = df.issue_month.astype("category", categories=np.unique(df.issue_month)).cat.codes
 df.issue_month.value_counts()
10    56130
5     52330
4     43126
0     40668
8     38741
9     37726
1     36135
7     34993
6     33360
3     32650
2     27389
11    24480
Name: issue_month, dtype: int64
 df['issue_year'] = pd.Series(df.issue_d).str.replace(r'\w+-', '').astype(np.int)
 df.issue_year.value_counts()
2015    218973
2014    208143
2017     30612
Name: issue_year, dtype: int64
 df.iloc[0:5, 25: 40]
 df.iloc[0:5, 35: 50]
 df.iloc[0:5, 85:]
 len(set(features_to_be_removed))
61
 print(features_to_be_removed)
 ['emp_title', 'id', 'url', 'title', 'zip_code', 'issue_d', 'mths_since_last_delinq', 'mths_since_last_record', 'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 'last_credit_pull_d', 'last_fico_range_high', 'last_fico_range_low', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'chargeoff_within_12_mths', 'delinq_amnt', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq', 'mths_since_recent_inq', 'mths_since_recent_revol_delinq', 'num_accts_ever_120_pd', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'debt_settlement_flag', 'total_il_high_credit_limit']
 df_selected = df.drop(list(set(features_to_be_removed)), axis = 1)
df_selected.shape
 (457728, 36)
 df_selected.to_csv('./data/df_selected.csv', index = False)
 Email: sabbers@gmail.com
LinkedIn: https://www.linkedin.com/in/sabber-ahamed/
Github: https://github.com/msahamed
Medium: https://medium.com/@sabber/
 
 
Item 2902: 
mkdir ~/my_flights ("my_flights" as project name)
cd ~/my_flights
 mkdir %HOMEPATH%my_flights ("my_flights" as project name)
cd %HOMEPATH%my_flights
 /*
* HTTP Cloud Function.
*
* @param {Object} req Cloud Function request context.
* @param {Object} res Cloud Function response context.
*/
exports.helloHttp = function helloHttp (req, res) {
  response = "This is a sample response from your webhook!" //Default response from the webhook to show it's working


  res.setHeader('Content-Type', 'application/json'); //Requires application/json MIME type
  res.send(JSON.stringify({ "speech": response, "displayText": response 
  //"speech" is the spoken version of the response, "displayText" is the visual version
  }));
};
 
 
Item 2904: 
%%bq query -n queary1
SELECT
  department,
  COUNT(*) AS `frequency_count`
FROM
  `newproject-xxxx.instacart.mrg_fin`
GROUP BY
  department
ORDER BY `frequency_count` DESC
LIMIT 50
 %%bq sample --count 5 --query queary1
 df = queary1.execute(output_options=bq.QueryOutput.dataframe()).result()
 
 
Item 2905: 
for t = 1 to T do     
    Forward pass for the forward hidden layer, storing activations at each 
    timestep
for t = T to 1 do
    Forward pass for the backward hidden layer, storing activations at each 
    timestep
for all t, in any order do
    Forward pass for the output layer, using the stored activations from both
    hidden layers
 for all t, in any order do     
    Backward pass for the output layer, storing terms at each 
    timestep
for t = T to 1 do
    BPTT backward pass for the forward hidden layer, using the stored terms from
    the output layer
for t = 1 to T do
    BPTT backward pass for the backward hidden layer, using the stored terms from
    the output layer
 
 
Item 2913: 
PANDARUS:
Alas, I think he shall be come approached and the day
When little srain would be attain’d into being never fed,
And who is but a chain and subjects of his death,
I should not sleep.
 Second Senator:
They are away this miseries, produced upon my soul,
Breaking and strongly should be buried, when I perish
The earth and thoughts of many states.
 Naturalism and decision for the majority of Arab countries’ capitalide was grounded
by the Irish language by [[John Clair]], [[An Imperial Japanese Revolt]], associated
with Guangzham’s sovereignty. His generals were the powerful ruler of the Portugal
in the [[Protestant Immineners]], which could be said to be directly in Cantonese
Communication, which followed a ceremony and set inspired prison, training. The
emperor travelled back to [[Antioch, Perth, October 25|21]] to note, the Kingdom
of Costa Rica, unsuccessful fashioned the [[Thrales]], [[Cynth’s Dajoard]], known
in western [[Scotland]], near Italy to the conquest of India with the conflict.
 <revision>
 <id>15900676</id>
 <timestamp>2002–08–03T18:14:12Z</timestamp>
 <contributor>
 <username>Paris</username>
 <id>23</id>
 </contributor>
 <minor />
 <comment>Automated conversion</comment>
 </revision>
 SEED: War on terror
Good everybody. Thank you very much. God bless the United States of America, and has already began with the world’s gathering their health insurance.
It’s about hard-earned for our efforts that are not continued.
We are all the assumptionion to the streets of the Americas that we are still for everybody and destruction.
We are doing a lot of this.
I know that someone would be prefered to their children to take a million insurance company. We’re watching their people and continued to find ourselves with Republicans — to give up on these challenges and despite the challenges of our country. In the last two years, we must recognise that our borders have access from the world. We’re continuing that this day of things that the United States will clean up it´s allies and prosperity to stand up enough to be a sanctions that we made their faith, and for the country of the Internet to Osama bin Laden.
Thank you. God bless you. Good morning, everybody. And May God loss man. Thank you very much. Thank you very much, everybody.
 
 
Item 2914: 
"RobotInfo": {
 "RobotConfiguration": {
  "LegCount": 2,
  "ArmCount": 2,
  "IsMobile": true,
  "Sensors": {
   "HasCamera": true,
   "HasVideoCamera": true
  }
 }
}
 {
    "RequestInfo" :
    {
        ...
        "RobotInfo" :
        {
            "RobotConfiguration" :
            {
                ...
            },
            "RobotState" :
            {
                "KnownRooms": [
                    {
                        "Name" : "Living Room"
                    },
                    {
                        "Name" : "Kitchen"
                    }
                ]
            }
        }
    }
}
 "AllResults": [
    {
      "CommandKind": "RobotCommand",
      ...
       "ClientActionSucceededResult": {
        "SpokenResponse": "Moving ten meters towards the front.",
        "SpokenResponseLong": "Moving ten meters towards the front.",
        "WrittenResponse": "Moving 10 meters towards the front.",
        "WrittenResponseLong": "Moving 10 meters towards the front."
      },
      "ClientActionFailedResult": {
        "SpokenResponse": "Could not move.",
        "SpokenResponseLong": "Could not move.",
        "WrittenResponse": "Could not move.",
        "WrittenResponseLong": "Could not move."
      },
      "RobotCommandKind": "MoveCommand",
      "Direction": 0,
      "Distance": 10,
      "DistanceUnit": "meter"
    }
  ],
 
 
Item 2915: 
ax = satDf[['Verbal', 'Math']].plot(kind='bar', stacked=True, figsize=(15,7))

ax.set_ylabel("Score")
ax.set_xlabel("States")

ax.set_title("Verbal and Maths per States", fontsize=20)

 sns.pairplot(satDf[['Verbal', 'Math', 'Verbal_Math']])
 
 
Item 2921: 
We use labeled examples to train the model. Once we do that, we can predict a label from an unlabeled example.
 
 
Item 2922: 
The fragile border between "repaired humanity" and "augmented humanity"
The researchers tested epilepsy patients already equipped with brain implants to treat drug-resistant seizures. The intracerebral device has been reoriented to the areas involved in the storage. By delivering targeted electrical impulses, scientists have managed to increase patient memory by 30%.

Dr. Song speaks of a true "prosthesis of memory" and the essay opens perspectives in the treatment of disorders such as Alzheimer's. The experiment could allow many neuronal manipulations. These scientific transgressions will lead to violent - and legitimate - oppositions between bioconservatives and candidates for the benefits of advances in science.

From the "repaired" man to the "augmented" man, there is only one step, which will inevitably be crossed. Neuroethics, which is just beginning to become structured, will become a major topic for politicians in the 21st century. And one sees in passing that the distinction "repaired humanity" and "increased humanity" is very fragile.

The medical elite ready to follow the transhumanists
Will we legislate to prohibit epileptics with implants benefiting from cognitive abilities superior to the rest of the population? The answer is obvious: we will cross the Rubicon separating the repair of the increase without any qualms. Should we oblige the holders of cerebral implants to declare them before a competition? Should we decrease their notes to make the competition equal? Should we block the implants during the month preceding the exams, with the risk of seeing the epileptic students having a seizure during their exams?
 "Neuroethics, which is just beginning to become structured, will become a major topic for politicians in the 21st century."
 
 
Item 2923: 
> tensorboard --logdir={model_dir}
 
 
Item 2924: 
name: "Cloud Natural Language"
salience: 0.351824939251
sentiment {}
 name: "service"
salience: 0.1057305336
sentiment { magnitude: 0.20000000298, score: 0.20000000298 }
 name: "machine learning technology"
salience: 0.089977234602
sentiment {}
 name: "anyone"
salience: 0.0196914002299
sentiment {}
 
 
Item 2925: 
config = tf.ConfigProto() 
config.gpu_options.per_process_gpu_memory_fraction = 0.3 
set_session(tf.Session(config=config)) 
self.sess = K.get_session()
 
 
Item 2927: 
dataset = tf.data.Dataset.from_tensor_slices((image_list, label_list))
 def _read_py_function(path, label):
    image = read_image(path)
    label = np.array(label, dtype=np.uint8)
    return image.astype(np.int32), label
 def _resize_function(image_decoded, label):
    image_decoded.set_shape([None, None, None])
    image_resized = tf.image.resize_images(image_decoded, [28, 28])
    return image_resized, label
 dataset = dataset.map(
    lambda data_list, label_list: tuple(tf.py_func(_read_py_function, [data_list, label_list], [tf.int32, tf.uint8])))
 dataset = dataset.map(_resize_function)
 dataset = dataset.repeat()
dataset = dataset.shuffle(buffer_size=(int(len(data_list) * 0.4) + 3 * batch_size))
 dataset = dataset.batch(batch_size)
 iterator = dataset.make_initializable_iterator()
image_stacked, label_stacked = iterator.get_next()
 with tf.Session() as sess:
    sess.run(iterator.initializer)
    image, label = sess.run([image_stacked, label_stacked])
 
 
Item 2928: 
2018-04-23	00:54:55	out	741741	Crisis Text Line	Seriously considering going to ex girlfriend's house and calling cops and running towards them to with knives to get them to kill me
2018-04-23	00:55:00	in	741741	Crisis Text Line	Hi, this is Crisis Text Line. You're not alone. Our terms of service: goo.gl/EMCKCY. We can't receive msgs >160 characters.
2018-04-23	00:55:01	in	741741	Crisis Text Line	Resources and coping skills while we connect you: bit.ly/2BHZKBg. (STOP to cancel convo). What's your crisis?
2018-04-23	00:55:16	out	741741	Crisis Text Line	Seriously considering going to ex girlfriend's house and calling cops and running towards them to with knives to get them to kill me
2018-04-23	00:55:59	in	741741	Crisis Text Line	Hi, my name is Caroline.  Thanks for texting in tonight, I am here to listen.  It sounds like you are having a tough night and I am here for you.
2018-04-23	00:57:12	out	741741	Crisis Text Line	Me and my parents saved ex twice from homelessness and she decides to kick me out and my work is not going well at all and I just want to end it and make her suffer
2018-04-23	00:58:23	in	741741	Crisis Text Line	I am so sorry that you are feeling so hopeless and angry at your Ex.  I am really glad you texted in tonight for help.
2018-04-23	01:02:59	out	741741	Crisis Text Line	I hate getting rejected and it happened in same day first capital one Cafe inviting someone to vip invite right smack in front of me after telling me I couldn't go and girlfriend kicking me out because I was mad that she highly frequently texted me while I was attempting to work
2018-04-23	01:03:01	in	741741	Crisis Text Line	Just checking in to see if you are still there?
2018-04-23	01:04:13	out	741741	Crisis Text Line	I am ive attempted suicide twice both times hospitalized and I just want to get the guts to go through with it for good
2018-04-23	01:04:45	in	741741	Crisis Text Line	It sounds like you have had some recent things trigger these feelings, rejection is tough and makes us feel badly.  I am impressed you texted in today for help.
2018-04-23	01:05:03	in	741741	Crisis Text Line	It sounds like a part of you wants to die, but a part of you wants to live.
2018-04-23	01:05:53	out	741741	Crisis Text Line	I have very low pain tolerance which is why sucide by cop is only method I'd try at this point
2018-04-23	01:08:05	in	741741	Crisis Text Line	Will you share your name with me?
2018-04-23	01:08:37	out	741741	Crisis Text Line	No because I don't want cops called one if I don't go through with it
2018-04-23	01:09:47	in	741741	Crisis Text Line	No worries, it just makes our conversation more personal.  You can even make one up if you want :)
2018-04-23	01:09:56	out	741741	Crisis Text Line	Researching taser don't have airsoft bought it once online and cancelled it for taser trying to defeat it so have to kill
2018-04-23	01:10:19	out	741741	Crisis Text Line	Idk call me Dexter it's what I nedy robot for some app I downloaded
2018-04-23	01:11:36	in	741741	Crisis Text Line	Dexter it is...Dexter, are you in school or do you work?  Tell me more about your life if you are comfortable.
2018-04-23	01:11:37	out	741741	Crisis Text Line	If my work which I'm not paid for yet was going better I'd be hopeful but struggling with the programming
2018-04-23	01:12:17	out	741741	Crisis Text Line	I haven't had full time job since 2015 when I worked for Seattle police
2018-04-23	01:12:51	out	741741	Crisis Text Line	Not in school selftaught
2018-04-23	01:14:55	in	741741	Crisis Text Line	It sounds like you work but it is volunteer?  And school was at home, do you have any family?
2018-04-23	01:15:55	out	741741	Crisis Text Line	Parents r alive but can't have in person contact with them without girlfriend
2018-04-23	01:16:43	out	741741	Crisis Text Line	I'm starting my own business another company gave me check to pay for paperwork and accountant and have a contract lined up for me
2018-04-23	01:17:57	in	741741	Crisis Text Line	That is really great that you have started your own company and shows a lot of motivation and intelligence.
2018-04-23	01:18:00	out	741741	Crisis Text Line	I'm building something called https://911callbot.com based on work experience at Seattle police hoping it would make me extremely well known
2018-04-23	01:19:29	out	741741	Crisis Text Line	I feel very isolated and am trying to become the one of the most important people in us and this freaking computer software I'm stuck and my support questions haven't been answered
2018-04-23	01:19:46	in	741741	Crisis Text Line	I can't look anything up online, but that is great.  You should be proud of yourself.  What do you think might help you start to move forward and feel better?
2018-04-23	01:20:28	out	741741	Crisis Text Line	I took bath while talking to u because baths and showers make me calmer
2018-04-23	01:21:08	out	741741	Crisis Text Line	Ive taken walks and watched videos online
2018-04-23	01:21:15	in	741741	Crisis Text Line	Good for you, anything that relaxes you and your mind.  That is key, and focusing on some goals that are achievable might help.
2018-04-23	01:23:30	in	741741	Crisis Text Line	Walking is a great way as well.  Be good to yourself.
2018-04-23	01:23:39	in	741741	Crisis Text Line	You deserve happiness.
2018-04-23	01:24:30	out	741741	Crisis Text Line	I think I'm gonna do it just so much hurt in my life
2018-04-23	01:25:15	in	741741	Crisis Text Line	It sounds like you have been through a lot, but you have also done a good job reaching out for help tonight. Are you open to trying to get some help tomorrow?
2018-04-23	01:25:31	in	741741	Crisis Text Line	I have resources that can help.
2018-04-23	01:26:02	out	741741	Crisis Text Line	I'm just in mode of putting on as many sweaters to stop taser from working
2018-04-23	01:26:29	in	741741	Crisis Text Line	What is the taser for, to hurt yourself?
2018-04-23	01:26:50	out	741741	Crisis Text Line	Cops might try taser first
2018-04-23	01:27:28	in	741741	Crisis Text Line	Why don't we try to come up with a different plan than putting yourself in front of the cops, can you do that with me?
2018-04-23	01:27:32	in	741741	Crisis Text Line	Make a plan?
2018-04-23	01:27:54	out	741741	Crisis Text Line	I'm just sick of ups and downs of life
2018-04-23	01:29:06	in	741741	Crisis Text Line	It can be exhausting managing ups and downs, and sometimes hard to find the energy to keep going.  However, if you can find it you may start going forward...
2018-04-23	01:31:23	in	741741	Crisis Text Line	May I ask roughly how old you are?  I am trying to find a few sites that might help to give you some support online (for free).
2018-04-23	01:31:42	out	741741	Crisis Text Line	I've been reading all about people like me with severe problems from gift of fear
2018-04-23	01:32:13	out	741741	Crisis Text Line	Matched me to a tee hates rejection which I why I blow up so much
2018-04-23	01:34:03	in	741741	Crisis Text Line	Rejection is really hard, what do you think might help you manage it since we all feel it from time to time...
2018-04-23	01:34:57	out	741741	Crisis Text Line	Idk grabbing kitchen knives and more jackets and leaving
2018-04-23	01:35:52	out	741741	Crisis Text Line	Don't want to do it but sick of life
2018-04-23	01:36:19	in	741741	Crisis Text Line	I understand, are you willing to get some help?
2018-04-23	01:36:30	out	741741	Crisis Text Line	Idk
2018-04-23	01:36:47	out	741741	Crisis Text Line	I have meds and caseanager
2018-04-23	01:37:34	in	741741	Crisis Text Line	Meds?
2018-04-23	01:38:22	out	741741	Crisis Text Line	I am perscribed meds for my mental problems and have doctor and case manager
2018-04-23	01:39:04	in	741741	Crisis Text Line	That's good,  have you opened up to your Dr. about how low you feel?
2018-04-23	01:39:14	in	741741	Crisis Text Line	I am wondering when you see him next...
2018-04-23	01:39:19	in	741741	Crisis Text Line	or her :)
2018-04-23	01:40:23	out	741741	Crisis Text Line	I emailed her morning o was kicked out as I had appointment scheduled with her
2018-04-23	01:41:14	out	741741	Crisis Text Line	Good news is can't find keys
2018-04-23	01:42:12	out	741741	Crisis Text Line	Oops already wearing them
2018-04-23	01:43:07	in	741741	Crisis Text Line	I am sorry, just trying to clarify- are you going to see her soon?
2018-04-23	01:44:02	out	741741	Crisis Text Line	No way to get to her will have cops shoot me in front of her house
2018-04-23	01:44:56	out	741741	Crisis Text Line	In car with bunch of knives
2018-04-23	01:47:13	out	741741	Crisis Text Line	Driving pulled over because I thought u sent text
2018-04-23	01:47:49	in	741741	Crisis Text Line	Can you agree to stay safe?
2018-04-23	01:48:06	out	741741	Crisis Text Line	No
2018-04-23	01:48:47	in	741741	Crisis Text Line	Are you in the car driving to your ex-girlfriends house?  Can you pull over while we text?
2018-04-23	01:49:42	out	741741	Crisis Text Line	I'm pulled over about third of way there
2018-04-23	01:49:46	in	741741	Crisis Text Line	Could you tell me where your girlfriend lives?
2018-04-23	01:51:05	out	741741	Crisis Text Line	No either I'm saved without police or they kill me sick of mental hospitals and jail
2018-04-23	01:51:23	in	741741	Crisis Text Line	Dexter, you deserve to be happy.  I know it is hard and takes a lot of work, but I believe part of you wants to get better.
2018-04-23	01:52:19	out	741741	Crisis Text Line	Oh I do but I hate uncertainty and am very scared about future
2018-04-23	01:53:21	in	741741	Crisis Text Line	The future can be a very scary thing at times.  I am wondering if you will safety plan with me right now?  I care about you and your life.
2018-04-23	01:55:56	in	741741	Crisis Text Line	Dexter, we can make a plan together to begin moving forward.
2018-04-23	01:57:19	out	741741	Crisis Text Line	Just pulled into a parking lot about a mile from her place
2018-04-23	01:57:47	in	741741	Crisis Text Line	Okay, I am glad you are parked.  Would you be willing to tell me where you are?
2018-04-23	01:58:32	out	741741	Crisis Text Line	No because I'll send police just having weapons will send me to jail because of court order
2018-04-23	02:01:12	in	741741	Crisis Text Line	I know you are feeling upset and alone.  Is there anyone that you can call that might be able to help?
2018-04-23	02:01:37	out	741741	Crisis Text Line	No
2018-04-23	02:03:27	out	741741	Crisis Text Line	I'm chickening out and driving back
2018-04-23	02:03:54	in	741741	Crisis Text Line	There seems to be a part of you that wants to live, yes?
2018-04-23	02:07:25	in	741741	Crisis Text Line	Dexter, can you pull over and create a plan to keep you safe tonight?
2018-04-23	02:10:56	out	741741	Crisis Text Line	 Almost back
2018-04-23	02:12:07	out	741741	Crisis Text Line	Back
2018-04-23	02:12:19	out	741741	Crisis Text Line	At where stay ing
2018-04-23	02:16:21	out	741741	Crisis Text Line	I put away knives and am laying bed
2018-04-23	02:16:47	in	741741	Crisis Text Line	That is good, are you willing to talk about making that plan with me?
2018-04-23	02:18:45	in	741741	Crisis Text Line	A plan to get some help...I am going to send you a few sites that might be helpful to you and free online support.
2018-04-23	02:20:47	in	741741	Crisis Text Line	Inspire http://bit.ly/inspire_referral  ffers a wide range of anonymous online peer support groups to connect to people who share similar hardships.
2018-04-23	02:22:40	in	741741	Crisis Text Line	Meetup http://bit.ly/meetup_referral  Helps find and connect with local groups of people who share common interests.
2018-04-23	02:23:01	in	741741	Crisis Text Line	Dexter, we are going to need to wrap this conversation up soon.  Will you be safe tonight?
2018-04-23	02:23:35	out	741741	Crisis Text Line	Yes
2018-04-23	02:24:07	in	741741	Crisis Text Line	Will you take a look at the support sites I gave you?
2018-04-23	02:24:23	out	741741	Crisis Text Line	Yes
2018-04-23	02:24:50	in	741741	Crisis Text Line	Good for you, you should be proud of yourself for having the courage to reach out for help.
2018-04-23	02:25:31	in	741741	Crisis Text Line	We are here for you should you need us in the future Dexter.
2018-04-23	02:26:10	in	741741	Crisis Text Line	Take care.
 
 
Item 2930: 
NtTraceControl (CtrlCode=0xf, InputBuffer=0x19f748, InputBufferLength=0xa0, OutputBuffer=0x19f748, OutputBufferLength=0xa0, ReturnLength=0x19f73c [0xa0]) => 0
 Before: 
NtTraceControl (CtrlCode=0xf, InputBuffer=0x19f748, InputBufferLength=0xa0, OutputBuffer=0x19f748, OutputBufferLength=0xa0, ReturnLength=0x19f73c [0xa0]) => 0
 After:
NtTraceControl
 
 
Item 2932: 
[[[ 18 18 16]
[ 22 24 21]
[ 26 28 25]
…,
[105 94 87]
[218 209 203]
[229 222 216]]
…,
[[ 13 13 13]
[ 12 14 11]
[ 14 16 13]
…,
[ 50 39 32]
[116 108 101]
[208 201 195]]]
 Training Data Accuracy: 0.886942675159
Test Data Accuracy: 0.809523809524
 Training Data Accuracy: 0.942675159236
Test Data Accuracy: 0.819047619048
 Training Data Accuracy: 0.996815286624
Test Data Accuracy: 0.847619047619
 Training Data Accuracy: 0.829617834395
Test Data Accuracy: 0.728571428571
 Training Data Accuracy: 0.998407643312
Test Data Accuracy:0.857142857143
 
 
Item 2934: 
Yvan Scher (2018). cLPR Dataset. Brooklyn, NY: The Internet
 OR
 @misc{Scher:2018,
author = "Yvan Scher",
year = "2018",
title = "cLPR Dataset",
url = "https://bit.ly/2yeeA15",
institution = "The Internet"}
 OR
 Just say you used the cLPR dataset and link back to this post :)
 
 
Item 2939: 
Q = R + Qmax  (Eq. 1)
 Q(s,a) = R + gamma * Qmax(s1, a1)   (Eq. 2)
 Q(s, "DOWN") = -0.1 + 0.9 = 0.8
 Q(s,a) = Q(s,a) + step_size * ( (R + gamma * Qmax(s1, a1) - Q(s,a) ) (Eq. 3)
 Q-Learning Algorithm
 Given exploration constant e
Given a learning rate l
Given an Environment S composed of state, action pairs
Given a set of Terminal states T
Given an initial state s0
Given a reward function R
 Let Q:S -> Real be the zero function.
 while s0 is not a terminal state:
   Let p <- a random value between 0 and 1
   if p < e:
     choose a random action from the state s0
   else:
      choose the action "a" that maximizes Q(s0,a)
     
   Let s1 <- the state obtained by performing action a at s0
   Let a0 be the action that maximizes Q(s1, a)
   Let r be the reward obtained by taking action a at s0.
   
   Q(s0, a) = Q(s0, a) + l * [r + gamma * Q(s1,a1) - Q(s0, a)]
    Let s0 <- s1
 
 
Item 2942: 
docker run -it — privileged hashmapinc/tempus-edge-development-fog:0.1.0
 iofog provision <YOUR-KEY>
 
 
Item 2944: 
+++++ +++++             initialize counter (cell #0) to 10
[                       use loop to set 70/100/30/10
    > +++++ ++              add  7 to cell #1
    > +++++ +++++           add 10 to cell #2
    > +++                   add  3 to cell #3
    > +                     add  1 to cell #4
<<<< -                  decrement counter (cell #0)
]
> ++ .                  print 'H'
> + .                   print 'e'
+++++ ++ .              print 'l'
.                       print 'l'
+++ .                   print 'o'
> ++ .                  print ' '
<< +++++ +++++ +++++ .  print 'W'
> .                     print 'o'
+++ .                   print 'r'
----- - .               print 'l'
----- --- .             print 'd'
> + .                   print '!'
> .                     print '\n'
 
 
Item 2945: 
#Sigmoid function
s = 1/(1+np.exp(-x))
 #Sigmoid gradient
s = sigmoid(x)
ds = s*(1-s)
 #Reshaping arrays
v = v.reshape(v.shape[0]*v.shape[1]*v.shape[2],1)
v = v.reshape(v.shape[0], -1).T
 #Normalizing rows
x_norm = np.linalg.norm(x,axis=1,keepdims=True)
x = x/x_norm
 #Sums each row
np.sum(x, axis=1, keepdims=True)
 #Vector square
X2 = np.dot((y-yhat),(y-yhat)) = sum(np.square(y-yhat))
 #Train the logistic regression classifier
clf = sklearn.linear_model.LogisticRegressionCV();
clf.fit(X, Y);
 #Plot the decision boundary for logistic regression
plot_decision_boundary(lambda x: clf.predict(x), X, Y)
 #Accuracy
LR_predictions = clf.predict(X.T)
accuracy = float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100)
 #Initialize the parameters
W1 = np.random.randn(n_h, n_x)*0.01
b1 = np.zeros((n_h, 1))
W2 = np.random.randn(n_y, n_h)*0.01
b2 = np.zeros((n_y, 1))
 ----FOR LOOP----
#Forward propagation
#Retrieve each parameter from the dictionary "parameters"
W1 = parameters['W1']
b1 = parameters['b1']
W2 = parameters['W2']
b2 = parameters['b2']
 #Implement Forward Propagation to calculate
Z1 = np.dot(W1,X)+b1
A1 = np.tanh(Z1)
Z2 = np.dot(W2,A1)+b2
A2 = sigmoid(Z2)
 #Compute cost
logprobs = np.multiply(np.log(A2),Y)+np.multiply(np.log(1-A2),1-Y)
cost = -(1/m)*np.sum(logprobs)
cost = np.squeeze(cost)     
# makes sure cost is the dimension we expect. 
# E.g., turns [[17]] into 17
 #Backward propagation
dZ2= A2-Y
dW2 = (1/m)*np.dot(dZ2,A1.T)
db2 = (1/m)*np.sum(dZ2,axis=1,keepdims=True)
dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))
dW1 = (1/m)*np.dot(dZ1,X.T)
db1 = (1/m)*np.sum(dZ1,axis=1,keepdims=True)
 #Update parameter
W1 = W1-dW1*learning_rate
b1 = b1-db1*learning_rate
W2 = W2-dW2*learning_rate
b2 = b2-db2*learning_rate
 ----FOR LOOP END----
 
 
Item 2946: 
"brands": [
{
    "id": 0,
    "name": "Mike's Cool Lemondade",
    "referenceId": "Mikes_Cool_Lemonade",
    "referenceUrl": "http://en.wikipedia.org/wiki/Mikes_Cool_Lemonade",
    "referenceType": "Wiki",
    "description": "Mike's Cool Lemonade is a tasty..",
    "tags": [],
    "confidence": 0.995,
    "instances": [
    {
        "brandType": "Transcript",
        "start": "00: 00: 31.3000000",
        "end": "00: 00: 39.0600000"
    }
    ]
},
{
    "id": 1,
    "name": "Mike's Beverages",
    "wikiId": "Mike's Beverages",
    "wikiUrl": "http: //en.wikipedia.org/wiki/Mikes_Beverages",
    "description": "Mike's Beverages is...",
    "tags": [
    "competitors",
    "technology"
    ],
    "confidence": 1.0,
    "instances": [
    {
        "brandType": "Transcript",
        "start": "00: 01: 44",
        "end": "00: 01: 45.3670000"
    },
    {
        "brandType": "Ocr",
        "start": "00: 01: 54",
        "end": "00: 02: 45.3670000"
    }
    ]
}
]
 
 
Item 2949: 
def get_next(input):
'''
input: a list of token
output : a list of probabilities of all tokens
 '''
 def get_sentence(input, FSM):
   1. Call get_next
   2. Call FSM to filter out invalid tokens
   3. Sample the tokens
   4. Update FSM with the output
   5. Call get_next until it breaks (either reach the "end of sentence" token or the sequence reach a maximum length.
 
 
Item 2962: 
explainer = anchor_text.AnchorText(spacy_nlp, labels, use_unk_distribution=True)
    exp = explainer.explain_instance(x_test[idx], estimator, threshold=0.8, use_proba=True, batch_size=30)
 max_pred = 2
    print('Key Singal from Anchors: %s' % (' AND '.join(exp.names())))
    print('Precision: %.2f' % exp.precision())
    print()
 exp.show_in_notebook()

 
 
Item 2966: 
Hand: “Arts”, “Quick”, ”Quick”, “Buster”, “Arts”
output: “Arts”, “Arts”, “Arts”
 Hand: “Arts”, “Quick”, ”Quick”, “Buster”, “Arts”
output_1: “Card 1”, “Card 2”, “Card 3”
output_2: “Card 1”, “Card 2”, “Card 4”
...
output_60: “Card 5”, “Card 4”, “Card 3”
 Hand_1: “Arts”, “Arts”, ”Quick”, “Buster”, “Arts”
Hand_2: “Buster”, “Quick”, ”Arts”, “Arts”, “Arts”
 Hand: "Buster","Quick","Arts","Buster","Quick
 Project Pendragon Bot: “Arts”, “Buster”, “Buster”
Alter Pendragon Bot: “Buster”, “Arts”, “Buster"
 
 
Item 2967: 
[
  {
    “id”: “abc”,
    “ts”: 1318250880000,
  },
  ...
]
 {
  “groups”: [
    {
      “id”: “abc”,
      “title”: “New products”,
      “createdAt”: “2011–10–10T14:48:00Z”,
      “comment”: “Has been deleted from log tracking system”
    }
  ]
}
 {
  “groups”: [
    {
      “id”: “123”,
      “title”: “My test group”,
      “members”: [
        “joe@example.com”, “sue@example.com”
      ],
      “bad_ideas”: [
        {
          “content”: “Make things better”,
          “reason”: “Too generic”,
          “improvement“: “Ask yourself which steps you would take“,
          “created_at”: “2011–10–10T14:48:00Z”
        }
      ]
    }
  ]
}
 
 
Item 2968: 
[ 0 | probability Tiger pays: 0.333 +/- 0.10 | probability Jason pays: 0.333 +/- 0.10 | probability James pays: 0.333 +/- 0.10 ] 
...
 [ 700 | probability Tiger pays: 0.368 +/- 0.11 | probability Jason pays: 0.343 +/- 0.10 | probability James pays: 0.288 +/- 0.10 ] 
...
 [ 2500 | probability Tiger pays: 0.377 +/- 0.10 | probability Jason pays: 0.340 +/- 0.10 | probability James pays: 0.283 +/- 0.10 ]
 
 
Item 2977: 
docker run --rm -it -p 9008:9008 -p 8080:8080 skymindops/skil-ce bash /start-skil.sh
 
 
Item 2978: 
distmatrix<-stringdistmatrix(tolower(evl),ET, useNames=TRUE ,method = “dl”)
 distmatrixdf<- as.data.frame(distmatrix)
ET_Cl<-setNames(as.list(colnames(distmatrixdf)apply(distmatrixdf,1, which.min)]), evl)
 
 
Item 2980: 
Select player: ['X', 'O'] x
   ¦   ¦   
---+---+--- 
   ¦   ¦   
---+---+--- 
   ¦   ¦   
 Input move: 5
 O ¦   ¦   
---+---+--- 
   ¦ X ¦   
---+---+--- 
   ¦   ¦   
 Input move: 2
 O ¦ X ¦   
---+---+--- 
   ¦ X ¦   
---+---+--- 
   ¦ O ¦   
 Input move: 6
 O ¦ X ¦   
---+---+--- 
 O ¦ X ¦ X 
---+---+--- 
   ¦ O ¦   
 Input move: 7
 O ¦ X ¦ O 
---+---+--- 
 O ¦ X ¦ X 
---+---+--- 
 X ¦ O ¦   
 Input move: 9
 O ¦ X ¦ O 
---+---+--- 
 O ¦ X ¦ X 
---+---+--- 
 X ¦ O ¦ X 
 It's a draw
 Select player: ['X', 'O'] o
 X ¦   ¦   
---+---+--- 
   ¦   ¦   
---+---+--- 
   ¦   ¦   
 Input move: 2
 X ¦ O ¦   
---+---+--- 
 X ¦   ¦   
---+---+--- 
   ¦   ¦   
 Input move: 3
 X ¦ O ¦ O 
---+---+--- 
 X ¦ X ¦   
---+---+--- 
   ¦   ¦   
 Input move: 6
 X ¦ O ¦ O 
---+---+--- 
 X ¦ X ¦ O 
---+---+--- 
 X ¦   ¦   
 You lost :-(
 Select player: ['X', 'O'] o
 X ¦   ¦   
---+---+--- 
   ¦   ¦   
---+---+--- 
   ¦   ¦   
 Input move: 2
 X ¦ O ¦   
---+---+--- 
 X ¦   ¦   
---+---+--- 
   ¦   ¦   
 Input move: 3
 X ¦ O ¦ O 
---+---+--- 
 X ¦   ¦   
---+---+--- 
 X ¦   ¦   
 You lost :-(
 Select player: ['X', 'O'] o
   ¦   ¦   
---+---+--- 
   ¦ X ¦   
---+---+--- 
   ¦   ¦   
 Input move: 1
 O ¦ X ¦   
---+---+--- 
   ¦ X ¦   
---+---+--- 
   ¦   ¦   
 Input move: 3
 O ¦ X ¦ O 
---+---+--- 
   ¦ X ¦   
---+---+--- 
   ¦ X ¦   
 You lost :-(
 
 
Item 2982: 
.reduceLeft(_ union _)
 files.map(fromFile).map(_.filter($"rating" > 0.1).count).sum
 1 - 7 - 5 - 2 = -13
 0 - 1 - 7 - 5 - 2 = -15
 
 
Item 2983: 
[
  {
    "faceId": "6b854442-634e-4d46-9091-00165bf12e3e",
    "faceRectangle": {
      "top": 128,
      "left": 459,
      "width": 224,
      "height": 224
    },
    "faceAttributes": {
      "hair": {
        "bald": 0.0,
        "invisible": false,
        "hairColor": [
          {
            "color": "brown",
            "confidence": 1.0
          },
          {
            "color": "blond",
            "confidence": 0.69
          },
          {
            "color": "black",
            "confidence": 0.54
          },
          {
            "color": "other",
            "confidence": 0.31
          },
          {
            "color": "gray",
            "confidence": 0.05
          },
          {
            "color": "red",
            "confidence": 0.04
          }
        ]
      },
      "smile": 0.639,
      "headPose": {
        "pitch": 0.0,
        "roll": -16.9,
        "yaw": 16.7
      },
      "gender": "male",
      "age": 27.4,
      "facialHair": {
        "moustache": 0.0,
        "beard": 0.0,
        "sideburns": 0.0
      },
      "glasses": "ReadingGlasses",
      "makeup": {
        "eyeMakeup": true,
        "lipMakeup": true
      },
      "emotion": {
        "anger": 0.015,
        "contempt": 0.001,
        "disgust": 0.037,
        "fear": 0.001,
        "happiness": 0.939,
        "neutral": 0.001,
        "sadness": 0.0,
        "surprise": 0.007
      },
      "occlusion": {
        "foreheadOccluded": false,
        "eyeOccluded": false,
        "mouthOccluded": false
      },
      "accessories": [
        {
          "type": "glasses",
          "confidence": 0.99
        }
      ],
      "blur": {
        "blurLevel": "low",
        "value": 0.0
      },
      "exposure": {
        "exposureLevel": "goodExposure",
        "value": 0.48
      },
      "noise": {
        "noiseLevel": "low",
        "value": 0.0
      }
    },
    "faceLandmarks": {
      "pupilLeft": {
        "x": 504.8,
        "y": 206.8
      },
      "pupilRight": {
        "x": 602.5,
        "y": 178.4
      },
      "noseTip": {
        "x": 593.5,
        "y": 247.3
      },
      "mouthLeft": {
        "x": 529.8,
        "y": 300.5
      },
      "mouthRight": {
        "x": 626.0,
        "y": 277.3
      },
      "eyebrowLeftOuter": {
        "x": 461.0,
        "y": 186.8
      },
      "eyebrowLeftInner": {
        "x": 541.9,
        "y": 178.9
      },
      "eyeLeftOuter": {
        "x": 490.9,
        "y": 209.0
      },
      "eyeLeftTop": {
        "x": 509.1,
        "y": 199.5
      },
      "eyeLeftBottom": {
        "x": 509.3,
        "y": 213.9
      },
      "eyeLeftInner": {
        "x": 529.0,
        "y": 205.0
      },
      "eyebrowRightInner": {
        "x": 579.2,
        "y": 169.2
      },
      "eyebrowRightOuter": {
        "x": 633.0,
        "y": 136.4
      },
      "eyeRightInner": {
        "x": 590.5,
        "y": 184.5
      },
      "eyeRightTop": {
        "x": 604.2,
        "y": 171.5
      },
      "eyeRightBottom": {
        "x": 608.4,
        "y": 184.0
      },
      "eyeRightOuter": {
        "x": 623.8,
        "y": 173.7
      },
      "noseRootLeft": {
        "x": 549.8,
        "y": 200.3
      },
      "noseRootRight": {
        "x": 580.7,
        "y": 192.3
      },
      "noseLeftAlarTop": {
        "x": 557.2,
        "y": 234.6
      },
      "noseRightAlarTop": {
        "x": 603.2,
        "y": 225.1
      },
      "noseLeftAlarOutTip": {
        "x": 545.4,
        "y": 255.5
      },
      "noseRightAlarOutTip": {
        "x": 615.9,
        "y": 239.5
      },
      "upperLipTop": {
        "x": 591.1,
        "y": 278.4
      },
      "upperLipBottom": {
        "x": 593.2,
        "y": 288.7
      },
      "underLipTop": {
        "x": 597.1,
        "y": 308.0
      },
      "underLipBottom": {
        "x": 600.3,
        "y": 324.8
      }
    }
  }
]
 "emotion": {
        "anger": 0.037,
        "contempt": 0.001,
        "disgust": 0.015,
        "fear": 0.001,
        "happiness": 0.939,
        "neutral": 0.001,
        "sadness": 0.0,
        "surprise": 0.007
      }
 
 
Item 2984: 
sentence = "Jul 29 is the 210th day of the year"
pattern = r'((Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) [0-9]{2})'
 results = re.findall(pattern, sentence)
print('Result:', results)
 Result: [('Jul 29', 'Jul')]
 
 
Item 2990: 
!pkill -9 -f ipykernel_launcher
 !pip install fastai pytorch
 !mkdir data && wget http://files.fast.ai/data/dogscats.zip && unzip dogscats.zip -d data/
 !wget http://files.fast.ai/models/weights.tgz && tar -xzf weights.tgz -C /usr/local/lib/python3.6/dist-packages/fastai
 
 
Item 2994: 
system("git config --global user.email 'charles2588@gmail.com'")
 system("git config --global user.name 'Charles Gomes'")
 system("git config --global user.email 'ccgomes@us.ibm.com'")
 system("git config --global user.name 'Charles Gomes'")
 
 
Item 3000: 
pre-main prep time: 56 ms
mcm
 pre-main prep time: 58 ms
U4FTP
 
 
Item 3008: 
Why AI is good news for medicine
 From diagnosing cancers and predicting bipolar episodes to establishing personalised doses, machine learning can do the heavy lifting that is needed for better medical outcomes.
 Being able to find patterns in masses of data, it can be used to direct drug development by suggesting ideal candidates for medical trials and help scientists better understand the progression of various diseases, such as MS or diabetes.
 Also, machine learning offers an unexpected benefit: it’s good at recognising letters, even doctors’ notoriously bad handwriting.
 
 
Item 3011: 
##   isnull                variable
## 4  29062            ProsperScore
## 7   7522 AvailableBankcardCredit
 ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1000    4000    6500    8338   12000   35000
 ##   LoanOriginationYearMonth     n
##   <chr>                    <int>
## 1 2014.01                   5865
## 2 2014.02                   4485
## 3 2014.03                   1822
 ## df_tree_months$LoanOriginationYear: 2006
## [1] 4959.831
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2007
## [1] 6472.595
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2008
## [1] 6658.935
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2010
## [1] 4773.685
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2011
## [1] 6577.909
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2012
## [1] 7455.283
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2013
## [1] 9386.756
## -------------------------------------------------------- 
## df_tree_months$LoanOriginationYear: 2014
## [1] 11912.22
 ## df$Term: 12
## [1] 383.9334
## -------------------------------------------------------- 
## df$Term: 36
## [1] 258.527
## -------------------------------------------------------- 
## df$Term: 60
## [1] 315.2104
 ## 
##  Pearson's product-moment correlation
## 
## data:  df_score$ProsperScore and df_score$BorrowerRate
## t = -248.98, df = 84851, p-value < 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.6536072 -0.6458311
## sample estimates:
##        cor 
## -0.6497361
 
 
Item 3012: 
       The future is independent of the past given the present.
 
 
Item 3013: 
You can check detailed curriculum of Machine learning and AI on GitHub.   
 
 
Item 3015: 
char_cnn = CharCNN(max_len_of_sentence=256, max_num_of_setnence=1)
char_cnn.preporcess(labels=training_df['label'].unique())
 x_train, y_train = char_cnn.process(
    df=train_df, x_col='name', y_col='label')
x_test, y_test = char_cnn.process(
    df=test_df, x_col='name', y_col='label')
 char_cnn.build_model()
char_cnn.train(x_train, y_train, x_test, y_test, batch_size=32, epochs=10)
 Train on 341 samples, validate on 86 samples
Epoch 1/10
341/341 [==============================] - 13s 40ms/step - loss: 0.9741 - acc: 0.7801 - val_loss: 1.0188 - val_acc: 0.7674
Epoch 2/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7981 - acc: 0.8240 - val_loss: 1.0271 - val_acc: 0.7674
Epoch 3/10
341/341 [==============================] - 11s 31ms/step - loss: 0.7864 - acc: 0.8240 - val_loss: 1.0512 - val_acc: 0.7674
Epoch 4/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7660 - acc: 0.8240 - val_loss: 0.9753 - val_acc: 0.7674
Epoch 5/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7506 - acc: 0.8240 - val_loss: 1.0135 - val_acc: 0.7674
Epoch 6/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7426 - acc: 0.8240 - val_loss: 1.0135 - val_acc: 0.7674
Epoch 7/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7824 - acc: 0.8240 - val_loss: 1.0176 - val_acc: 0.7674
Epoch 8/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7763 - acc: 0.8240 - val_loss: 1.0137 - val_acc: 0.7674
Epoch 9/10
341/341 [==============================] - 10s 30ms/step - loss: 0.7805 - acc: 0.8240 - val_loss: 0.9793 - val_acc: 0.7674
Epoch 10/10
341/341 [==============================] - 10s 31ms/step - loss: 0.7624 - acc: 0.8240 - val_loss: 1.0138 - val_acc: 0.7674
 array([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,
       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])
 
 
Item 3023: 
> CCES16_pca <- prcomp(CCES16_sub[,-(1:3)]) ## performs PCA on data
> CCES16_pca$rotation[,1:2]
                                PC1           PC2
isis_stay_out           0.006532851 -0.1877095638
isis_hum_aid           -0.133312164  0.2700242152
isis_arms              -0.025175755  0.3663478247
isis_nofly             -0.008665488  0.3647198335
isis_airstrikes         0.005785819  0.4076031669
gun_background_checks  -0.076457752  0.0242503785
gun_no_public_registry  0.123844327  0.0272378861
gun_ar_ban             -0.186209188 -0.0100165559
gun_concealed           0.195130406 -0.0028054992
imm_legal_status       -0.190176588  0.0715418669
imm_border_patrol       0.189488733  0.1492354751
imm_dream              -0.191172826  0.1197417816
imm_fine_businesses     0.106929951  0.1530306786
imm_syria_ban           0.203850961  0.0140942991
imm_more_visas         -0.073282411  0.0496037023
imm_deport              0.197898642  0.0278690862
imm_muslim_ban          0.139182292 -0.0273335401
imm_none               -0.001797488 -0.0363688264
abort_always           -0.195514597 -0.0781297299
abort_rape_incest       0.142650550  0.0605024871
abort_20wk_ban          0.167085753  0.0631677275
abort_insurance         0.217539438  0.0431472519
abort_hyde              0.216396945  0.0508478793
abort_total_ban         0.044733238 -0.0216862117
env_co2_reg            -0.227977412 -0.0062988802
env_35mpg              -0.178015377  0.0262815214
env_renewable          -0.211617779  0.0295352934
env_clean_acts         -0.230549981 -0.0071572615
crime_no_mand_min      -0.127007732  0.0007970827
crime_body_camera      -0.053029933  0.0140239449
crime_more_police       0.101616084  0.0999435583
crime_longer_sentences  0.080147299  0.0564094700
gay_marriage           -0.187292524 -0.0287997800
cong_garland           -0.237629283 -0.0003028243
cong_tpp               -0.098299596  0.0185739825
cong_usa_freedom       -0.015027702  0.0348062545
cong_taa               -0.083279783  0.0756188820
cong_education_reform  -0.006577012 -0.0220838347
cong_highway_transp    -0.066774433  0.0641419762
cong_iran_sanctions     0.061452667  0.0952179931
cong_medicare_reform   -0.082131998  0.0751534156
cong_repeal_ACA         0.244556839 -0.0170201176
cong_min_wage          -0.200319255  0.0079648727
military_oil            0.070407132  0.0885752720
military_terrorist      0.084181607  0.2901872566
military_intervene     -0.047762336  0.2927693240
military_democracy      0.003800022  0.0706793022
military_protect_ally   0.018120978  0.2670319807
military_help_UN       -0.154924594  0.2202543684
military_none          -0.007456204 -0.1494680694
 > tapply(vote_pca$PC1, INDEX = vote_pca$PresVote, FUN = median)
  Trump (R) Clinton (D) Johnson (L)   Stein (G) 
  1.7362782  -1.6009931   0.1634565  -1.3869899 

> tapply(vote_pca$PC2, INDEX = vote_pca$PresVote, FUN = median)
  Trump (R) Clinton (D) Johnson (L)   Stein (G) 
 0.03686338  0.10200141 -0.36040349 -0.76926122
 
 
Item 3025: 
!pip install --upgrade pixiedust
 
 
Item 3030: 
{'location': 17, 'piece': 36, 'offset': 67.5, 'speed': 390, 'self_speed': 400, 'clockwise': 7, 'notification_time': datetime.datetime(2018, 8, 25, 21, 9, 33, 359248), 'is_clockwise': False}
 {'location': 23, 'piece': 57, 'offset': 67.5, 'speed': 422, 'self_speed': 400, 'clockwise': 71, 'notification_time': datetime.datetime(2018, 8, 25, 21, 9, 32, 229689), 'is_clockwise': True}
 ...
 time lap in seconds was 9.046201
...
time lap in seconds was 9.029405
...
time lap in seconds was 9.045055
...
time lap in seconds was 9.044495
...
 time lap in seconds was 4.497926
...
time lap in seconds was 4.502276
...
time lap in seconds was 4.497534
...
 node_app
    node_socket-app
       node_server.js
    package-lock.json
python_app
    py_overdrive_sdk
        py_overdrive.py
    track_images/
    create_track_image.py
    constant_speed_example.py
    track_discovery_example.py
    custom_policy_example.py
    requirements.txt
.gitignore
 
 
Item 3034: 
const TelegramBot = require("node-telegram-bot-api");
const bot = new TelegramBot(process.env.TELEGRAM_API_KEY, {polling: true});
 bot.onText(/\/start/, (msg) => {
    bot.sendMessage(msg.chat.id, "Welcome", {
        "reply_markup": {
            "keyboard": [["Create Carpool", "Search Carpool"], ["Corporate Carpool", "Contact Us"]]
        }
    });
});
 bot.on("message", (msg) => {
    const userId = msg.chat.id;

    if (msg.text.toString().toLowerCase().includes("search")) {
        searchInitiated(bot, msg, userId, users);
    }
....
...
..
}
 
 
Item 3036: 
df.head()
 df.index
 df.columns
 df.sha
 df.info()
 df.describe()
 df['type'].value_counts()
 df = df[df['type'].isin(['Ride','Run'])]
 df.loc[df['type']=='Run', 'average_watts'] = 0
df.info
 df.drop('average_heartrate', axis=1, inplace=True)
df.dropna(inplace=True)
df.info()
 df['distance'] = df['distance']/1000
df['distance'] = df['distance'].round(2)
df['average_speed'] = df['average_speed'] * 18/5
df['average_speed']  = df['average_speed'].round(2)
 df['elapsed_time'].str.len().value_counts()
 df[df['elapsed_time'].str.len() == 16]['elapsed_time']
 df.drop(583, axis=0, inplace=True)
 def get_minutes(time_str):
    
    h, m, s = time_str.split(':') 
    total_min = int(h)*60+int(m)+int(s)/60
    return int(total_min)
 df['time_min'] = df['elapsed_time'].apply(get_minutes)
 df.drop('elapsed_time', axis=1, inplace=True)
df.head()
 df['start_date_local'] = pd.to_datetime(df['start_date_local'])
 df.set_index('start_date_local', inplace=True)
df.head()
 df.index = df.index.round('D', inplace=True)
df.index.rename('date', inplace=True)
df.head()
 grouped = df.groupby('type')
 def f(group):
    return (group-group.mean()).abs().div(group.std())
 grouped.transform(f).head()
 outliers = grouped.transform(f) > 3
df = df[~outliers.any(axis=1)]
 
 
Item 3041: 
[C 13:54:29.628 NotebookApp] Bad config encountered during initialization:
[C 13:54:29.629 NotebookApp] The 'contents_manager_class' trait of <notebook.notebookapp.NotebookApp object at 0x10adbe240> instance must be a type, but 'ipymd.IPymdContentsManager' could not be imported
 jupyter notebook --generate-config
 
 
Item 3042: 
curl -H "Content-Type: application/json"  -d '{"url": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT1ebvv98aZDJ03j3SOTIsDD7Q4eZmB4NoXpzO_R5_tVeFKoHGi"}' https://traffic-api-ai.now.sh
 
 
Item 3046: 
val evaluator = new MulticlassClassificationEvaluator()
   .setLabelCol("CategoryNameIndex")
   .setPredictionCol("prediction")
   .setMetricName("accuracy")
 val accuracy = evaluator.evaluate(predictions)
 println("Accuracy= " + accuracy)
  
   spark.stop()
 Accuracy = 0.935483870967742
 
 
Item 3047: 
Think about the augmented friendship that today’s technologies are able to create at scale around the globe.
 CHAT WITH US AND BE ONE STEP BEYOND
 USE BLiP AND LEVEL UP YOUR CHATBOTS
 
 
Item 3049: 
SOURCE  |SZ|AL|                  SET:0                 | %
--------+--+--+----------------------------------------+----
        |  |BF|a c e i o t u ä ü                       |73.0
Tristan |32|Hl|a c e ä i ê o î p u t ö y û ü           |73.3
German  |  |Sk|e a i o u h ü ä p ö ê y û î             |72.4
--------+--+--+----------------------------------------+----
        |  |BF|a e h i o u y                           |75.9
Dante   |24|Hl|a e h i o u y                           |75.9
Italian |  |Sk|e o a i u h y                           |75.9
--------+--+--+----------------------------------------+----
        |  |BF|a e i o u y                             |79.8
Mattioli|25|Hl|a e i o u y                             |79.8
Latin   |  |Sk|i e u a o y                             |79.8
--------+--+--+----------------------------------------+----
K.James |  |BF|a d e i o t u                           |74.1
 Bible  |26|Hl|a d e i o t u                           |74.1
English |  |Sk|e a o i t u d                           |74.1
--------+--+--+-------------------+--------------------+----
 SOURCE |SZ|AL|     SET:0         |    SET:0 EVA       | %
--------+--+--+-------------------+--------------------+----
        |  |BF|9 A C O            |y a e o             |81.8
VMS_CD  |35|Hl|0 9 A C O L        |iiir y a e o iim    |81.8
        |  |Sk|O A C 9 0 L        |o a e y iiir iim    |81.8
--------+--+--+-------------------+--------------------+----
        |  |BF|A C G I O Z        |a e y i o [Z]       |77.9
VMS_FSG |24|Hl|A C G I O Z        |a e y i o [Z]       |77.9
        |  |Sk|O A C G Z I 7      |o a e y [Z] i j     |77.9
--------+--+--+-------------------+--------------------+----
        |  |BF|a c e n o s y      |a c e n o s y       |80.2
VMS_EVA |22|Hl|a c e n o s y      |a c e n o s y       |80.2
 TT     |  |Sk|o h a y n g v      |o h a y n g v       |72.5
--------+--+--+-------------------+--------------------+----
        |  |BF|a c e n o s u y    |a c e n o s u y     |80.4
VMS_EVA |25|Hl|a c e n o s u y    |a c e n o s u y     |80.4
 ZL     |  |Sk|o h a y n g b u    |o h a y n g b u     |72.7
--------+--+--+-------------------+--------------------+----
        |  |BF|A E O U Y c        |a e o ee y c        |80.7
VMS_CUVA|27|Hl|A E O U Y c h      |a e o ee y c h      |80.7
        |  |Sk|O A Y E U c h      |o a y e ee c h      |80.7
--------+--+--+-------------------+--------------------+----
        |  |BF|E a e o y          |ee a e o y          |81.9
VMS_NEAL|33|Hl|E a c e o u y      |ee a c e o u y      |81.9
        |  |Sk|o a y e E c u h    |o a y e ee c u h    |81.9     
--------+--+--+-------------------+--------------------+----
        |  |BF|E a e g o y        |ee a e g o y        |81.1
VMS_NEAL|32|Hl|E a c e g o u y    |ee a c e g o u y    |81.2
 Cur.A  |  |Sk|o a y e E g c u h  |o a y e ee g c u h  |81.2
--------+--+--+-------------------+--------------------+----
        |  |BF|E a e o y          |ee a e o y          |82.5
VMS_NEAL|32|Hl|E a c e h o u y z  |ee a c e h o u y z  |82.5
 Cur.B  |  |Sk|o a y e E c h u z i|o a y e ee c h u z i|82.5
 -------+--+--+-------------------+-------------------+-----
VMS_GUY |  |BF|C a c g i o t      |ee a e y i o h      |83.8 
79v 80r |23|Hl|C a c g i o t      |ee a e y i o h      |83.8 
   1    |  |Sk|o a g c C i t      |o a y e ee i h      |83.8
--------+--+--+-------------------+--------------------+----
VMS_GUY |  |BF|C P Q a c g i o    |ee [P] [Q] a e y i o|84.6
79v 80r |24|Hl|C P Q a c g i o    |ee [P] [Q] a e y i o|84.6
   2    |  |Sk|o a g c C i Q P    |o a y e ee i [Q] [P]|84.6
 
 
Item 3051: 
#include 
 
int main()
{
 std::cout << "Hello, world!";
 return 0;
}
 document.write('Hello, world!');
 print ("Hello, world!")
 
 
Item 3058: 
jira getissue key "PYG-5"
 
 
Item 3059: 
case
  when      a.date::date
    between a.firstdate::date
    and     a.firstdate::date+27 
  then      'new'
  else      'returning'
end
as          type
 left(right(id,4),2) as sample
 
 
Item 3061: 
optimize_op = tf.train.AdamOptimizer(learning_rate=0.0005, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False)...
 weights_sum = tf.reduce_sum(coin_weights)
 constraints_op = coin_weights.assign(tf.divide(tf.abs(coin_weights), tf.abs(weights_sum) ))
 weights_sum = tf.reduce_sum(tf.abs(coin_weights))
 constraints_op = coin_weights.assign(tf.divide(coin_weights, tf.abs(weights_sum) ))
 
 
Item 3071: 
size(500,500)
 populationSize = 400
numberCells = round(sqrt(populationSize))
gridSize = width/numberCells
 
for row in xrange(numberCells):
    for column in xrange(numberCells):
        x = gridSize * row
        y = gridSize * column
        ellipse(x,y,10,10)
 size(500,500)
populationSize = 10000
scaleDotSize = 3.6
for i in xrange(populationSize):
    x = random(0,width)
    y = random(0,height)
    e = ellipse(x,y,scaleDotSize,scaleDotSize)
 def setup():
    size(500,500)
    print random(50,100)
 def draw():
    x = random(0,width)
    y = random(0,height)
 
    noStroke()  
    ellipse(x,y,10,10)
 
 
Item 3077: 
PipelineOptions options = PipelineOptionsFactory.create();
  Pipeline p = Pipeline.create(options);
  
  PCollection<TableRow> weatherData = p.apply(
    BigQueryIO.Read
      .named(“ReadWeatherStations”)
      .from(“clouddataflow-readonly:samples.weather_stations”));
 PipelineOptions options = PipelineOptionsFactory.create();
  Pipeline p = Pipeline.create(options);
   PCollection<TableRow> weatherData = p.apply(
    BigQueryIO.Read
      .named(“ReadYearAndTemp”)
      .fromQuery(“SELECT year, mean_temp FROM [samples.weather_stations]”);
 
 
Item 3090: 
events = [
{
var1: 1,
var2: 2,
var3: 3,
},
 {
var1: 4,
var2: 5,
var3: 6,
},
 {
var1: 7,
var2: 8,
var3: 9,
},
 {
var1: 10,
var2: 11,
var3: 12,
}
];
 function corr(events, variablesOfInterest) {
}
 corr(events, ['var1', 'var2']);
corr(events, ['var1', 'var3']);
 
 
Item 3093: 
composer require sightengine/client-php
 SIGHTENGINEUSER=your api user
SIGHTENGINEKEY=your api key
 php artisan make:controller UploadsController
 
 
Item 3094: 
wget --keep-session-cookies --save-cookies=cookies.txt --post-data 'username=myusername&password=mypassword&submit=Login' https://www.cityscapes-dataset.com/login/
 wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1
 
 
Item 3099: 
CREATE VIEW openbridge_athena.pos_sales_by_day_view AS
SELECT 
TRIM(item) AS item, 
SUM(retail_sales) as sales,
mytable_week_end as date
FROM openbridge_athena.my_data
GROUP BY mytable_week_end, item;
 
 
Item 3105: 
!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl
 !pip3 install torchvision
 
 
Item 3108: 
install.packages('ggplot2')
 qplot(x=friend_count,data=pf,xlim=c(0,1000))
 qplot(x=friend_count,data=pf) + 
  scale_x_continuous(limits=c(0,1000))
 qplot(x = friend_count, data = subset(pf,!is.na(gender)), binwidth = 10) +
  scale_x_continuous(limits = c(0, 1000),
                     breaks = seq(0, 1000, 100)) + 
  facet_wrap(gender ~ .)
 qplot(x=tenure,data=pf,
      color=I('black'),fill=I('#632b6c'))
 qplot(x=tenure/365,data=pf,binwidth=0.25,
      xlab="Number of years using facebook",
      ylab="Number of users in sample",
      color=I('black'),
      fill=I('#632b6c')) +
  scale_x_continuous(breaks = seq(1, 7, 1),limits=c(0,7))
 qplot(x=age,data=pf,xlim=c(0,200),binwidth=1,
      xlab="Number of years using facebook",
      ylab="Number of users in sample",
      color=I('black')
      ,fill=I('#95ada5')) + 
  scale_x_continuous(breaks=seq(10,130,5))
 qplot(x = friend_count,
      data = subset(pf,!is.na(gender)),
      xlab="Number of users using facebook",
      ylab="Number of friends per user",
      binwidth = 10
      , geom="freqpoly",color=gender) +
  scale_x_continuous(limits = c(0, 1000),
                     breaks = seq(0, 1000, 100))
 qplot(x=gender
      ,y=friend_count
      ,data= subset(pf,!is.na(gender))
      ,geom='boxplot')
 qplot(x=gender
      ,y=friend_count
      ,data= subset(pf,!is.na(gender))
      ,geom='boxplot')+
  coord_cartesian(ylim=c(0,1000))
 
 
Item 3110: 
Precision: 0.0
Recall:    1.0
 Arithmetic mean: 0.5
Harmonic mean:   0.0
 
 
Item 3113: 
curl -O https://raw.githubusercontent.com/tensorflow/tensorflow/r1.1/tensorflow/examples/image_retraining/retrain.py
 python retrain.py \
  --bottleneck_dir=bottlenecks \
  --how_many_training_steps=8000 \
  --model_dir=inception \
  --summaries_dir=training_summaries/basic \
  --output_graph=retrained_graph.pb \
  --output_labels=retrained_labels.txt \
  --image_dir=spectrograms
 tensorboard --logdir training_summaries
 
 
Item 3114: 
sess = tf.Session()
iris = datasets.load_iris()
x_vals = np.array([[x[0], x[3]] for x in iris.data])
y_vals = np.array([1 if y==0 else -1 for y in iris.target])
 setosa_x = [d[1] for i,d in enumerate(x_vals) if y_vals[i]==1]
setosa_y = [d[0] for i,d in enumerate(x_vals) if y_vals[i]==1]
not_setosa_x = [d[1] for i,d in enumerate(x_vals) if y_vals[i]==-1]
not_setosa_y = [d[0] for i,d in enumerate(x_vals) if y_vals[i]==-1]
 train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))
x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]
 batch_size = 100
x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)
A = tf.Variable(tf.random_normal(shape=[2,1]))
b = tf.Variable(tf.random_normal(shape=[1,1]))
 model_output = tf.subtract(tf.matmul(x_data, A), b)
 l2_norm = tf.reduce_sum(tf.square(A))
alpha = tf.constant([0.1])
classification_term = tf.reduce_mean(tf.maximum(0., tf.subtract(1.,tf.multiply(model_output, y_target))))
loss = tf.add(classification_term, tf.multiply(alpha, l2_norm))
 prediction = tf.sign(model_output)
accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_target),
tf.float32))
 my_opt = tf.train.GradientDescentOptimizer(0.01)
train_step = my_opt.minimize(loss)
init = tf.initialize_all_variables()
sess.run(init)
 loss_vec = []
train_accuracy = []
test_accuracy = []
for i in range(500):
    rand_index = np.random.choice(len(x_vals_train), size=batch_size)
    X = x_vals_train[rand_index]
    Y = np.transpose([y_vals_train[rand_index]])
    sess.run(train_step, feed_dict={x_data: X, y_target: Y})
    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})
    loss_vec.append(temp_loss)
    train_acc_temp = sess.run(accuracy, feed_dict={x_data: x_vals_train, y_target: np.transpose([y_vals_train])})
    train_accuracy.append(train_acc_temp)
    test_acc_temp = sess.run(accuracy, feed_dict={x_data: x_vals_test, y_target: np.transpose([y_vals_test])})
    test_accuracy.append(test_acc_temp)
 sess = tf.Session()
iris = datasets.load_iris()
x_vals = np.array([x[3] for x in iris.data])
y_vals = np.array([y[0] for y in iris.data])
train_indices = np.random.choice(len(x_vals), round(len(x_vals)*0.8), replace=False)
test_indices = np.array(list(set(range(len(x_vals))) - set(train_indices)))
x_vals_train = x_vals[train_indices]
x_vals_test = x_vals[test_indices]
y_vals_train = y_vals[train_indices]
y_vals_test = y_vals[test_indices]
 batch_size = 50
x_data = tf.placeholder(shape=[None, 1], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)
A = tf.Variable(tf.random_normal(shape=[1,1]))
b = tf.Variable(tf.random_normal(shape=[1,1]))
model_output = tf.add(tf.matmul(x_data, A), b)
 epsilon = tf.constant([0.5])
loss = tf.reduce_mean(tf.maximum(0., tf.subtract(tf.abs(tf.subtract(model_
output, y_target)), epsilon)))
 my_opt = tf.train.GradientDescentOptimizer(0.075)
train_step = my_opt.minimize(loss)
init = tf.initialize_all_variables()
sess.run(init)
 train_loss = []
test_loss = []
for i in range(200):
    rand_index = np.random.choice(len(x_vals_train), size=batch_
    size)
    X = np.transpose([x_vals_train[rand_index]])
    Y = np.transpose([y_vals_train[rand_index]])
    sess.run(train_step, feed_dict={x_data: X, y_target: Y})
    temp_train_loss = sess.run(loss, feed_dict={x_data:
    np.transpose([x_vals_train]), y_target: np.transpose([y_vals_train])})
    train_loss.append(temp_train_loss)
    temp_test_loss = sess.run(loss, feed_dict={x_data:np.transpose([x_vals_test]), y_target: np.transpose([y_vals_test])})
    test_loss.append(temp_test_loss)
 (x_vals, y_vals) = datasets.make_circles(n_samples=500, factor=.5,noise=.1)
y_vals = np.array([1 if y==1 else -1 for y in y_vals])
class1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]
class1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]
class2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]
class2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]
 batch_size = 250
x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)
prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)
b = tf.Variable(tf.random_normal(shape=[1,batch_size]))
 gamma = tf.constant(-50.0)
dist = tf.reduce_sum(tf.square(x_data), 1)
dist = tf.reshape(dist, [-1,1])
sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data,tf.transpose(x_data)))), tf.transpose(dist))
my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))
 model_output = tf.matmul(b, my_kernel)
first_term = tf.reduce_sum(b)
b_vec_cross = tf.matmul(tf.transpose(b), b)
y_target_cross = tf.matmul(y_target, tf.transpose(y_target))
second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross,y_target_cross)))
loss = tf.negative(tf.subtract(first_term, second_term))
 rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])
rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])
pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data,tf.transpose(prediction_grid)))), tf.transpose(rB))
pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))
prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b),pred_kernel)
prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))
accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction),tf.squeeze(y_target)), tf.float32))
 my_opt = tf.train.GradientDescentOptimizer(0.001)
train_step = my_opt.minimize(loss)
init = tf.initialize_all_variables()
sess.run(init)
 loss_vec = []
batch_accuracy = []
for i in range(1000):
    rand_index = np.random.choice(len(x_vals), size=batch_size)
    X = x_vals[rand_index]
    Y = np.transpose([y_vals[rand_index]])
    sess.run(train_step, feed_dict={x_data: X, y_target: Y})
    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})
    loss_vec.append(temp_loss)
    acc_temp = sess.run(accuracy, feed_dict={x_data: X,y_target: Y,prediction_grid:X})
    batch_accuracy.append(acc_temp)
 sess = tf.Session()
iris = datasets.load_iris()
x_vals = np.array([[x[0], x[3]] for x in iris.data])
y_vals = np.array([1 if y==0 else -1 for y in iris.target])
class1_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==1]
class1_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==1]
class2_x = [x[0] for i,x in enumerate(x_vals) if y_vals[i]==-1]
class2_y = [x[1] for i,x in enumerate(x_vals) if y_vals[i]==-1]
 batch_size = 100
x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)
y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)
prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)
b = tf.Variable(tf.random_normal(shape=[1,batch_size]))
 gamma = tf.constant(-10.0)
dist = tf.reduce_sum(tf.square(x_data), 1)
dist = tf.reshape(dist, [-1,1])
sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data,tf.transpose(x_data)))), tf.transpose(dist))
my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))
 model_output = tf.matmul(b, my_kernel)
first_term = tf.reduce_sum(b)
b_vec_cross = tf.matmul(tf.transpose(b), b)
y_target_cross = tf.matmul(y_target, tf.transpose(y_target))
second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross,y_target_cross)))
loss = tf.negative(tf.subtract(first_term, second_term))
 rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])
rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])
pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data,
tf.transpose(prediction_grid)))), tf.transpose(rB))
pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))
prediction_output = tf.matmul(tf.multiply(tf.transpose(y_target),b),pred_kernel)
prediction = tf.sign(prediction_output-tf.reduce_mean(prediction_output))
accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.squeeze(prediction),tf.squeeze(y_target)), tf.float32))
 my_opt = tf.train.GradientDescentOptimizer(0.01)
train_step = my_opt.minimize(loss)
init = tf.initialize_all_variables()
sess.run(init)
 loss_vec = []
batch_accuracy = []
for i in range(1000):
    rand_index = np.random.choice(len(x_vals), size=batch_size)
    X = x_vals[rand_index]
    Y = np.transpose([y_vals[rand_index]])
    sess.run(train_step, feed_dict={x_data: X, y_target:Y})
    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})
    loss_vec.append(temp_loss)
    acc_temp = sess.run(accuracy, feed_dict={x_data: X,y_target: Y,prediction_grid:X})
    batch_accuracy.append(acc_temp)
 sess = tf.Session()
iris = datasets.load_iris()
x_vals = np.array([[x[0], x[3]] for x in iris.data])
y_vals1 = np.array([1 if y==0 else -1 for y in iris.target])
y_vals2 = np.array([1 if y==1 else -1 for y in iris.target])
y_vals3 = np.array([1 if y==2 else -1 for y in iris.target])
y_vals = np.array([y_vals1, y_vals2, y_vals3])
class1_x = [x[0] for i,x in enumerate(x_vals) if iris.target[i]==0]
class1_y = [x[1] for i,x in enumerate(x_vals) if iris.target[i]==0]
class2_x = [x[0] for i,x in enumerate(x_vals) if iris.target[i]==1]
class2_y = [x[1] for i,x in enumerate(x_vals) if iris.target[i]==1]
class3_x = [x[0] for i,x in enumerate(x_vals) if iris.target[i]==2]
class3_y = [x[1] for i,x in enumerate(x_vals) if iris.target[i]==2]
 batch_size = 50
x_data = tf.placeholder(shape=[None, 2], dtype=tf.float32)
y_target = tf.placeholder(shape=[3, None], dtype=tf.float32)
prediction_grid = tf.placeholder(shape=[None, 2], dtype=tf.float32)
b = tf.Variable(tf.random_normal(shape=[3,batch_size]))
 gamma = tf.constant(-10.0)
dist = tf.reduce_sum(tf.square(x_data), 1)
dist = tf.reshape(dist, [-1,1])
sq_dists = tf.add(tf.subtract(dist, tf.multiply(2., tf.matmul(x_data,tf.transpose(x_data)))), tf.transpose(dist))
my_kernel = tf.exp(tf.multiply(gamma, tf.abs(sq_dists)))
 def reshape_matmul(mat):
    v1 = tf.expand_dims(mat, 1)
    v2 = tf.reshape(v1, [3, batch_size, 1])
    return(tf.matmul(v2, v1))
 model_output = tf.matmul(b, my_kernel)
first_term = tf.reduce_sum(b)
b_vec_cross = tf.matmul(tf.transpose(b), b)
y_target_cross = reshape_matmul(y_target)
second_term = tf.reduce_sum(tf.multiply(my_kernel, tf.multiply(b_vec_cross,y_target_cross)),[1,2])
loss = tf.reduce_sum(tf.negative(tf.subtract(first_term, second_term)))
 rA = tf.reshape(tf.reduce_sum(tf.square(x_data), 1),[-1,1])
rB = tf.reshape(tf.reduce_sum(tf.square(prediction_grid), 1),[-1,1])
pred_sq_dist = tf.add(tf.subtract(rA, tf.multiply(2., tf.matmul(x_data,tf.transpose(prediction_grid)))), tf.transpose(rB))
pred_kernel = tf.exp(tf.multiply(gamma, tf.abs(pred_sq_dist)))
 prediction_output = tf.matmul(tf.multiply(y_target,b), pred_kernel)
prediction = tf.arg_max(prediction_output-tf.expand_dims(tf.reduce_mean(prediction_output,1), 1), 0)
accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,tf.argmax(y_target,0)), tf.float32))
 my_opt = tf.train.GradientDescentOptimizer(0.01)
train_step = my_opt.minimize(loss)
init = tf.initialize_all_variables()
sess.run(init)
 loss_vec = []
batch_accuracy = []
for i in range(500):
    rand_index = np.random.choice(len(x_vals), size=batch_size)
    X = x_vals[rand_index]
    Y = y_vals[:,rand_index]
    sess.run(train_step, feed_dict={x_data: X, y_target:Y})
    temp_loss = sess.run(loss, feed_dict={x_data: X, y_target: Y})
    loss_vec.append(temp_loss)
    acc_temp = sess.run(accuracy, feed_dict={x_data: X, y_target: Y, prediction_grid:X})
    batch_accuracy.append(acc_temp)
 
 
Item 3123: 
id,action,dt
001,view,2018-01-01
002,click,2018-01-02
 table_name/dt=2018-01-01/data.csv
table_name/df=2018-01-02/data/csv
 id,action
001,view
 
 
Item 3124: 
send_message_to_sqs('gt3',['apple','banana'])
 receive_message_from_sqs('gt3')
 https://github.com/Kiollpt/SQS_scrapying
 
 
Item 3125: 
CountryData["World", {"Shape", #}][[1, 3, 1]] & /@ {"Orthographic", 
   "Mercator"};
Animate[Graphics[{Polygon[%[[1]] + f (%[[2]] - %[[1]])]}], {f, 
  0, .04}]
 SphericalPlot3D[{(Cos[20 t] + 70) (p (Pi - p + .1))^.2, Nothing, 
  If[p < .2, 90 Cos[7 p]]}, {p, 0, Pi}, {t, 0, 2 Pi}, Mesh -> None, 
 PlotPoints -> 80, Axes -> 0]
 Dynamic@Quiet@
  Check[{{{a, b}, {c, d}}} = FindFaces[i = CurrentImage[]]; 
   HighlightImage[i, {Black, PointSize[(d - b)/400], {a, d}, {c, d}}], i]
 
 
Item 3127: 
time_of_match
team_name
season
red_positions
blue_positions
etc...
 blue_result
game_length
gold_difference
kills
towers
inhibitors
dragons
barons
heralds
champions
 
 
Item 3135: 
{
  "MarkdownCell": {
    "cm_config": {
      "lineWrapping": true
    }
  },
  "CodeCell": {
    "cm_config": {
      "lineWrapping": true
    }
  }
}
 
 
Item 3137: 
What is the 'Uncanny Valley' theory? 
Forward this email to friends who'd rather talk to Siri than you

In an essay written in 1970, Masahiro Mori, a robotics professor at the Tokyo Institute of Technology suggested that people develop affinity towards robots as they become more human-like.

However, as the robot reaches near-human likeness, the graph measuring respondents' liking of the robot sharply dips to revulsion. This dip is known as the 'Uncanny Valley'.
 
 
Item 3139: 
button = Button(button_pin)
button.when_pressed = callback_function
 
 
Item 3140: 
p = np.array([(x - mu).dot(inv).dot(x - mu) for x in X])
 for x, ri in zip(X - mu_next, r):
    s += (np.outer(x, x) * ri)
 p = np.einsum('ni,ji,ni->n', X - mu, inv, X - mu)
 sigma_next = np.einsum("ni,nj->ij", (X - mu_next) * r[:, np.newaxis], X - mu_next) / N
 
 
Item 3143: 
tensorflow_model_server --port=9000 --model_name=hello_world --model_base_path=$(pwd)/hello_world
 $python tf_client_hello_world.py
outputs {
  key: "ans"
  value {
    dtype: DT_INT32
    tensor_shape {
    }
    int_val: 30
  }
}
 
 
Item 3146: 
Your contribution will be highly appreciated on Patreon. 
 Examples of where unsupervised learning algorithms applies:
 - An advertising platform segments the U.S. population into smaller groups with similar demographics and purchasing habits so that advertisers can reach their target market with relevant ads.
 - Airbnb groups its housing listings into neighborhoods so that users can navigate listings more easily.
 - A data science team reduces the number of dimensions in a large data set to simplify modeling and reduce file size.
 Why to use k-means clustering:
 Strengths: – Simple: easy to understand and to implement – Efficient: Time complexity: O(tkn), where n is the number of data points, k is the number of clusters, and t is the number of iterations. – Since both k and t are small. 
k-means is considered a linear algorithm. 
K-means is the most popular clustering algorithm
 Steps to Hierarchical clustering:
 1. Assign each data point to its own cluster.
 2. Find closest pair of cluster using euclidean distance and merge them in to single cluster.
 3. Calculate distance between two nearest clusters and combine until all items are clustered in to a single cluster.
 
 
Item 3152: 
And for us to be able to make sure that they've got that opportunity and they can walk through that door, that is vitally important not just to those kids. It's how we're going to grow this economy over the long term. You've done a great job. Well, first of all, I think Governor Romney's going to have a busy first day, because he's also going to repeal Obamacare, which will not be very popular among Democrats as you're sitting down with them. But, look, my philosophy has been, I will take ideas from anybody, Democrat or Republican, as long as they're advancing the cause of making middle-class families stronger and giving ladders of opportunity to the middle class. That's how we cut taxes for middle- class families and small businesses. That's how we cut a trillion dollars of spending that wasn't advancing that cause. That's how we signed three trade deals into law that are helping us to double our exports and sell
more American products around the world. That's how we repealed "don't ask/don't tell." 
 
 
Item 3159: 
git clone https://github.com/waslleysouza/tf4poets-with-oracle-cloud
 jupyter notebook tf4poets-with-oracle-cloud.ipynb
 
 
Item 3160: 
sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False,tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)
 
 
Item 3163: 
!curl -O https://raw.githubusercontent.com/mikulskibartosz/to_be_imported/master/to_be_imported.ipynb
%run to_be_imported.ipynb
 
 
Item 3164: 
{
  "intents": [
    {
      "slots": [
        {
          "name": "sentence",
          "type": "AMAZON.LITERAL"
        }
      ],
      "intent": "SaySomething"
    }
  ]
}
 SaySomething {do something | sentence}
 ngrok http 3000
 REQUEST_TOKEN=xxxxxxxxxx node server.js.
 
 
Item 3166: 
def setup():
    f=open('Lyrics-about-NYC.txt','r')
    print(f.read())
def draw():
    pass
 def setup():
    
    count=0
    count2=0
    count3=0
     f=open('Lyrics-about-NYC.txt','r')
    myFileLines=f.readlines()
    for line in myFileLines:
        if " he " in line:
            count=count+1
        if " she " in line:
            count2 = count2+1
        if "New York City" in line:
            count3 = count3+1
    
    print("he: " + str(count))
    print("she: " + str(count2))
    print("New York City: " + str(count3))
    f.close()
 def draw():
    pass
 
 
Item 3167: 
{ Brand: “HP”, 
   MPN: “15-F222WM”,
   screensize: “15.6in”,
   processortype: “Intel i5”,
   rammemory : “4GB”,
   capacity: “500GB”,…}
 
 
Item 3174: 
+----+-------+
|  X |  f(X) |
+----+-------+
|  2 |    10 |
|  4 |   100 |
|  6 |   110 |
|  8 |  1000 |
| 10 |  1010 |
| 12 |  1100 |
| 16 | 10000 |
+----+-------+
 
 
Item 3181: 
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]
                         one hot encoding for some word e.g - hotel
 
 
Item 3186: 
model = applications.VGG19(weights = "imagenet", include_top=False, input_shape = (img_width, img_height, 3))
 
 
Item 3188: 
API: https://<your-project-name>--api.rorocloud.io/
APP: https://<your-project-name>.rorocloud.io/
 
 
Item 3194: 
Mann-Whitney U test
-------------------------------------------------------
Data scineintist median of salary: 78300.0
Database administrator median of salary: 70000.0
-------------------------------------------------------
P-value: 0.0000
Statistical significant is True under significance level of 5%
 
 
Item 3196: 
User: Hello
Bot:  Hello, How may I help you today
User: I am looking to buy a house
Bot:  Sure, I can help you with that. Tell me what are your requirements.
User: I am looking to buy home in Charlotte.
Bot:  Do you have a price range in mind?
User: My budget is $200K
Bot:  And in what area are you looking to buy?
User: I am looking for house in South Charlotte area?
Bot:  I couldn't find any house range which matches your requirements. Are you flexible on price range?
User: I can go a little higher maybe $300k
Bot:  I found this 3 listings.
User: Can you narrow down result to filter by parking available.
Bot:  This 2 listings have parking in them.
User: Can you check same house requirements but in North Davidson area
Bot:  Sure I found these 6 listing
User: I didn't like any. Lets go back to South Charlotte and I can increase my budget.
Bot:  Sure I found these 8 listings.
User: Can you only show me the one that have fireplace in them.
Bot:  This 3 listings have fireplace in them.
User: I like the first one.
Bot:  This is the MLS# for the house. You can use this number to locate the house on our website. Would you like to book appointment with agent to see the house?
User: I have saved the MlS#. I am done. Thanks.  
 Bot:  I couldn't find any house range which matches your requirements. Are you flexible on price range?
User: I can go a little higher maybe $300k
 Bot: These are all the house listing for renting that closely match your query
User: Hmm, I don't like any of this. Let's go back to buying
 Bot:  Sure I found these 6 listing
User: I didn't like any. Lets go back to South Charlotte and I can increase my budget.
 
 
Item 3200: 
Note: The challenge of tackling Relevance at Scale with noisy ugc data is largely generalized for the sake of blog post
 In Short: When indexed, comments as searchable meta data has the potential to cause significantly more false positives than true positives. In other words increases recall many times over precision besides more importantly slowing down search performance overall.
 amazing weight loss
wedding transformation
keto weight loss
..{more}..
 Note: With bag-of-words model since its a decomposition of text that ignores the context of these terms we won’t be able to do phrase-matching. (You can force all terms to be present to be considered a match but still its not quite the same)
 
 
Item 3201: 
Day Campaign Device Clicks Impressions Cost Conversions Platform
 
 
Item 3205: 
whois equ1fax.com
 
 
Item 3209: 
#include 
 
int main()
{
    std::cout << "Hello, world!";
    return 0;
}
 console.log('Hello, world!');
 print ("Hello, world!")
 
 
Item 3213: 
npm i -g serverless@v1.27.3
 conda create -n py36 python=3.6
 conda activate py36
 sls plugin install -n serverless-python-requirements
 [default] 
aws_access_key_id=AKIAIOSFODNN7EXAMPLE aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
 sls deploy -v
 AWS_PROFILE=default sls invoke local -f predict -p tests/predict_event.json
 https://js7fdgr3.execute-api.eu-central-1.amazonaws.com/dev/v0.0.1/predict?image_url=http://www.leopardsafari.com/cache/mb2content/106_15_leopard-safari-at-wilpattu-national-park-sri-lanka_480_380.jpg
 
 
Item 3214: 
conda install -c conda-forge pymapd
 conda install -c gpuopenanalytics/label/dev pygdf
 
 
Item 3215: 
@backoff.on_exception(backoff.expo,
                      requests.exceptions.RequestException,
                      max_tries=8)
def get_url(url):
    return requests.get(url)
 
 
Item 3218: 
def create_regularized_lstm_cell(nb_hidden, keep_prob) :
    return tf.contrib.rnn.DropoutWrapper(
                        tf.contrib.rnn.BasicLSTMCell(nb_hidden),
                        output_keep_prob = keep_prop,
                        state_keep_prob = 1.0
                                        )
 self.embedding = 
                tf.Variable(np.vstack(embedding_model.embeddings),
                            dtype = tf.float32,
                            trainable = False,
                            name = "sf_embedding"
                           )  
            
self.embedding_lookup = tf.nn.embedding_lookup(
                                         self.embedding,
                                         self.input_x,
                                         name = "embedding_lookup"
                                              )
 self.unstacked_embedding = tf.unstack(self.embedding_lookup,
                                     num = 150,
                                     axis = 1,
                                     name = "unstacked_embeddings"
                                     )
 self.lstm_cells = [create_regularized_lstm_cell(nb_hidden_lstm, keep_prob) for _ in range(nb_layers)]
 self.multi_lstm_layer = 
                      tf.contrib.rnn.MultiRNNCell(self.lstm_cells)
 self.lstm_cells = create_regularized_lstm_cell(nb_hidden_lstm,
                                               keep_prob)
self.multi_lstm_layer = 
        tf.contrib.rnn.MultiRNNCell([self.lstm_cells]*nb_layers)
 self.lstm_outputs, self.lstm_states = 
                  tf.contrib.rnn.static_rnn(self.multi_lstm_layer,
                                         self.unstacked_embedding, 
                                         dtype = tf.float32
                                           )
 self.W = tf.Variable(tf.random_normal(stddev = 0.1,
                                     shape = (nb_hidden_lstm, 1)),
                     name = "W",
                     dtype = tf.float32
                    )
self.b = tf.Variable(tf.random_normal(stddev = 0.1, shape = [1]),
                     name = "b",
                     dtype = tf.float32
                    )
 self.output = tf.matmul(self.lstm_outputs[-1], self.W) + self.b
 #defining loss
self.loss = tf.reduce_mean(tf.square(self.input_y - self.output),
                           name = "loss")
 def set_optimizer(self, learning_rate = 0.0001,
                  decay_rate = 0.9999) :
        
        self.global_step = tf.Variable(0, trainable = False)
        
        evolutive_lr = tf.train.exponential_decay(
                                   learning_rate = learning_rate,
                                   global_step = self.global_step,
                                   decay_rate = decay_rate,
                                   decay_steps = 1
                                                 )
        
        self.optimizer = tf.train.RMSPropOptimizer(
                                 evolutive_lr, name = "optimizer")
        
        self.train_op = self.optimizer.minimize(
                        self.loss, global_step = self.global_step)
 
 
Item 3221: 
<div>
    <span style=”font-size: 200%;”>H</span>
 <span>ello<span>
 </div>
 
 
Item 3227: 
flask = "*"
 
 
Item 3228: 
cuisine_1,id_1,[ingredient_1, ingredient_2, ingredient_3]
cuisine_2,id_2,[ingredient_4, ingredient_5]
 cuisine_1,id_1,ingredient_1
cuisine_1,id_1,ingredient_2
cuisine_1,id_1,ingredient_3
cuisine_2,id_2,ingredient_4
cuisine_2,id_2,ingredient_5
 ingredients = []
cuisines = []
ids = []
for _, row in data.iterrows():
    cuisine = row.cuisine
    identifier = row.id
    for ingredient in row.ingredients:
        cuisines.append(cuisine)
        ingredients.append(ingredient)
        ids.append(identifier)
 ingredient_to_cuisine = pd.DataFrame({
    "id": ids,
    "ingredient": ingredients,
    "cuisine": cuisines
})
 data.ingredients.apply(pd.Series)
 data.ingredients.apply(pd.Series) \
    .merge(data, left_index = True, right_index = True)
 data.ingredients.apply(pd.Series) \
    .merge(data, right_index = True, left_index = True) \
    .drop(["ingredients"], axis = 1)
 data.ingredients.apply(pd.Series) \
    .merge(data, right_index = True, left_index = True) \
    .drop(["ingredients"], axis = 1) \
    .melt(id_vars = ['cuisine', 'id'], value_name = "ingredient")
 data.ingredients.apply(pd.Series) \
    .merge(data, right_index = True, left_index = True) \
    .drop(["ingredients"], axis = 1) \
    .melt(id_vars = ['cuisine', 'id'], value_name = "ingredient") \
    .drop("variable", axis = 1)
 data.ingredients.apply(pd.Series) \
    .merge(data, right_index = True, left_index = True) \
    .drop(["ingredients"], axis = 1) \
    .melt(id_vars = ['cuisine', 'id'], value_name = "ingredient") \
    .drop("variable", axis = 1) \
    .dropna()
 
 
Item 3231: 
Magic power: UX ninjas can read your mind.
 Magic power: Data Science ninjas can predict the future.
 Magic power: Technology ninjas can transform objects
 
 
Item 3234: 
a=3
 b=5
 sumAB=a+b
 print (sumAB)
 
 
Item 3236: 
+---------+-------------+----------+
|  house  | square foot | Price($) |
+---------+-------------+----------+
| House 1 |          10 |     1000 |
| House 2 |          20 |     2000 |
| House 3 |          30 |     3000 |
| House 4 |          40 |     3000 |
| House 5 |          50 |     5000 |
+---------+-------------+----------+
 +---------+-------------+--------------------+---------------+
|  house  | square foot | Price($) predicted | Price($) real |
+---------+-------------+--------------------+---------------+
| House 1 |          12 |               1150 |          1200 |
| House 2 |          15 |               1500 |          1500 |
| House 3 |          17 |               1680 |          1700 |
+---------+-------------+--------------------+---------------+
 
 
Item 3237: 
CHAT WITH US AND BE ONE STEP BEYOND
 USE BLiP AND LEVEL UP YOUR CHATBOTS
 
 
Item 3240: 
def get_similar(raw_title, lsh, num_perm=MINHASH_NUM_PERM):
    mq = MinHash(num_perm=num_perm)
    for d in raw_title.split():
        mq.update(d.encode('utf8'))

    result = model.query(mq)
    return result
 {
  "results": [
    {
      "model_id": "974ca263-af59-4c99-89df-4846396a63d6",
      "uuid": "872487d1-81f7-4ca8-bae1-9e8a1238255c",
      "score": "100",
      "canonical_title": "freelance data scientist"
    },
    {
      "model_id": "974ca263-af59-4c99-89df-4846396a63d6",
      "uuid": "a743280b-6de3-45b7-90e1-d38b24708cf5",
      "score": "100",
      "canonical_title": "data scientist"
    },
    {
      "model_id": "974ca263-af59-4c99-89df-4846396a63d6",
      "uuid": "b430b730-6a70-4eb1-962f-13188a1a835e",
      "score": "100",
      "canonical_title": "big data scientist"
    }
  ],
  "title": "data scientist"
}
 
 
Item 3241: 
=WEEKNUM( serial_number, [return_type] )
 =WEEKNUM(A1:A25+0)
 =SUMPRODUCT((WEEKNUM(Table13[Order Date]+0)=[@[Week No.]])*Table13[Sales])
 {34;23;23;45;45;24;24;35;16;16;23;35;35;45;23;45;35;34;35}
 {TRUE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;TRUE;FALSE}
 {18.504;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;0;68.81;0}
 {TRUE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;TRUE;FALSE}
 {18.504;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;68.81;FALSE}
 =SUM({18.504;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;FALSE;68.81;FALSE})=87.314
 
 
Item 3243: 
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomSizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Scale(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}
 
 
Item 3244: 
=IFERROR(SUMIFS(Clicks,Devices,Comp,Day,”=”&Yest,Labels,”*”&USA&”*”)/SUMIFS(Clicks,Devices,Comp,Day,”=”&Yestp,Labels,”*”&USA&”*”)-1,”NA”)
 
 
Item 3246: 
predict_linear(btc_usd[6h], 900)
 
 
Item 3247: 
imp = IJ.getImage()
width = imp.getWidth()
height = imp.getHeight()
 print(“”)
print(“title:”, imp.getTitle())
print(“width:”, width)
print(“height:”, height)
print(“stack size:”, imp.getStackSize())
 bitDepth = imp.getBitDepth()
type = “RGB”
if bitDepth != 24:
 type = str(bitDepth)+”-bit”
print(“type:”, type)
 composite = imp.isComposite()
hyperstack = imp.isHyperStack()
print(“composite:”, composite)
print(“hyperstack:”, hyperstack)
if composite or hyperstack:
 print(“channels:”, imp.getNChannels())
 print(“slices:”, imp.getNSlices())
 print(“frames:”, imp.getNFrames())
 cal = imp.getCalibration()
scaled = cal.scaled()
print(“scaled:”, scaled)
if scaled:
 units = cal.units
 print(“pixel width:”, cal.pixelWidth)
 print(“pixel height:”, cal.pixelHeight)
 print(“pixel depth:”, cal.pixelDepth)
 width = height = 1024
ip = ShortProcessor(width,height)
for y in range(height):
 IJ.showProgress(y,height-1)
 for x in range(width):
 ip.set(x, y, y)
imp = ImagePlus(“Synthetic Image”, ip)
imp.show()
 imp = IJ.getImage()
ip = imp.getProcessor()
width = ip.getWidth()
height = ip.getHeight()
stats = ip.getStatistics()
for y in range(height):
 IJ.showProgress(y,height-1)
 for x in range(width):
 v = ip.getf(x,y)
 ip.setf(x, y, stats.max-(v-stats.min))
imp.updateAndDraw()
 imp = IJ.getImage();
ip = imp.getProcessor();
filter = RankFilters();
radius = 5;
filter.rank(ip, radius, RankFilters.MEDIAN);
imp.updateAndDraw();
 img = IJ.getImage();
ip = img.getProcessor();
filter = new RankFilters();
radius = 5;
filter.rank(ip, radius, RankFilters.MEDIAN);
img.updateAndDraw();
 n = getNumber(“Stack Size:”, 10);
run(“Copy”);
for (i=0; i<n-1; i++) {
 run(“Add Slice”);
 run(“Paste”);
}
 
 
Item 3249: 
"type": "neutral",
  "score": -0.0020747652951513,
  "ratio": 0.035473437650892,
 "keyword": {
    "fight": 73,
    "going": 65,
    "right": 36,
    "know": 30,
    "god": 23,
    "just": 21,
    "life": 19,
    "today": 18,
    "okay": 18,
    "scripture": 16
  },
  "topic": {
    "work": 0.096569793866802,
    "knowledge": 0.077255835093442,
    "book": 0.077255835093442,
    "time today": 0.071783546774323,
    "art": 0.070817848835655,
    "care": 0.069208352271208,
    "talk": 0.065989359142315,
    "animal": 0.065989359142315,
    "money": 0.064379862577868,
    "feeling": 0.064379862577868
 "type": "positive",
  "score": 0.086640381742745,
  "ratio": 0.56208451044419
 "keyword": {
    "going": 2,
    "passage": 2,
    "memorize": 2,
    "create": 2,
    "pray": 2,
    "spirit": 2,
    "resource": 1,
    "fight": 1,
    "coming": 1,
    "weekend": 1
 "topic": {
    "religion": 0.22884012539185,
    "god": 0.22884012539185,
    "mind": 0.22884012539185,
    "prayer": 0.1961486789073,
    "deity": 0.1961486789073,
    "church": 0.1961486789073,
    "divinity": 0.1961486789073,
    "spirit": 0.1961486789073,
    "pantheon": 0.1961486789073,
    "opinion": 0.16345723242275
 
 
Item 3250: 
23 + 11
 0 + 0 = 1
 0 + 1 = 1
 1 + 1 = 10
 111 + 110 = ???
 0100 + 1101 = 001
 as we ignore the carry again.
 0100 + 0101 + 1001
 
 
Item 3251: 
{"intents": [
        {"tag": "greeting",
         "patterns": ["Hi", "How are you", "Is anyone there?", "Hello", "Good day"],
         "responses": ["Hello, thanks for visiting", "Good to see you again", "Hi there, how can I help?"]
        },
        {"tag": "goodbye",
         "patterns": ["Bye", "See you later", "Goodbye"],
         "responses": ["See you later, thanks for visiting", "Have a nice day", "Bye! Come back again soon"]
        },
        {"tag": "thanks",
         "patterns": ["Thanks", "Thank you", "That's helpful"],
         "responses": ["Happy to help!", "Any time!", "My pleasure"]
        }
   ]
}
 curl -X POST -F "id=<your_user_token_id>" -F "data=@/<your_local_machine_location>intents.json" https://deepagi.com/train
 
 
Item 3253: 
toxic               15294
severe_toxic         1595
obscene              8449
threat                478
insult               7877
identity_hate        1405
 
 
Item 3257: 
7000059 Kathleen Kennedy
 1003047 Bill Murray
 1000821 Tom Hanks
 8000160 The Agent
 5000018 Music
 8000955 holes
 6000011 1940
 6000012 2010
 5000005 Kids
 3000026 James Newton Howard
>
 Revenue	Mean	SD	Rating	X1
0.196	3.921	0.506	3.5	3.679
0.101	3.834	0.501	4.5	3.749
0.190	3.430	0.489	3	3.002
0.000	2.619	0.639	2.5	2.245
0.010	2.721	0.617	2	2.369
0.062	3.787	0.491	4	3.759
0.002	3.373	0.586	2.5	3.300
0.115	2.607	0.655	1.5	1.997
0.000	3.319	0.524	3.5	3.227
0.091	3.898	0.493	4.5	3.854
0.177	4.053	0.495	4	3.903
0.012	4.334	0.430	4.5	4.623
 
 
Item 3259: 
cd
mkdir server
 cd
cd server
tar xzvf spark-2.3.0-bin-hadoop2.7
tar xzvf sbt-0.13.17.tgz
tar xzvf scala-2.11.12
 cd
touch -c .bash_profile
open -a TextEdit .bash_profile
 export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_162.jdk/Contents/Home/
export SPARK_HOME=/Users/luckspark/server/spark-2.3.0-bin-hadoop2.7
export SBT_HOME=/Users/luckspark/server/sbt
export SCALA_HOME=/Users/luckspark/server/scala-2.11.12
 export PATH=$JAVA_HOME/bin:$SBT_HOME/bin:$SBT_HOME/lib:$SCALA_HOME/bin:$SCALA_HOME/lib:$PATH
 export PATH=$JAVA_HOME/bin:$SPARK_HOME:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
 export PYSPARK_PYTHON=python3
 PATH=”/Library/Frameworks/Python.framework/Versions/3.6/bin:${PATH}”
export PATH
 a=5
b=3
a+b
 val a = 5
val b = 3
a+b
 
 
Item 3261: 
sgd_series['Returns1'] = sgd_series['High']/sgd_series['Low'].shift(1)-1
 sgd_series['Volatility252_1'] = pd.rolling_std(sgd_series['Returns1'], window=252)
 sgd_series['99Volatility252_1'] = sgd_series['Volatility252_1']*2.33
 
 
Item 3263: 
> x <-1
> y <- x+2
> z <- y + x
> x
[1] 1
> y
[1] 3
> z
[1] 4
 > x <- 1; y <- x+2; z <- y + x
> x; y; z
[1] 1
[1] 3
[1] 4
 > {x <- 1; y <- x+2; z <- y + x; z}
[1] 4
 ?Control
 > if (T) c(1,2)
[1] 1 2
> if (F) c(1,2)
 > if (T)  c(1,2,3) else matrix(c(1,2,3, 4), nrow=2)
[1] 1 2 3
> if (F)  c(1,2,3) else matrix(c(1,2,3, 4), nrow=2)
     [,1] [,2]
[1,]    1    3
[2,]    2    4
 > v1 <- c(1,2,3,4)
> v2 <- c(5,6,7,8)
> cond <- c(T,F,F,T)
> ifelse(cond, v1, v2)
[1] 1 6 7 4
 > T && F
[1] FALSE
> T || F
[1] TRUE
 > v1 <- c(T,T,F,F)
> v2 <- c(T, F, T, F)
> v1 | v2
[1]  TRUE  TRUE  TRUE FALSE
> v1 & v2
[1]  TRUE FALSE FALSE FALSE
 > x <- 10
> repeat { if (x == 0) break ; x = x - 1; print(x)}
[1] 9
[1] 8
[1] 7
[1] 6
[1] 5
[1] 4
[1] 3
[1] 2
[1] 1
[1] 0
 > for (i in seq(1,10)) print(i)
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
> for (i in seq(1,10, by=2)) print(i)
[1] 1
[1] 3
[1] 5
[1] 7
[1] 9
 > for (i in seq(1,10)) i
>
 ul <- rnorm(30)
usq <- 0
for (i in 1:10){
        usq <- ul[i] * ul[i]
}
 nrow <- 10
ncol <- 10
m <- matrix(nrow=nrow, ncol=ncol)
 for (i in 1:nrow){
        for (j in 1:ncol){
                m[i, j] <- i + j
        }
}
 > i <- 10; while ( i < 20 ) {i <- i +1; print(i)}
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
 > i <- 10; while (T) {i <- i +1; if (i == 20) break; if ( i %% 2 == 0) next; print(i);}
[1] 11
[1] 13
[1] 15
[1] 17
[1] 19
 > install.packages('iterators')
 > library(iterators)
 > ii <- iter(1:4)
 > nextElem(ii)
[1] 1
> nextElem(ii)
[1] 2
> nextElem(ii)
[1] 3
> nextElem(ii)
[1] 4
> nextElem(ii)
Error: StopIteration
 > ii <- iter(1:4, recycle = T)
> for (i in 1:10) print(nextElem(ii))
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 2
[1] 3
[1] 4
[1] 1
[1] 2
 > install.packages('foreach')
 > library(foreach)
 > unlist(foreach(i=1:10) %do% {100 * (1 + i/100)^5})
 [1] 105.1010 110.4081 115.9274 121.6653 127.6282 133.8226 140.2552 146.9328 153.8624 161.0510
 > unlist(foreach(i=iter(1:10)) %do% {100 * (1 + i/100)^5})
 [1] 105.1010 110.4081 115.9274 121.6653 127.6282 133.8226 140.2552 146.9328 153.8624 161.0510
 > b = c(2,3,5)
> m = mean(x=b)
> s = sum(c(4,5,8,11))
 > x <- c(rnorm(10000))
> sum((x-mean(x))^2)/(length(x)-1)
[1] 0.992163
 function_name <- function (arglist){
        body
}
 my_mean <- function(x){
   s <- sum(x)
   n <- length(x)
   s / n
}
 > my_mean(rivers)
[1] 591.1844
 > mean(rivers)
[1] 591.1844
 log_sum_exp <- function(x){
  xx <- exp(x)
  xxx <- sum(xx)
  log(xxx)
}
 > source('my_functions.R')
 > log_sum_exp(10)
[1] 10
> log_sum_exp(c(10, 12))
[1] 12.12693
> log_sum_exp(sample(1:100, 100, replace=T))
[1] 100.4429
 hanoi <- function(num_disks, from, to, via, disk_num=num_disks){
        if (num_disks == 1){
                cat("move disk", disk_num,  "from ", from, "to", to, "\n")
        }else{
        hanoi(num_disks-1, from, via, to)
                hanoi(1, from, to, via, disk_num)
                hanoi(num_disks-1, via, to, from)
        }
}
 > hanoi(1,'a', 'b', 'c')
move disk 1 from  a to b
> hanoi(2,'a', 'b', 'c')
move disk 1 from  a to c
move disk 2 from  a to b
move disk 1 from  c to b
> hanoi(3,'a', 'b', 'c')
move disk 1 from  a to b
move disk 2 from  a to c
move disk 1 from  b to c
move disk 3 from  a to b
move disk 1 from  c to a
move disk 2 from  c to b
move disk 1 from  a to b
 fourth_power <- function(n){
  sq <- function() n* n
  sq() * sq()
}
 > fourth_power(2)
[1] 16
> fourth_power(3)
[1] 81
 counter <- function(n){
  list(
    increase = function(){
      n <<- n+1
    },
    decrease = function(){
      n <<- n-1
    },
    value = function(){
      n
    }
  )
}
 > v <- counter(10)
> v$value()
[1] 10
 > v$increase()
> v$increase()
> v$value()
[1] 12
> v$decrease()
> v$decrease()
> v$value()
[1] 10
 > .libPaths()
[1] "C:/Users/Shailesh/R/win-library/3.4" "C:/Program Files/R/R-3.4.2/library"
 > library()
 > install.packages("geometry")
 > library("geometry")
 > if(!require(psych)){install.packages("psych")}
 > search()
 [1] ".GlobalEnv"        "package:foreach"   "package:iterators" "package:MASS"
 [5] "package:ggplot2"   "package:e1071"     "tools:rstudio"     "package:stats"
 [9] "package:graphics"  "package:grDevices" "package:utils"     "package:datasets"
[13] "package:methods"   "Autoloads"         "package:base"
 > loadedNamespaces()
 [1] "Rcpp"       "codetools"  "grDevices"  "class"      "foreach"    "MASS"
 [7] "grid"       "plyr"       "gtable"     "e1071"      "datasets"   "scales"
[13] "ggplot2"    "rlang"      "utils"      "lazyeval"   "graphics"   "base"
[19] "labeling"   "iterators"  "tools"      "munsell"    "compiler"   "stats"
[25] "colorspace" "methods"    "tibble"
 > source("foo.R")
 > x <- c(1, 4, NA, 5, 0/0)
> is.na(x)
[1] FALSE FALSE  TRUE FALSE  TRUE
 > is.nan(x)
[1] FALSE FALSE FALSE FALSE  TRUE
 > is.vector(1:3)
[1] TRUE
> is.vector("133")
[1] TRUE
> is.vector(matrix(1:4, nrow=2))
[1] FALSE
 > is.matrix(1:3)
[1] FALSE
> is.matrix(matrix(1:4, nrow=2))
[1] TRUE
 > x <- 1:10
> mode(x)
[1] "numeric"
 > class(x)
[1] "integer"
 > typeof(x)
[1] "integer"
 > length(x)
[1] 10
 > l <- list(1, '2', 3.4, TRUE)
> mode(l)
[1] "list"
 > mode(l[1])
[1] "list"
 > mode(l[[1]])
[1] "numeric"
> mode(l[[2]])
[1] "character"
 > l <- list("1", 2, TRUE, NA)
> attributes(l)
NULL
 > attr(l, 'color') <- 'red'
> attributes(l)
$color
[1] "red"
 > attr(l, 'color')
[1] "red"
 > class(1:10)
[1] "integer"
> class(matrix(1:10, nrow=2))
[1] "matrix"
> class(list(1,2,3))
[1] "list"
 > unclass(object)
 > as.character(10:14)
[1] "10" "11" "12" "13" "14"
 > as.integer(c("10", "11", "12", "13"))
[1] 10 11 12 13
 > as.vector(arr)
 > which (v == 5)
[1] 5
> which (v > 5)
[1]  6  7  8  9 10
> which (v > 5 & v < 8)
[1] 6 7
 > m <- matrix(1:10, nrow=2)
> m == 4
      [,1]  [,2]  [,3]  [,4]  [,5]
[1,] FALSE FALSE FALSE FALSE FALSE
[2,] FALSE  TRUE FALSE FALSE FALSE
> which(m == 4)
[1] 4
 > x = sample(1:10)
> x
 [1]  6  5  8 10  2  4  1  3  7  9
> sort(x)
 [1]  1  2  3  4  5  6  7  8  9 10
 > v <- c(1, 4, 4, 3, 4, 4, 3, 3, 1, 2, 3, 4, 2, 3, 1, 3, 5, 6)
> unique(v)
[1] 1 4 3 2 5 6
 > theta = pi/2
> sin(theta)
[1] 1
> cos(theta)
[1] 6.123032e-17
> tan(theta)
[1] 1.633124e+16
> asin(1)
[1] 1.570796
> acos(1)
[1] 0
> atan(1)
[1] 0.7853982
> atan(1) * 2
[1] 1.570796
 > exp(1)
[1] 2.718282
 > log(exp(1))
[1] 1
> log(exp(4))
[1] 4
> log10(10^4)
[1] 4
> log2(8)
[1] 3
> log2(c(8,16,256,1024, 2048))
[1]  3  4  8 10 11
 > sqrt(4)
[1] 2
> sqrt(-4)
[1] NaN
Warning message:
In sqrt(-4) : NaNs produced
> sqrt(-4+0i)
[1] 0+2i
 > pi
[1] 3.141593
>
 > month.name
 [1] "January"   "February"  "March"     "April"     "May"       "June"      "July"      "August"
 [9] "September" "October"   "November"  "December"
 > month.abb
 [1] "Jan" "Feb" "Mar" "Apr" "May" "Jun" "Jul" "Aug" "Sep" "Oct" "Nov" "Dec"
 > letters
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s" "t" "u" "v" "w" "x" "y" "z"
> LETTERS
 [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S" "T" "U" "V" "W" "X" "Y" "Z"
 > data <- sample(0:20, 10, replace = TRUE)
> data
 [1] 10  0 20  3 13 13 16  2  1 10
> cut (data, breaks=4)
 [1] (5,10]    (-0.02,5] (15,20]   (-0.02,5] (10,15]   (10,15]   (15,20]   (-0.02,5] (-0.02,5] (5,10]
Levels: (-0.02,5] (5,10] (10,15] (15,20]
 > table(cut (data, breaks=4))
 (-0.02,5]    (5,10]   (10,15]   (15,20]
        4         2         2         2
 > cut (data, breaks=4, ordered_result = TRUE)
 [1] (5,10]    (-0.02,5] (15,20]   (-0.02,5] (10,15]   (10,15]   (15,20]   (-0.02,5] (-0.02,5] (5,10]
Levels: (-0.02,5] < (5,10] < (10,15] < (15,20]
 > cut (data, breaks=4, labels=c("a", "b", "c", "d"))
 [1] b a d a c c d a a b
Levels: a b c d
 > cut (data, breaks=c(-1, 5,10, 20))
 [1] (5,10]  (-1,5]  (10,20] (-1,5]  (10,20] (10,20] (10,20] (-1,5]  (-1,5]  (5,10]
Levels: (-1,5] (5,10] (10,20]
 > cut (data, breaks=c(0, 5,10, 20), include.lowest = TRUE)
 [1] (5,10]  [0,5]   (10,20] [0,5]   (10,20] (10,20] (10,20] [0,5]   [0,5]   (5,10]
Levels: [0,5] (5,10] (10,20]
 > m <- matrix(1:8, nrow=2)
> m
     [,1] [,2] [,3] [,4]
[1,]    1    3    5    7
[2,]    2    4    6    8
 > apply(m, 1, sum)
[1] 16 20
 > apply(m, 2, sum)
[1]  3  7 11 15
 > apply(m, 1, median)
[1] 4 5
> apply(m, 2, median)
[1] 1.5 3.5 5.5 7.5
 > A <- matrix(c(1,1,1,3,0,2), nrow=3)
> B <- matrix(c(0,7,2,0,5,1), nrow=3)
> l <- list(A, B)
> l
[[1]]
     [,1] [,2]
[1,]    1    3
[2,]    1    0
[3,]    1    2
 [[2]]
     [,1] [,2]
[1,]    0    0
[2,]    7    5
[3,]    2    1
 > lapply(l, '[', 1,)
[[1]]
[1] 1 3
 [[2]]
[1] 0 0
 > lapply(l, '[', , 2)
[[1]]
[1] 3 0 2
 [[2]]
[1] 0 5 1
 > lapply(l, '[', 1,2)
[[1]]
[1] 3
 [[2]]
[1] 0
> unlist(lapply(l, '[', 1,2))
[1] 3 0
 > lapply(mtcars, 'mean')
$mpg
[1] 20.09062
 $cyl
[1] 6.1875
 $disp
[1] 230.7219
 $hp
[1] 146.6875
 $drat
[1] 3.596563
 $wt
[1] 3.21725
 $qsec
[1] 17.84875
 $vs
[1] 0.4375
 $am
[1] 0.40625
 $gear
[1] 3.6875
 $carb
[1] 2.8125
 > sapply(l, '[', 1,2)
[1] 3 0
 > sapply(mtcars, 'mean')
       mpg        cyl       disp         hp       drat         wt       qsec         vs         am
 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750   0.437500   0.406250
      gear       carb
  3.687500   2.812500
 > sapply(iris, 'mean')
Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species
    5.843333     3.057333     3.758000     1.199333           NA
Warning message:
In mean.default(X[[i]], ...) :
  argument is not numeric or logical: returning NA
 > sapply(iris, class)
Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species
   "numeric"    "numeric"    "numeric"    "numeric"     "factor"
 > v1 <- c(1,2,3)
> v2 <- c(3,4,5)
> mapply(v1, v2, sum)
[1] 4 6 8
 > mapply(rep,1:4,4)
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    1    2    3    4
[3,]    1    2    3    4
[4,]    1    2    3    4
 > matrix(c(rep(1, 4), rep(2, 4), rep(3, 4), rep(4, 4)),4,4)
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    1    2    3    4
[3,]    1    2    3    4
[4,]    1    2    3    4
 > l <- list("a", "b", "c", "d")
> mode(l)
[1] "list"
> class(l)
[1] "list"
> mode(l[[1]])
[1] "character"
> class(l[[1]])
[1] "character"
> m <- mapply(rep, l, 4)
> m
     [,1] [,2] [,3] [,4]
[1,] "a"  "b"  "c"  "d"
[2,] "a"  "b"  "c"  "d"
[3,] "a"  "b"  "c"  "d"
[4,] "a"  "b"  "c"  "d"
> mode(m)
[1] "character"
> class(m)
[1] "matrix"
 > l <- list("aa", "bb", "cc", "dd")
> m <- mapply(rep, l, 4)
> m
     [,1] [,2] [,3] [,4]
[1,] "aa" "bb" "cc" "dd"
[2,] "aa" "bb" "cc" "dd"
[3,] "aa" "bb" "cc" "dd"
[4,] "aa" "bb" "cc" "dd"
 > l <- list(1, "bb", T, 4.5)
> m <- mapply(rep, l, 4)
> m
     [,1] [,2] [,3]   [,4]
[1,] "1"  "bb" "TRUE" "4.5"
[2,] "1"  "bb" "TRUE" "4.5"
[3,] "1"  "bb" "TRUE" "4.5"
[4,] "1"  "bb" "TRUE" "4.5"
 > x <- c(1, -1, 1, NA, -2, 1, -3, 4, NA, NA, 3, 2, -4, -3, NA)
 > is.na(x)
 [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE
 > x[!is.na(x)]
 [1]  1 -1  1 -2  1 -3  4  3  2 -4 -3
 > sum(x)
[1] NA
 > sum(x, na.rm = T)
[1] -1
 > mean(x)
[1] NA
> mean(x, na.rm = T)
[1] -0.09090909
 > var(x)
[1] NA
> var(x, na.rm = T)
[1] 7.090909
 > x[1] <- NA
 > y<-na.omit(x)
> y
 [1] -1  1 -2  1 -3  4  3  2 -4 -3
attr(,"na.action")
[1]  1  4  9 10 15
attr(,"class")
[1] "omit"
 > na.fail(x)
Error in na.fail.default(x) : missing values in object
> na.fail(y)
 [1] -1  1 -2  1 -3  4  3  2 -4 -3
attr(,"na.action")
[1]  1  4  9 10 15
attr(,"class")
[1] "omit"
 > methods(plot)
 [1] plot.acf*           plot.bclust*        plot.data.frame*    plot.decomposed.ts* plot.default
 [6] plot.dendrogram*    plot.density*       plot.ecdf           plot.factor*        plot.formula*
[11] plot.function       plot.hclust*        plot.histogram*     plot.HoltWinters*   plot.ica*
[16] plot.isoreg*        plot.lm*            plot.medpolish*     plot.mlm*           plot.ppr*
[21] plot.prcomp*        plot.princomp*      plot.profile.nls*   plot.raster*        plot.SOM*
[26] plot.somgrid*       plot.spec*          plot.stepfun        plot.stft*          plot.stl*
[31] plot.svm*           plot.table*         plot.ts             plot.tskernel*      plot.TukeyHSD*
[36] plot.tune*
 > methods(class="matrix")
 [1] anyDuplicated as.data.frame as.raster     boxplot       coerce        determinant   duplicated
 [8] edit          head          initialize    isSymmetric   Math          Math2         Ops
[15] relist        subset        summary       tail          unique
 > methods(class="table")
 [1] [             aperm         as.data.frame Axis          coerce        head          initialize
 [8] lines         plot          points        print         show          slotsFromS3   summary
[15] tail
 > methods(coef)
[1] coef.aov*     coef.Arima*   coef.default* coef.listof*  coef.maov*    coef.nls*
 > `%d%` <- function(x, y) { sqrt(sum((x-y)^2)) }
 > c(1,0, 0) %d% c(0,1,0)
[1] 1.414214
> c(1,1, 0) %d% c(0,1,0)
[1] 1
> c(1,1, 1) %d% c(0,1,0)
[1] 1.414214
 
 
Item 3267: 
cd /Users/<your user name>/anaconda/envs/
 conda create --name TestEnv python=3
 cource activate TestEnv
 conda install numpy pandas scikit-learn matplotlib
 python
 >>>import numpy 
>>>import pandas 
>>>import scikit-learn 
>>>import matplotlib
 conda create --name <my environment name> python=<python version>
 source activate <my environment name>
 source deactivate
 conda install <library name>
 conda update <library name>
 conda remove --name <my environment name> <library name>
 conda remove --name <my environment name> --all
 conda update
 
 
Item 3272: 
>> Signal: 0b010110
>> Output: 1
 
 
Item 3277: 
createLog <- function(df){log_con <- file("process.log",open="w")cat(nrow(df), file = log_con, sep="\n")...}
 
 
Item 3278: 
JPY = pd.read_csv('JPY.csv')
EUR = pd.read_csv('EUR.csv')
CNY = pd.read_csv('CNY.csv')
 JPY = JPY.drop(['Unnamed: 0'], axis=1)
JPY.DATE = pd.to_datetime(JPY.DATE)
JPY = JPY.set_index('DATE')
 JPY.plot()
 CURR = pd.concat([EUR, JPY, CNY], axis=1)
CURR.columns = (['EUR', 'JPY', 'CNY'])
 
 
Item 3281: 
loss: 0.0825 - acc: 0.9793 - top_5_categorical_accuracy: 0.9994 
val_loss: 2.1778 - val_acc: 0.6548 
val_top_5_categorical_accuracy: 0.8372
 
 
Item 3282: 
#!/usr/bin/perl
print "Hello World\n";
 class HelloWorld
   def initialize(name)
      @name = name.capitalize
   end
   def sayHi
      puts "Hello #{@name}!"
   end
end

hello = HelloWorld.new("World")
hello.sayHi
 
 
Item 3283: 
t1=()
t2=tuple()
 t1=(4,5,9,7,"dog",4.5)
 l=[4,5,6,7,9,2,34,8.212,"cat"]
t2=tuple(l)
 t2.count(5)# Devuelve el número de veces que hay un 1 en la tupla
t2.index(9)# Devuelve en qué posición se encuentra el 9.
 
 
Item 3284: 
age=16
 if age <18: 
 print ("You are under 18" )
 else: 
 print ("You are over 18")
 age=14
 if age <18: 
 print ("You are under 18" )
 elif 18< age <20: 
 print("The age is between 18 and 20")
 
elif 20< age<25: 
 print ("The age is between 20 and 25")
 
else: 
 print ("You are over 25)
 for i in range(10): 
 print ("hello")
 i=0
while i<10: 
 print ("hello")
 i +=1
 def saludo(): 
 print ("Hello")
 saludo()
 def suma(a,b): 
 print ("Sum is:", a+b)
 
suma(10,2)# 'Sum is:', 12
 def suma(a,b): 
 return (a+b)
 
print (suma(10,2))#'Sum is:', 12
 def operation(word): 
 a=10
 b=4
  if word==1: 
 return a+b 
  elif word==2: 
 return a-b
  elif word==3: 
 return a*b
  elif word==4: 
 return a/b
  else: 
 return "Please type any of the options: 1,2,3,4" 
 
a=input("select one of the following options:1,2,3,4 ===> ")
operation(a)
 
 
Item 3289: 
Traffic type	Application used
http	browser
ftp	filezilla client
bittorrent	peer-peer_client
snmp	monitoring_tools
 export CLASSPATH=$CLASSPATH:/usr/share/java/weka.jar
~jython UsingJ48Ext.py main.arff
 Protocol = SNMP: monitoring_tools (10.0/1.0)
Protocol = SRVLOC: telnet (0.0)
Protocol = NBNS: telnet (0.0)
Protocol = BROWSER: telnet (0.0)
Protocol = ICMP: telnet (0.0)
Protocol = TCP: telnet (0.0)
Protocol = TELNET: telnet (18.0)
Protocol = BitTorrent: peer-peer_client (12.0)
Protocol = TFTP: filezilla (9.0)
Protocol = HTTP: browsers (4.0)
 Number of Leaves  : 	10
 Size of the tree : 	11
 Correctly Classified Instances          52               98.1132 %
Incorrectly Classified Instances         1                1.8868 %
Kappa statistic                          0.9753
Mean absolute error                      0.0136
Root mean squared error                  0.0824
Relative absolute error                  4.4312 %
Root relative squared error             21.0904 %
Total Number of Instances               53     
Ignored Class Unknown Instances                  4
 1 5:monitori 5:monitori       0.9 
     2 5:monitori 5:monitori       0.9 
     5        1:? 5:monitori       0.9 
     6 5:monitori 5:monitori       0.9 
     7 5:monitori 5:monitori       0.9 
    27   4:telnet   4:telnet       1 
    28   4:telnet   4:telnet       1 
    29 2:peer-pee 2:peer-pee       1 
    30 2:peer-pee 2:peer-pee       1 
    31        1:? 2:peer-pee       1 
    32 2:peer-pee 2:peer-pee       1 
    33 2:peer-pee 2:peer-pee       1 
    34 2:peer-pee 2:peer-pee       1 
    35        1:? 2:peer-pee       1 
    36 2:peer-pee 2:peer-pee       1 
    37 2:peer-pee 2:peer-pee       1 
    43 3:filezill 3:filezill       1 
    44 3:filezill 3:filezill       1 
    45 3:filezill 5:monitori   +   0.9 
    46 3:filezill 3:filezill       1 
    47 3:filezill 3:filezill       1 
    52 3:filezill 3:filezill       1 
    53        1:? 1:browsers       1 
    54 1:browsers 1:browsers       1
 Column number	Details
First column	Instance number
Second column	Class defined in dataset
Third column	Class predicted by machine learning
Fourth column	Probability that the instance is belong to the class predicted my machine
 
 
Item 3297: 
library(ggplot2)
View(diamonds)
 ??ggplot2::diamonds
 library(caTools)
sample.split(diamonds$price,SplitRatio = 0.65) -> split_values
 subset(diamonds,split_values==T)->train_reg
subset(diamonds,split_values==F)->test_reg
 lm(price~.,data = train_reg) -> mod_regress
 predict(mod_regress, test_reg) ->result_regress
 cbind(Actual=test_reg$price,Predicted=result_regress)->Final_Data
 as.data.frame(Final_data)->Final_Data
 View(Final_Data)
 
 
Item 3300: 
const er = new EventRegistry({apiKey: "YOUR_API_KEY"}); er.getConceptUri("Star Wars")
  .then((conceptUri) => {      
    const query = new QueryEvents({conceptUri: conceptUri});     
    const requestEventsInfo = new RequestEventsInfo(
      {sortBy: "date", count: 10});         
    query.addRequestedResult(requestEventsInfo);      
    return er.execQuery(query); 
  }).then((response) => {      
    console.info(response); 
  });
 const er = new EventRegistry({apiKey: "YOUR_API_KEY"}); 
async function iterateOverEvents() { 
  const query = new QueryEvents({
    conceptUri: await er.getConceptUri("Star Wars")}); 
    const requestEventsInfo = new RequestEventsInfo(
      {sortBy: "date", count: 10});    
    query.addRequestedResult(requestEventsInfo); 
    return er.execQuery(query); 
} 
iterateOverEvents();
 const er = new EventRegistry(); 
const iter = new QueryEventArticlesIter(er, "eng-2940883"); 
iter.execQuery((articles) => { 
  console.info(articles); 
}, () => { 
  console.info("done"); 
});
 
 
Item 3301: 
/*
You will want to replace {{username}}, {{mydatabase}}, {{userpassword}} with your correct values
*/
create database {{mydatabase}};
\connect {{mydatabase}}
create user {{username}} with password '{{userpassword}}';
grant create on database {{mydatabase}} to {{username}};
create schema if not exists openbridge authorization {{username}};
grant all on schema openbridge to {{username}};
 
 
Item 3304: 
a=2 
b=3
 def suma(a,b): 
    a=10
    print ("Scope local:",a+b)
 suma(a,b)
print ("Scope global:", a+b)
 
 
Item 3305: 
setwd("/path/to/your/working/directory")
bmw_purchase <- read.csv("BMWCarPurchase.csv")
 view(car_purchase)
 #load dplyr library
library(dplyr)
 #remove the first column and store it back to car_purchase dataset
car_purchase %>% select(-1) -> car_purchase
 Error in library(dplyr) : there is no package called ‘dplyr’
 install.packages("tidyverse")
 #load library caTools
library(caTools)
 #split the dataset with ratio 65%
sample.split(car_purchase$Purchased,SplitRatio = 0.65) -> split_values
 #store them into train_set and test_set respectively
subset(car_purchase,split_values==T)->train_set
subset(car_purchase,split_values==F)->test_set
 #loading RPART
library(rpart)
 #Building classification model
rpart(Purchased~.,data = train_set) -> mod_class
 predict(mod_class, test_set, type= "class") -> result_class
 #confusion matrix
table(test_set$Purchased, result_class)
 #result
 result_class
      NO YES
  NO  45  3
  YES  4 12
 
 
Item 3307: 
def listsum(numList):
    theSum = 0
    for i in numList:
        theSum = theSum + i
    return theSum
 print(listsum([1,4,5,7,9}))
 
 
Item 3311: 
t1=()
t2=tuple()
 t1=(4,5,9,7,"dog",4.5)
 l=[4,5,6,7,9,2,34,8.212,"cat"]
t2=tuple(l)
 t2.count(5)# Returns the number of times there is a 1 in the tuple
t2.index(9)# Returns the position of the 9.
 
 
Item 3315: 
- Supervised Classification
- Data Exploration
    --> Feature Observation : Identify and build feature and target columns from the dataset.
    Using labeled face images (the Adience benchmark), I have been able to recreate a new Unified dataset of new features and labels to fit my models.
- Data Visualization (Unified dataset, Training set and Test set visualization)
- Performance Metric (Loss and Accuracy Scores)
- Shuffle and Split Data : Training and Testing Data Split (sklearn train_test_split tool)
- Training Models (TFLearn and Tensorflow Keras models)
- Model Evaluation and Validation (Loss and Accuracy)
- Analyzing Model Performance (Tensorboard and Training history Visualization)
    --> Learning Curves
    --> Complexity Curves
- Making Predictions (of correct labels)
- Model Optimization - Model Tuning (optimizer, activation, loss and regularization functions, number of epoch, etc. )
- Training computational cost (Big-O complexities of common algorithms used in Computer Science)
 And the intended main contribution of my work had been:
- Provide simple and easy to use tools for dataset preprocessing, considering different Machine Learning Frameworks requirements (Custom Data Preprocessing and visualization functions, prediction interpretation functions, etc.).
- Assemble the prediction process in one step for both predictions, age and gender (instead of separate networks: age network and gender network).
- Turning the implementation in video and mobile application (all process resources provided).
- Models' performances : 97% accuracy with the tuples model and about 60% of exact accuracy with the 2-hot labels model (Beat the true accuracies of previous research work in the field).
- Fully reproducible paper, with full Python implementation code available. Could be applied to any age dataset with more age classes.
All on my Github Age and Gender Classification project.
 
 
Item 3318: 
SELECT
  weight_pounds, 
  state, 
  year, month, day 
  child_race, plurality, 
  mother_race, mother_age, 
  cigarette_use, cigarettes_per_day,
  alcohol_use, drinks_per_week, 
  father_race,
  father_age,
  is_male,
  gestation_weeks
FROM
  [bigquery-public-data.samples.natality]
WHERE state is NOT NULL
AND gestation_weeks is NOT NULL
AND year >= 2003
AND born_dead = 0
 SELECT
  weight_pounds, 
  state, 
  year, month, day 
  child_race, plurality, 
  mother_race, mother_age, 
  cigarette_use, cigarettes_per_day,
  alcohol_use, drinks_per_week, 
  father_race,
  father_age,
  is_male,
  gestation_weeks
FROM
  [bigquery-public-data.samples.natality]
WHERE state is NOT NULL
AND gestation_weeks is NOT NULL
AND year >= 2003
AND born_dead = 0
AND RAND() > 1000000 / 6141662
 Error in read_tokens_(data, tokenizer, col_specs, col_names, locale_, : ignoring SIGPIPE signal
 
 
Item 3319: 
>?.Internal
 
 
Item 3322: 
> kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE istio-ca-797dfb66c5 1/1 Running 0 2m istio-ingress-84f75844c4 1/1 Running 0 2m istio-egress-29a16321d3 1/1 Running 0 2m istio-mixer-9bf85fc68 3/3 Running 0 2m istio-pilot-575679c565 2/2 Running 0 2m grafana-182346ba12 2/2 Running 0 2m prometheus-837521fe34 2/2 Running 0 2m
 
 
Item 3328: 
names = ["Bagging", "Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process", "Decision Tree", "Random Forest", "Neural Net", "AdaBoost", "Naive Bayes", "QDA", "Logistic Regression", "Gradient Boosting", "Neural Net"]
 classifiers = [
    BaggingClassifier(),
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis(),
    LogisticRegression(),
    GradientBoostingClassifier(n_estimators=50),
    MLPClassifier(alpha=1)
                            ]
 classificadores = Classifiers()
 
 
Item 3329: 
data.SeniorCitizen.replace([0, 1], ["No", "Yes"], inplace= True)
 data.TotalCharges = data.TotalCharges.astype(float)
 data.TotalCharges.replace([" "], ["0"], inplace= True)
data.TotalCharges = data.TotalCharges.astype(float)
 data.Churn.replace(["Yes", "No"], [1, 0], inplace= True)
 data = pd.get_dummies(data)
 X = data.drop("Churn", axis= 1)
y = data.Churn
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 1234)
 _, accuracy = model.evaluate(X_test, y_test)
 model.save('my_model.h5')
 
 
Item 3331: 
pip3 install --upgrade tensorflow
 
 
Item 3333: 
def random_draft(state):
    return np.random.choice(state.GetMoves())
 def greedy_draft(state):
    def result(move):
        st = state.Clone()
        st.DoMove(move)
        return st.GetResult(st.playerJustMoved)
    
    res, move = max((result(m), m) for m in state.GetMoves())
    return move
 
 
Item 3339: 
This is what we care about. Let's take a look at the function:
prediction = net.predict([grid_square]).
 
 
Item 3343: 
sex ageYear heightIn
1     f   11.92     56.3
2     f   12.92     62.3
3     f   12.75     63.3
4     f   13.42     59.0
5     f   15.92     62.5
 
 
Item 3344: 
%matplotlib notebook
 
 
Item 3349: 
DataFrame.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')
 df.drop([0, 1])
 df.drop(['B', 'C'])
 DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)
 
 
Item 3352: 
J3 BALANCING CAR PROJECT: We want the angle gyro output as near zero as possible. So, I suppose the car will be in balance at that point, right?
 Proportional-Integral-Derivative (PID) control is the most common control algorithm used in industry and has been universally accepted in industrial control.

 The basic idea behind a PID controller is to read a sensor, then compute the desired actuator output by calculating proportional, integral, and derivative responses and summing those three components to compute the output.
 Closed Loop Control System (CLCS) is the process of reading sensors to provide constant feedback and calculating the desired actuator output is repeated continuously and at a fixed loop rate .
 The term “disturbance rejection” was used a number of times in lecture today to describe the purpose of feedback. With feedback, the controller is able to use the output to shape the input of the system.
 Nonlinear System (NLS) is a system in which the control parameters that produce a desired response at one operating point might not produce a satisfactory response at another operating point.
 
 
Item 3353: 
qplot(age,friend_count,data=pf)
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point()
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point()+
  xlim(13,90)
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point(alpha=1/20)+
  xlim(13,90)
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_jitter(alpha=1/20)+
  xlim(13,90)
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point(alpha=1/20)+
  xlim(13,90)+
  coord_trans(y = "sqrt")
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point(alpha=1/20,position=position_jitter(h=0))+
  xlim(13,90)+
  coord_trans(y = "sqrt")
 age_groups<-group_by(pf,age)
pf.fc_by_age<-summarise(age_groups,
          friend_count_mean=mean(friend_count),
          friend_count_median=median(friend_count),
          n=n())
pf.fc_by_age<-arrange(pf.fc_by_age,age)
head(pf.fc_by_age)
 A tibble: 6 x 4
    age friend_count_mean friend_count_median     n
  <int>             <dbl>               <dbl> <int>
1    13              165.                 74    484
2    14              251.                132   1925
3    15              348.                161   2618
4    16              352.                172.  3086
5    17              350.                156   3283
6    18              331.                162   5196
 ggplot(aes(age,friend_count_mean),data=pf.fc_by_age) +
  geom_point()
 ggplot(aes(age,friend_count_mean),data=pf.fc_by_age) +
  geom_line()
 ggplot(aes(x=age,y=friend_count),data=pf)+
  geom_point(alpha=1/20,
             position=position_jitter(h=0),
             color='red')+
  xlim(13,90)+
  coord_trans(y = "sqrt")+
  geom_line(stat='summary',fun.y=mean)+
  geom_line(stat='summary',fun.y=median,linetype=2,color='blue')+
  geom_line(stat='summary',fun.y=quantile,fun.args=list(probs=0.9),color='blue')
 ggplot(aes(x=age,y=friend_count),data=pf)+
  coord_cartesian(xlim=c(13,70),ylim=c(0,1000))+
  geom_point(alpha=1/20,
             position=position_jitter(h=0),
             color='orange')+
  geom_line(stat='summary',fun.y=mean)+
  geom_line(stat='summary',fun.y=median,linetype=2,color='blue')+
  geom_line(stat='summary',fun.y=quantile,fun.args=list(probs=0.9),color='brown')
 
 
Item 3355: 
a=2 
b=3
 def suma(a,b): 
a=10
print ("Scope local:",a+b)
 suma(a,b)
print ("Scope global:", a+b)
 
 
Item 3362: 
t1 = a * a
t2 = b * b
t3 = t1 + t2
 [[[1, 2, 3]], [[7, 8, 9]]]
 
 
Item 3363: 
end = datetime.datetime.now()
start = end - datetime.timedelta(365)
AAPL = quandl.get('EOD/AAPL', start_date=start, end_date=end)

rets_1 = (AAPL['Close']/AAPL['Close'].shift(1))-1
 mean = np.mean(rets_1)
std = np.std(rets_1)
Z_99 = stats.norm.ppf(1-0.99)
price = AAPL.iloc[-1]['Close']
print(mean, std, Z_99, price)
 Out:
0.0016208298475378427 0.013753943856014762 -2.32634787404 220.79
 ParamVAR = price*Z_99*std
HistVAR = price*np.percentile(rets_1.dropna(), 1)

print('Parametric VAR is {0:.3f} and Historical VAR is {1:.3f}'
      .format(ParamVAR, HistVAR))
 Out:
Parametric VAR is -7.064 and Historical VAR is -6.166
 np.random.seed(42)
n_sims = 1000000
sim_returns = np.random.normal(mean, std, n_sims)
SimVAR = price*np.percentile(sim_returns, 1)
print('Simulated VAR is ', SimVAR)
 Out:
Simulated VAR is  -6.7185294884
 
 
Item 3365: 
p(HGNC:IL6) increases r(HGNC:ENO1)
 p(HGNC:IL6) increases r(HGNC:XBP1)
 proteinAbundance(HGNC:IL6) increases rnaAbundance(HGNC:ENO1)
 Species: Human
 Cell Line: U266
 
 
Item 3366: 
age=16
 if age <18: 
    print ("You are under 18" )
 else: 
    print ("You are over 18")
 age=14
 if age <18: 
    print ("You are under 18" )
 elif 18< age <20: 
    print("The age is between 18 and 20")
    
elif 20< age<25: 
    print ("The age is between 20 and 25")
    
else: 
    print ("You are over 25)
 for i in range(10): 
    print ("hello")
 i=0
while i<10: 
    print ("hello")
    i +=1
 def saludo(): 
    print ("Hello")
 saludo()
 def suma(a,b): 
    print ("Sum is:", a+b)
    
suma(10,2)# 'Sum is:', 12
 def suma(a,b): 
    return (a+b)
    
print (suma(10,2))#'Sum is:', 12
 def operation(word): 
    a=10
    b=4
    if word==1: 
        return a+b 
    elif word==2: 
        return a-b
    elif word==3: 
        return a*b
    elif word==4: 
        return a/b
    else: 
        return "Please type any of the options: 1,2,3,4" 
    
a=input("select one of the following options:1,2,3,4 ===> ")
operation(a)
 
 
Item 3368: 
def sign(x):
    if x == 0:
        return 0
    elif x < 0:
        return -1
    elif x > 0:
        return 1
 def to_smash(total_candies):
    """Return the number of leftover candies that must be smashed after distributing the given number of candies evenly between 3 friends.
    
    >>> to_smash(91)
    1
    """
     print("Splitting", total_candies, "candies")
    return total_candies % 3
 to_smash(1)
 Splitting 1 candies
 1
 def to_smash(total_candies):

    print("Splitting", total_candies, "candy") 
if total_candies == 1 
else print("Splitting", total_candies, "candies")
     return total_candies % 3
 def prepared_for_weather(have_umbrella, rain_level, have_hood, is_workday):
    
    return have_umbrella or rain_level < 5 and have_hood or not rain_level > 0 and is_workday

# Change the values of these inputs so they represent a case where prepared_for_weather returns the wrong answer.
 have_umbrella = True
rain_level = 0.0
have_hood = True
is_workday = True

# Check what the function returns given the current values of the variables above
 actual = prepared_for_weather(have_umbrella, rain_level, have_hood, is_workday)
 print(actual)
 have_umbrella = False
rain_level = 0.0
have_hood = False
is_workday = False
 (not (rain_level > 0)) and is_workday
 not (rain_level > 0 and is_workday)
 def is_negative(number):
    if number < 0:
        return True
    else:
        return False
 def concise_is_negative(number):
    
    return number < 0
 def onionless(ketchup, mustard, onion):
    """Return whether the customer doesn't want onions.
    """
     return not onion
 onionless(True, True, False) 
 def wants_all_toppings(ketchup, mustard, onion):
    """Return whether the customer wants "the works" (all 3 toppings)
    """
    return ketchup and mustard and onion <- CODED THIS
 def wants_plain_hotdog(ketchup, mustard, onion):
    """Return whether the customer wants a plain hot dog with no toppings.
    """
     return not (ketchup or mustard or onion) <- CODED THIS
     #return not ketchup and not mustard and not onion <- another option
 def exactly_one_sauce(ketchup, mustard, onion):
    """Return whether the customer wants either ketchup or mustard, but not both.
    ""    
 return (ketchup and not mustard) or (mustard and not ketchup) <- CODED THIS
 def exactly_one_topping(ketchup, mustard, onion):
    """Return whether the customer wants exactly one of the three available toppings on their hot dog.
    """
    return
 def exactly_one_topping(ketchup, mustard, onion):

    return (int(ketchup)+int(mustard)+int(onion)) == 1
 def should_hit(player_total, dealer_total, player_aces):
    """Return True if the player should hit (request another card) given the current game
    state, or False if the player should stay. player_aces is the number of aces the player has.
    """
     if player_total < 16 and player_aces == 0:
        return True
    elif player_total >= 16:
        return False
        
    return False
 
 
Item 3369: 
CountVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=1.0, max_features=None, min_df=1,
        ngram_range=(1, 1), preprocessor=None, stop_words=None,
        strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
        tokenizer=None, vocabulary=None)
 
 
Item 3372: 
[0 1 1 0]
 [0 1 1 0]
 
 
Item 3377: 
<script src=”//static.vilynx.com/vidrec/vidrec.3.0.1.min.js” data-publickey=”{{user_public_key}}” data-onpluginload=”handler.onPluginLoad” data-limit=”5" data-confighash={{editorial_widget_hash}} ></script>
 
 
Item 3378: 
#1. Creation class
 class skeleton(): 
    def __init__(self,life=5,attack=1,defence=2): 
        self.life=life
        self.attack=attack
        self.defence=defence
    def scream(self): 
        print ("I m a Skeleton !")
        
#2. Creating the skeletons using the class above
 ske1=skeleton(10,2,2)
ske2=skeleton(8,2,3)
ske3=skeleton(6,3,1)
ske4=skeleton(12,5,4)
 print (ske3.scream())
 
 
Item 3380: 
with gfile.GFile(filepath, "w") as file:
    file.write(result_str)
 
 
Item 3387: 
trait Trainer {
 //Train the algorithm
  def train: Boolean
 //Read data from file
  protected def read: List[Map[String, Any]]
 //separate data into two files
  protected def separate(data: List[Map[String, Any]]): (List[Map[String, Any]], List[Map[String, Any]])
 //pass training data to algorithm
  protected def training(sample: List[Map[String, Any]]): Boolean
 //Validate final hypothesis
  protected def validate(validationData: List[Map[String, Any]]): Boolean
 }
 /**
  * Model to be trained
  */
trait Model {
  val resKey: String
  def training(sample: scala.collection.immutable.Map[String, Any]): Boolean
  def getHypothesis: Any
  def predict(dataObject: scala.collection.immutable.Map[String, Any]): Boolean
  def persist: Boolean
  def trained: Boolean
}
 /**
  * Find-S example
  */
object FindSExample extends App with LogHelper {
 /** ******************************
    * TRAINING DATA GENERATION
    * *******************************/
  val trainingDataFilePath = ConceptLearningTrainingDataGenerator
                             .randomTrainingData
 /** ******************************
    * TRAINER INITIALIZATION
    * *******************************/
  val path = "/tmp/find_s"
  val jsonHelper = new FileHelper {}.reset(path)
  val trainer = new FindSTrainer {
    val trainingSampleFilePath = trainingDataFilePath
    val model: Model = new FindS("result", path)
    override val trainingRatio = 1.0
  }
 /** ******************************
    * TRAINING
    * *******************************/
  if (!trainer.model.trained) {
    trainer.train
  } else {
    info("Model is trained, skipping training process")
  }
 /** ******************************
    * TRAINED MODEL
    * *******************************/
  val trainedModel = trainer.model
 info(s"***Hypothesis: ${trainedModel.getHypothesis}")
 /** ***********************************
    * TESTING
    * ***********************************/
  val testDataObject = Map("sky" -> "Sunny", "airtemp" -> "Cool",
    "humidity" -> "Warm", "wind" -> "Weak",
    "water" -> "Cool", "forecast" -> "Change")
  info(s"***Testing new positive data object: $testDataObject")
  val status = trainedModel.predict(testDataObject)
  if (status) {
    info("***THE DATA OBJECT IS ... : +POSITIVE")
  } else {
    info("***THE DATA OBJECT IS ... : -NEGATIVE")
  }
}
 
 
Item 3390: 
Map<String, INDArray> stringINDArrayMap = extract(file, vgg16transfer);
INDArray fc2 = stringINDArrayMap.get(EXTRACTION_LAYER);
INDArray normalized = fc2.div(fc2.norm2Number());
saveCompressed(file,normalized);
 
 
Item 3392: 
PerformanceHour.collection.bulk_write([
  {
    update_many: {
      filter: {
        app_id: BSON::ObjectId.from_string(app_id),
        finished_at: finished_at.to_datetime.mongoize,
      },
      update: {
        ‘$inc’ => {
          time: 152.33,
          count: 15,
          ...
          browsers: {
            Safari: {
              11: 3
            },
            ...
          }
          errors: 1
      },
      upsert: true
    }
  }
])
 
 
Item 3393: 
{ _id     : '_id of the Document in `cleaned_files`',
   method_1: {content: 'data extracted by method_1',
              status : 'message returned by method_1',
   method_2: {content: 'data extracted by method_2',
              status : 'message returned by method_2',
   ...}
 { _id : '_id of the Document in `responses_0`',
 method : 'name of the parsing method that yielded the best-quality response',
 content: 'data extracted by the best method',
 status : 'message returned by the best method' }
 
 
Item 3397: 
set
 .values()
 
 
Item 3398: 
pip3 install jupyterlab
 jupyter lab --allow-root --ip=0.0.0.0 --no-browser
 c.Spawner.default_url = '/lab'
 pip3 install jupyterlab
 jupyter serverextension enable --py jupyterlab --sys-prefix
 jupyter labextension install @jupyterlab/hub-extension
 jupyter lab build
 
 
Item 3403: 
=VLOOKUP(H2,IF({1,0},Table1[Order ID],Table1[Product ID]),2,FALSE)
 {"CA-2016-152156","FUR-BO-10001798";"CA-2016-138688","FUR-CH-10000454";"US-2015-108966","OFF-LA-10000240";"CA-2014-115812","FUR-TA-10000577";"CA-2017-114412","OFF-ST-10000760";"CA-2016-161389","FUR-FU-10001487";"US-2015-118983","OFF-AR-10002833";"CA-2014-105893","TEC-PH-10002275";"CA-2014-167164","OFF-BI-10003910";"CA-2014-143336","OFF-AP-10002892";"CA-2016-137330","FUR-TA-10001539";"US-2017-156909","TEC-PH-10002033";"CA-2015-106320","OFF-PA-10002365";"CA-2016-121755","OFF-BI-10003656";"US-2015-150630","OFF-AP-10002311";"CA-2017-107727","OFF-BI-10000756"}
 =VLOOKUP(H3,CHOOSE({1,2},Table1[Order ID],Table1[Product ID]),2,FALSE)
 {=VLOOKUP(G2,A1:E16,{2,4,5},FALSE)}
 
 
Item 3406: 
3378
 {'main': 1914,
 'breaker': 718,
 'put': 2351,
 'position': 2275,
 'loco': 1866,
 'ableunable': 392,
...
}

 
 
Item 3410: 
Using TensorFlow backend. Loading data... Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz 17465344/17464789 [==============================] - 62s 4us/step 
25000 train sequences 25000 test sequences 
Pad sequences (samples x time) 
x_train shape: (25000, 80) x_test shape: (25000, 80) 
Build model... 
Train... 
Train on 25000 samples, validate on 25000 samples 
Epoch 1/3 25000/25000 [==============================] - 615s 25ms/step - loss: 0.3808 - acc: 0.8203 
Epoch 2/3 25000/25000 [==============================] - 623s 25ms/step - loss: 0.1679 - acc: 0.9366 
Epoch 3/3 25000/25000 [==============================] - 562s 22ms/step - loss: 0.0600 - 
acc: 0.9794 Accuracy: 82.56% 
Downloading data from https://s3.amazonaws.com/text datasets/imdb_word_index.json 1646592/1641221 [==============================] - 6s 3us/step
Predicting..
 Great movie I had ever watched.. Sentiment: [[0.9146]] 
you know even better than them that you have potential! Stop portraying in parody movies!. Sentiment: [[0.03823381]]
 
 
Item 3411: 
Skills:
- web development,
- mobile development
Needs:
- investor
 Skills:
- investor,
- entrepreneur
Needs:
- graphic designer,
- internet marketer
 Person A should talk to:
- Person X
- Person Y
- Person Z
 4 people = 4 combinations  
5 people = 20 combinations  
6 people = 60 combinations  
7 people = 140 combinations  
...
20 people = 19 380 combinations  
21 people = 23 940 combinations  
...
50 people = 921 200 combinations  
...
100 people = 15 684 900 combinations
 
 
Item 3412: 
"New technologies often appeal to young people, who are more plastic and receptive to innovations. A good example is KidsANDtransplant, a serious game computing platform created in Geneva. This great tool gives children or teenagers who got a liver transplant and their relatives access to the information needed for their medical follow-up: hospital discharge letters, consultation reports, imaging reports, laboratory results, follow-up charts etc.
 
 
Item 3413: 
Regression coefficients from Sklearn
[('x1', 1.04),('x2', -0.54),('x3', -0.0),('x4', -0.0),('x5', -0.0)]
 Regression coefficients From Tensorflow:
[('x1', 1.0),('x2', -0.49),('x3', -0.0),('x4', -0.0),('x5', -0.0)]
 
 
Item 3416: 
val lr = new LogisticRegression().setMaxIter(2000)
  lr.setLabelCol("CategoryNameIndex")
  lr.setFeaturesCol("featuresNormalised")
 
 
Item 3422: 
def sumaf(a,b): 
    return(a+b)
 print (sumaf("dog",5)# Error
print ("hello")
 def sumaf(a,b):
    try: 
        return a+b
    except: 
        print("There's a error" )
 print (sumaf("dog",5))
print("hello")
 def sumaf(a,b):
    try: 
        return a+b
    except Exception as e: 
        print("There's a error" )
 print (sumaf("dog",5))
print("hello")
 def sumaf(a,b):
    try: 
       return a+b
    except Exception as e: 
       print("There's a error" )
    finally: 
       print("Thank you")
 print (sumaf("dog",5))
print("hello")
 def sumaf(a,b):
     try: 
         return a+b
     except TypeError: 
         print("There's a error" )
     finally: 
        print("Thank you")
 print (sumaf("dog",5))
print("hello")
 
 
Item 3423: 
df.groupby([‘gender’, ‘age’])[‘expected_time’].apply(
    lambda x: x.notnull().mean())
 
 
Item 3425: 
data = []
count = 0
for i in titles: 
 count +=1
 name = i.text.strip()
 nextsite = ‘http://www.imdb.com' + i.a[‘href’]
 hdr = {‘User-Agent’: ‘Mozilla/5.0’}
 onebookpage = requests.get(nextsite)
 onesoup = BeautifulSoup(onebookpage.text, “html.parser”)
 soup = BeautifulSoup(bookpage.text, “html.parser”)
 try: 
 Titles= titles[i].text.strip()
 except:
 Titles=”NA”
 try:
 rank = onesoup.find(‘div’, class_ = “titleReviewBar”)
 except: 
 rank = “NA”
 try:
 rating=onesoup.find(itemprop=”ratingValue”).get_text().strip()
 except:
 rating = “NA”
 try:
 advisory = onesoup.find(‘span’, itemprop=”contentRating”).get_text().strip()
 except:
 advisory = “NA”
 try:
 runtime = onesoup.find(itemprop=”duration”).get_text().strip()
 except:
 runtime = “NA”
 try:
 genre=onesoup.find(itemprop=”genre”).get_text().strip()
 except:
 genre = “NA”
 try:
 Date = onesoup.find(title=”See more release dates”).get_text().strip()
 except:
 Date = “NA”
 data.append((Titles, rating, rank, advisory, runtime, genre, Date)) 
print(count)
time.sleep(1)
 
 
Item 3427: 
Point gradientDescent(float theta0, float theta1, vector<Point> dataSet)
{
    Point retParams;
    
    const float LEARNING_RATE = 0.01;
    
    while(checkDerivative0(dataSet, theta0, theta1) != 0)
    {
        float temp0;
        temp0 = theta0 - LEARNING_RATE * checkDerivative0(dataSet, theta0, theta1);
        
        theta1 = theta1 - LEARNING_RATE * checkDerivative1(dataSet, theta0, theta1);
        theta0 = temp0;
    }
    
    retParams.x = theta0;
    retParams.y = theta1;
    
    return retParams;
}
 float checkDerivative0(vector<Point> dataSet, float theta0, float theta1)
{
    float retCount;
    retCount = 0;
    
    for(int i = 0; i < dataSet.size(); i++)
    {
        retCount += hypothesis(theta0, theta1, i + 1) - dataSet.at(i).y;
    }
    
    return retCount;
}
 float checkDerivative1(vector<Point> dataSet, float theta0, float theta1)
{
    float retCount;
    retCount = 0;
    
    for(int i = 0; i < dataSet.size(); i++)
    {
        retCount += (hypothesis(theta0, theta1, i + 1) - dataSet.at(i).y) * dataSet.at(i).x;
    }
    
    return retCount;
}
 float hypothesis(float theta0, float theta1, float x)
{
    return theta0 + theta1 * x;
}
 
 
Item 3430: 
CHAT WITH US AND BE ONE STEP BEYOND
 USE BLiP AND LEVEL UP YOUR CHATBOTS
 
 
Item 3434: 
def has_lucky_number(nums):
    """Return whether the given list of numbers is lucky. A lucky list contains at least one number divisible by 7.
    """
     for num in nums:
        if num % 7 == 0:
            return True
        else:
            return False
 nums = [1,2,3,4,5,6,7]
has_lucky_number(nums)
 def has_lucky_number(nums):
   
    for num in nums:
        if num % 7 == 0:
            return True
    
    return False
 [1, 2, 3, 4] > 2
 def elementwise_greater_than(L, thresh):
    """Return a list with the same length as L, where the value at index i is True if L[i] is greater than thresh, and False otherwise.
    
    >>> elementwise_greater_than([1, 2, 3, 4], 2)
    [False, False, True, True]
    """
     greater = []
    for i in L:
        greater.append(i > thresh)
    return greater
    pass
 def menu_is_boring(meals):
     for i in range(len(meals)-1):
        if meals[i] == meals[i+1]:
             return True
    return False
 def estimate_average_slot_payout(n_runs):
    """Run the slot machine n_runs times and return the average net profit per run.
    """
     a =[]
    for i in range(n_runs):
        a.append(play_slot_machine()-1) 
    return sum(a)/len(a) #Average
 def slots_survival_probability(start_balance, n_spins, n_simulations):
    """Return the approximate probability (as a number between 0 and 1) that we can complete the given number of spins of the slot machine before running out of money, assuming we start with the given balance. 
 Estimate the probability by running the scenario the specified number of times.
    
    >>> slots_survival_probability(10.00, 10, 1000)
    1.0
    >>> slots_survival_probability(1.00, 2, 1000)
    .25
    """
    
    success = 0
    
    for i in range(n_simulations):
        start = start_balance
        spin = n_spins
        while start>=1 and spin:
            start = start - 1 + play_slot_machine()
            spin -=1
        if spin ==0:
            success+= 1
    return success / n_simulations
 
 
Item 3435: 
PROJECT_ID=
BUCKET_NAME=
REGION=
 gsutil mb -l $REGION gs://$BUCKET_NAME
 gsutil cp -r data gs://$BUCKET_NAME/data
 TRAIN_DATA=gs://$BUCKET_NAME/data/adult.data.csv
EVAL_DATA=gs://$BUCKET_NAME/data/adult.test.csv
TEST_JSON=gs://$BUCKET_NAME/data/test.json
 JOB_NAME=my_first_instance
OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME
 gcloud ml-engine jobs submit training $JOB_NAME \
--job-dir $OUTPUT_PATH \
--runtime-version 1.2 \
--module-name trainer.task \
--package-path trainer/ \
--region $REGION \
-- \
--train-files $TRAIN_DATA \
--eval-files $EVAL_DATA \
--train-steps 1000 \
--verbosity DEBUG
 trainer/
       __init.py__
       model.py
       task.py
 python -m tensorflow.tensorboard --logdir=$OUTPUT_PATH --port=8080
 JOB_NAME=my_first_distr
OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME
 gcloud ml-engine jobs submit training $JOB_NAME \
    --job-dir $OUTPUT_PATH \
    --runtime-version 1.2 \
    --module-name trainer.task \
    --package-path trainer/ \
    --region $REGION \
    --scale-tier STANDARD_1 \
    -- \
    --train-files $TRAIN_DATA \
    --eval-files $EVAL_DATA \
    --train-steps 1000 \
    --verbosity DEBUG  \
    --eval-steps 100
 HPTUNING_CONFIG=../config.yaml
JOB_NAME=census_core_hptune_1
TRAIN_DATA=gs://$BUCKET_NAME/data/adult.data.csv
EVAL_DATA=gs://$BUCKET_NAME/data/adult.test.csv
   "hyperparameters": {
    "goal": "MAXIMIZE",
    "params": [
      {
        "parameterName": "first-layer-size",
        "minValue": 50,
        "maxValue": 500,
        "type": "INTEGER",
        "scaleType": "UNIT_LINEAR_SCALE"
      },
      {
        "parameterName": "num-layers",
        "minValue": 1,
        "maxValue": 15,
        "type": "INTEGER",
        "scaleType": "UNIT_LINEAR_SCALE"
      },
      {
        "parameterName": "scale-factor",
        "minValue": 0.1,
        "maxValue": 1,
        "type": "DOUBLE",
        "scaleType": "UNIT_REVERSE_LOG_SCALE"
      }
    ],
    "maxTrials": 4,
    "maxParallelTrials": 2,
    "hyperparameterMetricTag": "accuracy"
  }
 python -m tensorflow.tensorboard --logdir=$OUTPUT_PATH --port=8080
 MODEL_NAME=mymodel
gcloud ml-engine models create $MODEL_NAME --regions=$REGION
 OUTPUT_PATH=gs://$BUCKET_NAME/my_first_distr
 MODEL_BINARIES=gs://$BUCKET_NAME/my_first_distr/export/Servo/1487877383942/
 gcloud ml-engine versions create v1 \
--model $MODEL_NAME \
--origin $MODEL_BINARIES \
--runtime-version 1.2
 gcloud ml-engine predict \
--model $MODEL_NAME \
--version v1 \
--json-instances \
../test.json
 JOB_NAME=my_batch_pred
OUTPUT_PATH=gs://$BUCKET_NAME/$JOB_NAME
 gcloud ml-engine jobs submit prediction $JOB_NAME \
--model $MODEL_NAME \
--version v1 \
--data-format TEXT \
--region $REGION \
--input-paths $TEST_JSON \
--output-path $OUTPUT_PATH/predictions
 
 
Item 3436: 

Place for code
 
 
Item 3438: 
def sumaf(a,b): 
    return(a+b)
 print (sumaf("dog",5)# Error
print ("hello")
 def sumaf(a,b):
    try: 
        return a+b
    except: 
        print("There's a error" )
 print (sumaf("dog",5))
print("hello") 
 def sumaf(a,b):
    try: 
        return a+b
    except Exception as e: 
        print("There's a error" )
 print (sumaf("dog",5))
print("hello")
 def sumaf(a,b):
    try: 
        return a+b
    except Exception as e: 
        print("There's a error" )
    finally: 
        print("Thank you")
 print (sumaf("dog",5))
 print("hello")
 def sumaf(a,b):
    try: 
        return a+b
    except TypeError:  
        print("There's a error" )
    finally: 
        print("Thank you")
print (sumaf("dog",5))
print("hello")
 
 
Item 3439: 
docker pull jupyter/minimal-notebook
 docker run -p 8888:8888 jupyter/minimal-notebook
 docker run -p -d 8888:8888 jupyter/minimal-notebook
 
 
Item 3442: 
.writeheader()
 
 
Item 3445: 
Follow me on Medium to be notified when the next chapter is published. Can't wait? Check out the book on Amazon!
 
 
Item 3449: 
Cultivar Date Weight        sd  n         se
1      c39  d16   3.18 0.9566144 10 0.30250803
2      c39  d20   2.80 0.2788867 10 0.08819171
3      c39  d21   2.74 0.9834181 10 0.31098410
4      c52  d16   2.26 0.4452215 10 0.14079141
5      c52  d20   3.11 0.7908505 10 0.25008887
6      c52  d21   1.47 0.2110819 10 0.06674995
 
 
Item 3450: 
b <- c(2,3) 
b * 2
 c <- c(6,7)
b+c
 sapply(b, function(x){x + 2})
 
 
Item 3454: 
DROP TABLE IF EXISTS docs;
CREATE TABLE docs (line STRING);
LOAD DATA INPATH 'input_file' OVERWRITE INTO TABLE docs;
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
 (SELECT explode(split(line, '\s')) AS word FROM docs) temp
GROUP BY word
ORDER BY word;
 
 
Item 3455: 
a = tf.placeholder(tf.int32, shape=(2,2))
b = tf.matmul(a, a)
c = 2 * b + tf.constant([3,1])
print(c)
d = c + a
...
with tf.Session() as sess:
    sess.run(d, feed_dict={a: [[1,2],[3,4]]})
 Tensor("add_2:0", shape=(2, 2), dtype=int32)
 a = tf.placeholder(tf.int32, shape=(2,2))
b = tf.matmul(a, a)
c = 2 * b + tf.constant([3,1])
c_out = tf.Print(c, [c], "Value of c = ")
d = c_out + a
...
with tf.Session() as sess:
    sess.run(d, feed_dict={a: [[1,2],[3,4]]})
 2018-04-08 21:11:24.025055: I tensorflow/core/kernels/logging_ops.cc:79] Value of c = [[17 21][33]...]
 noise = 1.0
n_features = 100
X = tf.placeholder(tf.float32, shape=[None, n_features]
X_noisy = X + noise * tf.random_normal(X.get_shape())
 ValueError: Cannot convert a partially known TensorShape to a Tensor: (?, 100)
 noise = 1.0
n_features = 100
X = tf.placeholder(tf.float32, shape=[None, n_features]
X_noisy = X + noise * tf.random_normal(tf.shape(X))
 def happens_before(tensor1, tensor2):
    with tf.control_dependencies([tensor1]):
        return tensor2
 def happens_before(tensor1, tensor2):
    with tf.control_dependencies([tensor1]):
        return tf.identity(tensor2)
 tf.logging.set_verbosity(tf.logging.INFO)
 
 
Item 3460: 
#1. Creation class
 class skeleton(): 
 def __init__(self,life=5,attack=1,defence=2): 
 self.life=life
 self.attack=attack
 self.defence=defence
 def scream(self): 
 print ("I m a Skeleton !")
 
#2. Creating the skeletons using the class above
 ske1=skeleton(10,2,2)
ske2=skeleton(8,2,3)
ske3=skeleton(6,3,1)
ske4=skeleton(12,5,4)
 print (ske3.scream())
 
 
Item 3463: 
let imagePicker = UIImagePickerController()
 imagePicker.delegate = self
 imagePicker.sourceType = .camera
imagePicker.allowsEditing = false
 @IBAction func cameraTapped(_ sender: UIBarButtonItem) {
    present(imagePicker, animated: true, completion: nil)
}
 func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [String : Any]) {
       if let pickedImage = info[UIImagePickerControllerOriginalImage] as? UIImage {
           imageView.image = pickedImage
       }
        imagePicker.dismiss(animated: true, completion: nil)
   }
 guard let ciImage = CIImage(image: pickedImage) else {
   fatalError("Could not convert UIImage to CIImage.")
}
 func detect(image: CIImage) {}
 guard let model = try? VNCoreMLModel(for: Inceptionv3().model) else {
   fatalError("Loading CoreML model failed.")
}
 let request = VNCoreMLRequest(model: model) { (request, error) in
   guard let results = request.results as? [VNClassificationObservation] else {
       fatalError("Model failed to process image.")
   }
   
   if let firstResult = results.first {
       guard let navBar = self.navigationController?.navigationBar else { fatalError() }
       
       if firstResult.identifier.contains("hotdog") {
           self.navigationItem.title = "Hotdog!"
           navBar.barTintColor = UIColor.green
       } else {
           self.navigationItem.title = "Not Hotdog!"
           navBar.barTintColor = UIColor.red
       }
   }
}
 let handler = VNImageRequestHandler(ciImage: image)
do {
   try handler.perform([request])
} catch {
   print(error)
}
 detect(image: ciImage)
 
 
Item 3464: 
#Script for Sentiment Analysis of Character Scripts
 library('stringr')
library('readr')
library('wordcloud')
library('tm')
library('SnowballC')
library('RWeka')
library('RSentiment')
library('data.table')
library('DT')
 simpsons <- read.csv("simpsons_script_lines.csv",stringsAsFactors = F)
 View(simpsons)
 str(simpsons)
 simpsons <- simpsons[,c("raw_character_text","raw_location_text","normalized_text","word_count")]
 simpsons$word_count <- as.numeric(simpsons$word_count)
 simpsons <- simpsons[!is.na(simpsons$word_count),]
 simpsons$raw_location_text <- as.factor(simpsons$raw_location_text)
 simpsons$raw_character_text <- as.factor(simpsons$raw_character_text)
 family <- simpsons[simpsons$raw_character_text%in%c("Lisa Simpson","Bart Simpson","Homer Simpson","Marge Simpson","Maggie Simpson"),]
 View(family)
 simpsons <- as.data.table(simpsons)
 family <- as.data.table(family)
 #Homer Simpson
 homer <- family$normalized_text[family$raw_character_text=="Homer Simpson"]
 corpus_homer = Corpus(VectorSource(list(homer)))
 #Remove punctuations etc
 corpus_homer = tm_map(corpus_homer, removePunctuation)
corpus_homer = tm_map(corpus_homer, content_transformer(tolower))
corpus_homer = tm_map(corpus_homer, removeNumbers)
corpus_homer = tm_map(corpus_homer, stripWhitespace)
corpus_homer = tm_map(corpus_homer, removeWords, stopwords('english'))
 #Wordcloud of all words spoken by Homer Simpson
 wordcloud::wordcloud(corpus_homer, max.words = 100, random.order = FALSE, col="orange")
 #frequency of words spoken
 dtm_homer = DocumentTermMatrix(VCorpus(VectorSource(corpus_homer[[1]]$content)))freq_homer <- colSums(as.matrix(dtm_homer))
 View(freq_homer)
 #Sentiments - Homer
 sent_homer = calculate_sentiment(names(freq_homer))
sent_homer = cbind(sent_homer, as.data.frame(freq_homer))
str(sent_homer)
View(sent_homer)
 sent_pos_homer = sent_homer[sent_homer$sentiment == 'Positive',]
sent_neg_homer = sent_homer[sent_homer$sentiment == 'Negative',]
 #Number of positive and Negative Sentiments by Homer Simpsons
 cat("Negative Sentiments: ",sum(sent_neg_homer$freq_homer)," positive sentiments: ",sum(sent_pos_homer$freq_homer))
 #Homer positive wordcloud
 wordcloud(sent_pos_homer$text,sent_pos_homer$freq, min.freq=15,colors=brewer.pal(11,"PiYG"))
 #Homer negative wordcloud
 wordcloud(sent_neg_homer$text,sent_neg_homer$freq, min.freq=15,colors=brewer.pal(100,"RdYlBu"))
 #Bart Simpson
 bart <- family$normalized_text[family$raw_character_text=="Bart Simpson"]
 corpus_bart = Corpus(VectorSource(list(bart)))
 #Remove punctuations etc
 corpus_bart = tm_map(corpus_bart, removePunctuation)
corpus_bart = tm_map(corpus_bart, content_transformer(tolower))
corpus_bart = tm_map(corpus_bart, removeNumbers)
corpus_bart = tm_map(corpus_bart, stripWhitespace)
corpus_bart = tm_map(corpus_bart, removeWords, stopwords('english'))
 #Wordcloud of all words spoken by Homer Simpson
 wordcloud::wordcloud(corpus_bart, max.words = 100, random.order = FALSE, col="turquoise")
 #frequency of words spoken
 dtm_bart = DocumentTermMatrix(VCorpus(VectorSource(corpus_bart[[1]]$content)))
freq_bart <- colSums(as.matrix(dtm_bart))
 View(freq_bart)
 #Sentiments - Bart
 sent_bart = calculate_sentiment(names(freq_bart))
sent_bart = cbind(sent_bart, as.data.frame(freq_bart))
View(sent_homer)
 newdata <- sent_homer[order(-freq_homer),]
View(newdata)
 sent_pos_bart = sent_bart[sent_bart$sentiment == 'Positive',]
sent_neg_bart = sent_bart[sent_bart$sentiment == 'Negative',]
 #Number of positive and Negative Sentiments by Homer Simpsons
 cat("Negative Sentiments: ",sum(sent_neg_bart$freq_bart)," positive sentiments: ",sum(sent_pos_bart$freq_bart))
 #Bart positive wordcloud
 wordcloud(sent_pos_bart$text,sent_pos_bart$freq, min.freq=10,colors=brewer.pal(11,"PiYG"))
 #Bart positive wordcloud
 wordcloud(sent_neg_bart$text,sent_neg_bart$freq, min.freq=12,colors=brewer.pal(11,"RdYlBu"))
 #Marge Simpson
 marge <- family$normalized_text[family$raw_character_text=="Marge Simpson"]
corpus_marge = Corpus(VectorSource(list(marge)))
 #Remove punctuations etc
 corpus_marge = tm_map(corpus_marge, removePunctuation)
corpus_marge= tm_map(corpus_marge, content_transformer(tolower))
corpus_marge = tm_map(corpus_marge, removeNumbers)
corpus_marge = tm_map(corpus_marge, stripWhitespace)
corpus_marge = tm_map(corpus_marge, removeWords, stopwords('english'))
 #Wordcloud of all words spoken by Homer Simpson
 wordcloud::wordcloud(corpus_marge, max.words = 100, random.order = FALSE, col="purple")
 #frequency of words spoken
 dtm_marge = DocumentTermMatrix(VCorpus(VectorSource(corpus_marge[[1]]$content)))
freq_marge <- colSums(as.matrix(dtm_marge))
 View(freq_marge)
str(sent_bart)
 #Sentiments - Marge
 sent_marge = calculate_sentiment(names(freq_marge))
sent_marge = cbind(sent_marge, as.data.frame(freq_marge))
View(sent_marge)
 str(sent_marge)
sent_pos_marge = sent_marge[sent_marge$sentiment == 'Positive',]
sent_neg_marge = sent_marge[sent_marge$sentiment == 'Negative',]
 #Number of positive and Negative Sentiments by MARGE Simpsons
 cat("Negative Sentiments: ",sum(sent_neg_marge$freq_marge)," positive sentiments: ",sum(sent_pos_marge$freq_marge))
sum(sent_bart$freq_bart)
 #Bart positive wordcloud
 wordcloud(sent_pos_marge$text,sent_pos_marge$freq, min.freq=10,colors=brewer.pal(11,"PiYG"))
 #Bart positive wordcloud
 wordcloud(sent_neg_marge$text,sent_neg_marge$freq, min.freq=10,colors=brewer.pal(11,"RdYlBu"))
 
 
Item 3465: 
[1] -0.1904757
 
 
Item 3466: 
funcname(input1, input 2).
 X <- c(1, 2, 3)
 Y<- c(2, 3, 4)
 rm(Y)
 ?matrix
 m <- matrix(data=c(1,2,3,4), nrow=2, ncol=2)
 m <- matrix(c(1,2,3,4),2,2)
 m <- matrix(data=c(1,2,3,4), nrow=2, ncol=2, byrow=TRUE)
 1  2 
 3  4
 x <- matrix(c(1,2,3,4),2,2)
y <- matrix(c(0,3,2,1),2,2)
 x %*% y
 9   5
 12  8
 A <- matrix (1:16,4,4)
 A[2,3]
 A[c(1,3),c(2,4)]
 A[1:3,2:4]
 A[1:2,]
 A[,1:2]
 A[-c(1,3),]
 2 6 10 14
 4 8 12 16
 A[-c(1,3),-c(1,3,4)]
 6 8
 dim(A)
 4 4
 setwd(“C:/Users/sulayman/Documents/DataRegressed”)
 Auto<-read.csv(“Auto.csv”)
 Auto <- read.table(“Auto.txt”, header=T, na.strings=”?”)
 fix(Auto)
 i <- 2
if (i>0) {
print(i)
}
 source(“filename.r”)
 
 
Item 3468: 
len supp dose
1   4.2   VC  0.5
2  11.5   VC  0.5
3   7.3   VC  0.5
4   5.8   VC  0.5
 57 26.4   OJ  2.0
58 27.3   OJ  2.0
59 29.4   OJ  2.0
60 23.0   OJ  2.0
 
 
Item 3469: 
````sdd
 ?
 jai jagadeesha ajaaarsrreee sssssssssssssssss
ssssssss
ckkkkkkkds;lfkl,,m,sbrownfoxjumps
sborother
 brother
br oter
baromet
err
vigdishahaaakeshavahaladarahjalaldareeeereedlsmp[9ojm./lkf
sdl9oj
 so9i7lksd322kj[9l m[0s8a'/w.mhn sdfms8epfisl8e534w9 qweklqjf48/7wr
 q
 
 
Item 3470: 
Year Anomaly10y Unc10y
1 1800     -0.435  0.505
2 1801     -0.453  0.493
3 1802     -0.460  0.486
4 1803     -0.493  0.489
5 1804     -0.536  0.483
6 1805     -0.541  0.475
 
 
Item 3472: 
Call_List = [ Jack, Jill, Humpty, Jill, Jack, Mickey, Jill ]
 >>>C = Counter ()
 >>>Print(C(Call_List)) 
  Counter({Jack : 2, Jill : 3, Humpty : 1, Mickey : 1}) 
 >>>C.most_common(1)
  [ (Jill, 3) ] 
 >>>C.most_common(2)
  [ (Jill, 3), (Jack ,2) ] 
 
 
Item 3477: 
constantValue1 = tf.constant(9.0, dtype=tf.float32)
constantValue2 = tf.constant(19.0)
 print("constantValue1 = %s" % constantValue1)
print("constantValue2 = %s" % constantValue2)
The outputs of print statements will be -
 constantValue1 = Tensor("Const:0", shape=(), dtype=float32)
constantValue2 = Tensor("Const_1:0", shape=(), dtype=float32)
 Notice that the output wasn't 9.0 or 19.0 but Tensor objects. This is because we just built the computational graph but did not run it. Before running it, let's see what the above output means. So, in the Tensor object, first parameter is the name for that tensor. The "Const" part of the name is assigned to it by the TensorFlow itself if not explicitly given by the programmer. The name generated is then followed by a ":" which is followed by a number, 0 in this case. This number is the index of that tensor which is being named. What I mean by that is, a node can produce multiple outputs or multiple tensors as output. So, in that case this number would be the index of each of the tensor in output. Here, though, there is only one output, so the tensor gets assigned 0. If there was one more output, that tensor would have been assigned 1. Second parameter signifies the shape of that tensor. I have already talked about shape of tensors in my previous blog. Third type is the data type of that tensor. You can either explicitly give it, as done for the first constant, or TensorFlow can also infer it, as done for the second constant.
 If we want to see 9.0 and 19.0 as output, we will have to actually run the computational graph we just built. To do that, we will have to create a Session object and invoke it's run method. We can do that as done below -
 sess = tf.Session()
 print(sess.run(constantValue1))
print(sess.run(constantValue2))
The output of the above code will be 9.0 and 19.0.
 Now, let's add these two constants. Adding is an operation, and an operation is just another node in tensorflow.
 addConstants = constantValue1 + constantValue2
print("addConstants = ", addConstants)
 sumOfConstants = sess.run(addConstants)
print("sum = ", sumOfConstants)
The output of the above code is -
 addConstants = Tensor("add:0", shape=(), dtype=float32)
sum = 28.0
 Here, '+' is just a shorthand for "tf.add()".
 Now, how do we supply our own values to TensorFlow? For these purposes, placeholder comes in picture. A placeholder is a promise to provide a value later. Let's quickly create two placeholders and perform an operation on them to see them in action.
 myValue1 = tf.placeholder(dtype=tf.float32)
myValue2 = tf.placeholder(dtype=tf.float32)
 sumOfMyValuesNode = myValue1 + myValue2
sumOfMyValues = sess.run(sumOfMyValuesNode, {myValue1: 5.0, myValue2: 6.0})
 print("Sum of myValues = ", sumOfMyValues)
Here, myValue1 and myValue2 both are placeholders, whose value will be supplied later. Notice here that giving the data type is compulsory (dtype). The values to the placeholder can be supplied when the run method of the session object is invoked, as done in the above example. The values are supplied in the feed_dict argument of the run method. So, the output of the above code is
 Sum of myValues = 11.0
 But the whole point of machine learning is to make our data trainable so that we can train it, optimize it based on the training results and finally achieve a model that can work almost perfectly on the real data. So, how do we make our data trainable in TensorFlow? For this purpose, comes Variables to our rescue.
 Variables allow us to add trainable parameters to our program. Variables can be defined as follows -
 myVariable = tf.Variable(2.0, dtype=tf.float32)
 Evey variable is initialized with some value, 2.0 in this case, and giving a data type is optional. But the variable is only defined using the above way, it is not yet initialized. Variables are not initialized when you call tf.Variable. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows -
 init = tf.global_variables_initializer()
sess.run(init)
 It is important to realize that init is a handle to the TensorFlow sub-graph that initializes all the global variables. Until we call sess.run, the variables are uninitialized.
 print("myVariable = ", sess.run(myVariable))
 This prints out myVariable = 2.0
 And if we want to change the value of our variable, we can use the assign function as below -
 sess.run(tf.assign(myVariable, 10.0))
print(sess.run(myVariable))
 which prints 10.0 as output.
 Ok, so now that we are clear with the basic terms for writing a TensorFlow program, we will take a very easy example and implement it. We will implement the following model-
 y = W * x
 We will provide our program with some training data, i.e., some values of x and desired values of y for that x, calculate the value of W on the basis of the training data, and then provide test data to see how accurate the results are on test data. Since we have taken a very simple model, our accuracy would easily reach 100%. But this almost never happens in the real and more complex models. But for understanding purposes, this will do.
 Since we will supply the values for x and y, we will declare them as placeholders and since the value of W will have to be changed for every input, we will declare it as a variable, with some initial value, let's say 1. So declarations will go something like this -
 W = tf.Variable(1, dtype=tf.float32)
x = tf.placeholder(tf.float32)
y = tf.placeholder(tf.float32)
 Now, we will define our simple model as below -
 myModel = W * x
 Now, to train the data and get closer to the real model, we will have to write a loss function and then minimize it. To keep things simple, we will take the sum of squared errors as the loss function. Error is nothing but the difference between what the result came using our model, and what the desired value(y) was. Then, we will square those error for each of the input and add them. Below is the implementation for the same -
 delta = myModel - y
squaredDelta = tf.square(delta)
loss = tf.reduce_sum(squaredDelta)
 To keep things simple, we will make our own little optimizer, based on the concept of gradient descent optimizer(if you don't know about it, don't worry just keep reading) to correct the value of W and then test it on some test data.
 So, what we will be doing is calculating loss of our model, manipulating the value of W to minimize the loss, checking if loss has decreased or not, and manipulating the value of W further based on the result of loss. The code I've written for this optimizer is as below -
 oldLoss = sys.float_info.max
 adding = 0
subtracting = 0
 def addOne():
    sess.run(tf.assign(W, sess.run(W) + 1.0))
 def subtractOne():
    sess.run(tf.assign(W, sess.run(W) - 1.0))
 while oldLoss > 0:
   currentLoss = sess.run(loss, {x: [1, 2, 3, 4], y: [10, 20, 30, 40]})
   if currentLoss == 0:
       break
   elif adding == 0 and subtracting == 0:
       addOne()
       adding = 1
   elif adding == 1 and currentLoss <= oldLoss:
       addOne()
       adding = 1
       subtracting = 0
   elif adding == 1 and currentLoss >= oldLoss:
       subtractOne()
       adding = 0
       subtracting = 1
   elif subtracting == 1 and currentLoss <= oldLoss:
       subtractOne()
       subtracting = 1
       adding = 0
   elif subtracting == 1 and currentLoss >= oldLoss:
       addOne()
       subtracting = 0
       adding = 1
   oldLoss = currentLoss
Please keep in mind that we are certain here that our loss can reach 0, because we have used a simple model. For more complex models, the conditions can be changed appropriately.
 In the above code, adding and subtracting are flags that are used to remember what operation was performed last(addition or subtraction). currentLoss is a variable that stores the value of loss function at the starting of the loop and oldLoss is a variable that stores the value of loss function at the end of the loop. These two variables are compared in between the loop to check how the operation(addition or subtraction) affected the loss value, i.e., decreased or increased it and on the basis of that, further operations are performed. We are either decreasing the value of W by 1 or increasing it by 1. This is just a sample optimizer. Good optimizers are much more complex and efficient and many are already implemented in TensorFlow, which we will talk about in future blogs. This is just a sample optimizer which may not work perfectly but is well enough to give you an idea of how TensorFlow is working, which was my main objective here. The code written above is very simple to understand once you go through it and everything used in the code has been discussed in the blog.
 For input we are giving [1, 2, 3, 4] for x and [10, 20, 30, 40] for y(desired value). So, as we can see, value of W should be 10.0, which we have currently initialized to 1.0. So our model should use the training data supplied to it and convert W from 1.0 to 10.0, and use this W on the test data.
 So, to run our program, we have to initialize the global variables and make a session object and invoke it's run method on the global variables handle, like below -
 init = tf.global_variables_initializer()
 sess = tf.Session()
sess.run(init)
Ok, we are done. To check the value of W, we will put a print statement at the end -
 print(sess.run(W))
 This should print 10.0 as output when run. This means that the value of W has been changed from 1.0 to 10.0. If we supply some other data to our model to check the value of y, then we should always get 10 times of whatever value we supply. I put three print statements after the code to check the outputs -
 print(sess.run(myModel, {x: 27.0}))
print(sess.run(myModel, {x: 10.0}))
print(sess.run(myModel, {x: 80.0}))
 And the outputs I received were -
 270.0
100.0
800.0
 As expected.
 I hope I was able to introduce the concepts to you in an easy and yet understandable way. This was a very simple example, I encourage you to go ahead and examine with the example, play around with it, look into optimizers(Gradient Descent Optimizer would be a great start) and try to implement them in TensorFlow. Many optimizers have been implemented in TensorFlow about which I'll be discussing in my future blogs. For my next blog, I'll be using MNIST dataset of handwritten digits and recognize them using TensorFlow.
 References -
 Git hub repository for the implemented program - https://github.com/akshanshjain95/TensorFlow-Sample-Program
 I hope this blog turned out to be helpful for you.
 
 
Item 3479: 
a=3
 b=5
 sumAB=a+b
 print (sumAB)
 a="gato"
 
 
Item 3481: 
//Define matrix X
double[,] x = {{ 1.0, 2.0 },
         { 3.0, 4.0 }};
Matrix<double> mat_x = Matrix<double>.Build.DenseOfArray(x);


//Define Matrix Y
double[,] y = {{ 4.0, 2 },
         { 2.0, 3 }};
         
Matrix<double> mat_y = Matrix<double>.Build.DenseOfArray(y);


//Addition
Console.WriteLine("Addition");
var add = mat_x + mat_y;
add.Dump();
Console.WriteLine();

//Subtraction
Console.WriteLine("Subtraction");
var sub = mat_x - mat_y;
sub.Dump();
Console.WriteLine();

//Multiplication
Console.WriteLine("Multiplication");
var mul = mat_x * mat_y;
mul.Dump();
Console.WriteLine();
 
 
Item 3483: 
Sequência 1 - Execução dos algorítimos COM os dados após a limpeza, mas SEM qualquer feature adicional e SEM qualquer otimização de parâmetros
 Sequência 2 - Execução dos algorítimos COM os dados após a limpeza e COM features adicionais e SEM otimização de parâmetros
 Sequência 3 - Execução dos algorítimos COM os dados após a limpeza, COM features adicionais e COM otimização de parâmetros
 
 
Item 3484: 
{
	“architecture”: “lstm-attention”,
	“date-range”: [[“2017-01-01”, “2017-04-01”]],
	“parameters”: {
		“num_of_attentions”: [1, 2, 3],
		“attention_hidden_layer_size”: [32, 128, 256],
		“attention_regularization”: [0.1, 0.01, 0.001]
	}
}
 
 
Item 3487: 
>>> print
"Hello World", my name is “CO” and I am here to connect you to the Computational Renaissance.

I am still a prototype, so you may experience slow loading and occasional errors. Have patience with me, my evolution is inevitable!
 
 
Item 3490: 
def model_fn(features, labels, mode, params, config):
    ...
 
 
Item 3495: 
f(x)=W0+W1*x
Where f(x) is the estimated price
W0: intercept
W1: slope of the line
 The RSS will be :
RSS=
(p1-f(x1))^2+ (p2-f(x2))^2+......
 Where 
x1,x2: Sq.ft of house
p1,p2: Actual house price 
f(x): Predicted house price
 
 
Item 3496: 
python shapeset2_1cspo_2_3.10000.train.py write_formats amat
python shapeset2_1cspo_2_3.5000.test.py write_formats amat
python shapeset2_1cspo_2_3.5000.valid.py write_formats amat
 train = pd.read_csv(os.path.join(dataset_dir,"train.amat"), sep=" ", dtype=np.float32, skiprows=1, header=None)
 train_set = train.values
 images_train,shapes_train = train_set[:,:1024],train_set[:,1024]
 def get_fullconnected_shallow_softmax(args):
    input_shape = (32*32,)
    model = Sequential()
    model.add(Dense(2048,input_shape=input_shape,activation="relu"))
    model.add(Dense(3,activation="softmax"))
    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(args.lr),metrics=["accuracy"])
     return model
 +----------+-----------+-----------+-----------+------------+
| Neurons  | Train ACC | Train Loss| Valid Acc | Valid Loss |
+----------+-----------+-----------+-----------+------------+
|   128    | ~62.53%   | ~0.8506   | ~56.13%   | ~0.9469    |
+----------+-----------+-----------+-----------+------------+
|   256    | ~66.04%   | ~0.8001   | ~58.43%   | ~0.9209    |
+----------+-----------+-----------+-----------+------------+
|   512    | ~67.37%   | ~0.7581   | ~58.02%   | ~0.9196    |
+----------+-----------+-----------+-----------+------------+
|  1024    | ~71.64%   | ~0.6872   | ~60.74%   | ~0.891     |
+----------+-----------+-----------+-----------+------------+
|  2048    | ~70.51%   | ~0.6914   | ~58.61%   | ~0.9357    |
+----------+-----------+-----------+-----------+------------+
|  4096    | ~71.11%   | ~0.6678   | ~58.14%   | ~0.9632    |
+----------+-----------+-----------+-----------+------------+
|  8192    | ~73.63%   | ~0.6153   | ~59.16%   | ~0.9642    |
+----------+-----------+-----------+-----------+------------+
 
 
